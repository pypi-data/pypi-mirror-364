# ğŸ”„ Workflows del MÃ³dulo Services

## Flujo Principal de Chat Completo (13 pasos)

```
Usuario â†’ API â†’ ConversationService â†’ Semantic â†’ RAG â†’ Ollama â†’ Respuesta
                        â†“                                â†‘
                 SQLite + Weaviate â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detalles del flujo actualizado:

1. **API** recibe request â†’ **ChatService.process_message()**
2. **ChatService._handle_new_chat()** carga contexto previo automÃ¡ticamente (DecisiÃ³n #7)
3. **ChatService** integra Semantic para construir System Prompt DinÃ¡mico
4. **TaskService** detecta si es nueva tarea/continuaciÃ³n y gestiona jerarquÃ­a Task>Session>Message
5. **ChatService** usa HybridSearch desde RAG (70/30) + compresiÃ³n contextual para queries especÃ­ficos
6. **ChatService._generate_with_retry()** con Ollama usando retry logic robusto
7. **ChatService** usa Semantic para generar resumen CON chunks de contexto
8. **ConversationService.save_conversation_turn()** guarda resÃºmenes (NO mensajes completos)
9. **ConversationService** actualiza total_tokens acumulativo para estadÃ­sticas (DecisiÃ³n #12)
10. **IndexingService** indexa cÃ³digo validado en Weaviate con pipeline completo
11. **TaskService.save_technical_decision()** registra decisiones detectadas por Semantic
12. **GitService** reacciona a cambios y publica CacheInvalidateEvent (NO fetch automÃ¡tico)
13. **ConversationService** recibe eventos y propaga invalidaciÃ³n a HybridSearch

### CoordinaciÃ³n de tokens con TokenBudgetManager:

```python
# El context_size es el LÃMITE TOTAL del modelo
# Todo debe caber en este lÃ­mite: sistema + RAG + historial + pregunta + respuesta

Ejemplo con context_size = 32,768:
â”œâ”€â”€ Respuesta reservada: 10% (3,277 tokens)
â””â”€â”€ Disponible: 90% (29,491 tokens)
    â”œâ”€â”€ RAG chunks: 54% del total (17,694)
    â”œâ”€â”€ Historial: 27% del total (8,847)
    â””â”€â”€ Sistema: 9% del total (2,950)

# Si el historial excede su presupuesto:
# - Se comprimen mensajes antiguos en resÃºmenes
# - Se priorizan mensajes recientes y relevantes
# - SQLite guarda TODO, pero solo lo esencial va al LLM
```

### Ejemplo: GestiÃ³n de conversaciÃ³n larga con resÃºmenes

```
Pregunta 50 despuÃ©s de muchas interacciones:

SQLite tiene:           5,000 tokens en resÃºmenes (50 interacciones x 100 tokens promedio)
                              â†“
TokenBudgetManager selecciona 8,847 tokens:
  - Ãšltimos 5 resÃºmenes: 500 tokens
  - ResÃºmenes de la tarea actual: 2,000 tokens
  - ResÃºmenes relacionados por bÃºsqueda: 1,347 tokens
  - Mensajes recientes completos: 5,000 tokens
                              â†“
Enviado a Ollama:       sistema (2,950) + RAG (17,694) + historial (8,847) = 29,491 âœ“

*Nota: Los resÃºmenes permiten mantener contexto de miles de interacciones en solo unos pocos tokens*
```

## Flujo Completo del Sistema

```mermaid
graph TD
    A[Usuario mensaje] --> B[API /v1/chat/completions]
    B --> C[ChatService.process_message]
    
    C --> D{Â¿Session ID?}
    D -->|No| E[Crear nueva sesiÃ³n]
    E --> F[Buscar tarea activa]
    E --> G[ConversationService.create_session]
    D -->|SÃ­| H[Cargar contexto sesiÃ³n]
    
    C --> I[Semantic.analyze_query_intent]
    I --> J[TokenBudgetManager.allocate]
    
    C --> K[Semantic.detect_task_context]
    K --> L{Â¿Nueva tarea?}
    L -->|SÃ­| M[TaskService.create_task]
    L -->|No| N[TaskService.associate_session]
    
    C --> O[HybridSearch.search]
    O --> P{Â¿Query especÃ­fico?}
    P -->|SÃ­| Q[ContextualCompressor]
    P -->|No| R[Chunks sin comprimir]
    
    C --> S[Semantic.build_dynamic_context]
    S --> T[System Prompt]
    
    T --> U[OllamaClient.generate]
    Q --> U
    R --> U
    U --> V[Respuesta generada]
    
    C --> W[Semantic.generate_summary]
    V --> W
    Q --> W
    R --> W
    W --> X[Resumen con entidades]
    
    C --> Y[Semantic.detect_technical_decision]
    V --> Y
    Y --> Z{Â¿DecisiÃ³n detectada?}
    Z -->|SÃ­| AA[TaskService.save_technical_decision]
    
    X --> AB[ConversationService.save_conversation_turn]
    AB --> AC[SQLite + Weaviate]
    
    V --> AD[Respuesta al usuario]
```

## Flujo de IndexaciÃ³n con EventBus

```mermaid
graph LR
    A[Git Hook/Manual] --> B[IndexingService.index_files]
    B --> C[Filtrado de archivos]
    C --> D[Chunking inteligente]
    D --> E[ChunkType detection]
    E --> F[EnrichmentService]
    F --> G[EmbeddingService]
    G --> H[Weaviate storage]
    
    B --> I[_notify_progress con task_id]
    I --> J[EventBus.publish ProgressEvent]
    J --> K[WebSocket.handle_progress_event]
    K --> L[Filtrado por task_id]
    L --> M[NotificaciÃ³n al cliente]
```

### Ejemplo de cÃ³digo - IndexaciÃ³n inicial:

```python
# Script de instalaciÃ³n inicial
async def index_project():
    indexing_service = IndexingService()
    
    # Obtener todos los archivos del proyecto
    project_files = get_all_project_files()
    
    # Filtrar archivos soportados
    supported_files = indexing_service._filter_files(project_files)
    
    # Indexar con progreso
    result = await indexing_service.index_files(
        files=supported_files,
        trigger="installation",
        task_id="initial-indexing"
    )
    
    print(f"Indexados: {len(result['indexed'])} archivos")
    print(f"Chunks creados: {result['chunks_created']}")
    print(f"Embeddings generados: {result['embeddings_generated']}")
```

## Flujo de Git Reactivo

```mermaid
graph TD
    A[Usuario hace pull/fetch] --> B[Git hooks trigger]
    B --> C[GitService.detect_changes_from_others]
    C --> D{Â¿Cambios detectados?}
    D -->|SÃ­| E[Publicar CacheInvalidateEvent]
    E --> F[ConversationService recibe evento]
    F --> G[HybridSearch.invalidate_cache]
    
    D -->|SÃ­| H[notify_in_chat]
    H --> I[NotificaciÃ³n en chat]
```

### Ejemplo - DetecciÃ³n de cambios y notificaciÃ³n:

```python
# En ChatService cuando se crea nueva sesiÃ³n
async def _handle_new_chat(self):
    # ... cÃ³digo de inicializaciÃ³n ...
    
    # Verificar cambios de otros desarrolladores
    if self.git_service:
        changes = await self.git_service.detect_changes_from_others()
        if changes:
            # Generar notificaciÃ³n amigable
            notification = self.git_service.notify_in_chat(
                "others_changes",
                {"changes": changes}
            )
            
            # AÃ±adir al contexto inicial
            self.initial_context += f"\n\n{notification}"
            
            # Los eventos de invalidaciÃ³n se publican automÃ¡ticamente
```

## Sistema de InvalidaciÃ³n de Cache Coordinado

```mermaid
graph TB
    A[GitService detecta cambios] --> B[EventBus.publish CacheInvalidateEvent]
    B --> C[ConversationService suscrito]
    B --> D[IndexingService suscrito]
    B --> E[EnrichmentService suscrito]
    
    C --> F[HybridSearch.invalidate_cache]
    D --> G[Re-indexar archivos afectados]
    E --> H[Limpiar cache de metadata]
```

### ImplementaciÃ³n completa del flujo:

```python
# 1. GitService detecta cambios y publica evento
async def _publish_cache_invalidation(self, reason: str, files: List[str]):
    event = CacheInvalidateEvent(
        source="git_service",
        target_service="all",  # o ["conversation", "indexing"]
        key_pattern="|".join(f"*{file}*" for file in files),
        reason=reason
    )
    await self.event_bus.publish(event)

# 2. ConversationService estÃ¡ suscrito
def __init__(self):
    self._cache_subscription = event_bus.subscribe(
        EventType.CACHE_INVALIDATE,
        self._handle_cache_invalidation,
        filter=lambda e: e.target_service in ["conversation", "all"]
    )

# 3. ConversationService propaga a HybridSearch
async def _handle_cache_invalidation(self, event: CacheInvalidateEvent):
    logger.info(f"Invalidating cache: {event.reason}")
    
    if self.hybrid_search and hasattr(self.hybrid_search, 'invalidate_cache'):
        if event.key_pattern == "*":
            self.hybrid_search.invalidate_cache()
        else:
            self.hybrid_search.invalidate_cache(pattern=event.key_pattern)

# 4. HybridSearch invalida entradas especÃ­ficas
def invalidate_cache(self, pattern: Optional[str] = None):
    if not pattern or pattern == "*":
        self.cache.clear()
    else:
        # Invalidar solo entradas que coincidan
        keys_to_remove = [
            key for key in self.cache.keys() 
            if self._matches_pattern(key, pattern)
        ]
        for key in keys_to_remove:
            del self.cache[key]
```

## Casos de Uso Comunes

### 1. Inicio de Nueva ConversaciÃ³n

```python
# Usuario inicia chat sin session_id
response = await chat_service.process_message(
    message="Quiero implementar autenticaciÃ³n JWT",
    session_id=None
)

# ChatService automÃ¡ticamente:
# 1. Busca tarea activa
# 2. Si no hay, busca Ãºltima sesiÃ³n
# 3. Crea nueva sesiÃ³n con contexto previo
# 4. Detecta que es nueva tarea (IMPLEMENTATION)
# 5. Crea TaskCheckpoint asociado
```

### 2. ContinuaciÃ³n de Tarea Existente

```python
# Usuario continÃºa trabajando (con session_id)
response = await chat_service.process_message(
    message="Â¿CÃ³mo quedÃ³ el middleware de auth?",
    session_id="abc123"
)

# ChatService:
# 1. Carga contexto de la sesiÃ³n
# 2. Encuentra la tarea asociada
# 3. Carga todas las sesiones de la tarea
# 4. Busca chunks relacionados con auth
# 5. Genera respuesta con contexto completo
```

### 3. BÃºsqueda SemÃ¡ntica de Conversaciones

```python
# Buscar conversaciones sobre un tema
request = ConversationSearchRequest(
    query="implementaciÃ³n de cache redis",
    limit=5,
    include_completed=True,
    date_from=datetime(2025, 1, 1)
)

results = await conversation_service.search_conversations(request)

# Retorna ConversationSearchResult con:
# - session_id
# - content (resumen)
# - similarity_score
# - metadata (fecha, tokens, etc.)
```

### 4. DetecciÃ³n y Guardado de DecisiÃ³n TÃ©cnica

```python
# En el flujo de chat
message = "@decision Vamos a usar Redis para cache en lugar de Memcached"

# ChatService detecta el marcador @decision
# Semantic analiza y extrae:
decision = DetectedDecision(
    decision_type="ARCHITECTURE",
    title="Redis para sistema de cache",
    description="Cambiar de Memcached a Redis",
    rationale="Redis ofrece persistencia y estructuras de datos",
    alternatives_considered=["Memcached", "Hazelcast"],
    impact_level=4
)

# ChatService completa con IDs y guarda
await task_service.save_technical_decision(
    TechnicalDecision(**decision.dict(), 
                     session_id=session_id,
                     task_id=task_id)
)
```

### 5. AnÃ¡lisis de Conflictos Potenciales

```python
# Antes de modificar archivos
files_to_modify = ["auth/middleware.py", "auth/jwt_handler.py"]

conflicts = await git_service.analyze_potential_conflicts(files_to_modify)

if conflicts["severity"] > 7:
    # Alta probabilidad de conflictos
    warning = git_service.notify_in_chat("potential_conflicts", conflicts)
    # Mostrar warning al usuario
```

## Performance Tips

### 1. OptimizaciÃ³n de BÃºsquedas

```python
# Usar compresiÃ³n para queries especÃ­ficos
if query_type == "specific_file_question":
    # La compresiÃ³n reduce chunks 60-80%
    compressed_chunks = await compressor.compress(chunks, query)
```

### 2. Batch Processing en IndexaciÃ³n

```python
# Configurar workers segÃºn CPU disponible
indexing_service = IndexingService(
    batch_size=50,  # MÃ¡s archivos por batch
    concurrent_workers=8  # MÃ¡s workers paralelos
)
```

### 3. Cache con TTL Apropiado

```python
# GitService usa 5 minutos por defecto
# Ajustar segÃºn frecuencia de cambios
GIT_CACHE_TTL = 300  # segundos

# Para proyectos muy activos, reducir
GIT_CACHE_TTL = 60  # 1 minuto
```

### 4. LÃ­mites de Sesiones Relacionadas

```python
# En .acolyte
limits:
  max_related_sessions: 5  # Reducir para menos contexto
  related_sessions_chain: 3  # Profundidad de bÃºsqueda
  max_summary_turns: 4  # Turnos en resumen
```

### 5. GestiÃ³n de Memoria en Embeddings

```python
# Para proyectos grandes
embeddings:
  batch_size: 10  # Procesar menos archivos simultÃ¡neamente
  max_tokens_per_batch: 50000  # LÃ­mite estricto de memoria
```

## Flujo de Datos CrÃ­tico

```
Usuario â†’ API â†’ ChatService â†’ Semantic (anÃ¡lisis)
                    â†“
              ConversationService â† HybridSearch (bÃºsqueda)
                    â†“
              TaskService (contexto)
                    â†“
              Ollama (generaciÃ³n) â†’ Semantic (resumen)
                    â†“
              ConversationService (persistir)
                    â†“
              Usuario (respuesta)

Paralelo:
Git changes â†’ GitService â†’ EventBus â†’ Cache invalidation â†’ Fresh searches
```

## Secuencias de Llamadas TÃ­picas

### Crear Nueva Tarea

```python
1. ChatService.process_message()
2. â”œâ”€â”€ Semantic.detect_task_context()
3. â”œâ”€â”€ TaskService.create_task()
4. â”œâ”€â”€ ConversationService.create_session()
5. â””â”€â”€ TaskService.associate_session_to_task()
```

### BÃºsqueda con CompresiÃ³n

```python
1. ChatService.process_message()
2. â”œâ”€â”€ Semantic.analyze_query_intent()
3. â”œâ”€â”€ HybridSearch.search()
4. â”œâ”€â”€ ContextualCompressor.compress()
5. â””â”€â”€ OllamaClient.generate()
```

### InvalidaciÃ³n por Git

```python
1. Git hook ejecutado
2. â”œâ”€â”€ GitService.detect_changes_from_others()
3. â”œâ”€â”€ EventBus.publish(CacheInvalidateEvent)
4. â”œâ”€â”€ ConversationService._handle_cache_invalidation()
5. â””â”€â”€ HybridSearch.invalidate_cache()
```
