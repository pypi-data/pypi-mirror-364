# üîÑ Workflows del M√≥dulo Services

## Flujo Principal de Chat Completo (13 pasos)

```
Usuario ‚Üí API ‚Üí ConversationService ‚Üí Semantic ‚Üí RAG ‚Üí Ollama ‚Üí Respuesta
                        ‚Üì                                ‚Üë
                 SQLite + Weaviate ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Detalles del flujo actualizado:

1. **API** recibe request ‚Üí **ChatService.process_message()**
2. **ChatService._handle_new_chat()** carga contexto previo autom√°ticamente (Decisi√≥n #7)
3. **ChatService** integra Semantic para construir System Prompt Din√°mico
4. **TaskService** detecta si es nueva tarea/continuaci√≥n y gestiona jerarqu√≠a Task>Session>Message
5. **ChatService** usa HybridSearch desde RAG (70/30) + compresi√≥n contextual para queries espec√≠ficos
6. **ChatService._generate_with_retry()** con Ollama usando retry logic robusto
7. **ChatService** usa Semantic para generar resumen CON chunks de contexto
8. **ConversationService.save_conversation_turn()** guarda res√∫menes (NO mensajes completos)
9. **ConversationService** actualiza total_tokens acumulativo para estad√≠sticas (Decisi√≥n #12)
10. **IndexingService** indexa c√≥digo validado en Weaviate con pipeline completo
11. **TaskService.save_technical_decision()** registra decisiones detectadas por Semantic
12. **GitService** reacciona a cambios y publica CacheInvalidateEvent (NO fetch autom√°tico)
13. **ConversationService** recibe eventos y propaga invalidaci√≥n a HybridSearch

### Coordinaci√≥n de tokens con TokenBudgetManager:

```python
# El context_size es el L√çMITE TOTAL del modelo
# Todo debe caber en este l√≠mite: sistema + RAG + historial + pregunta + respuesta

Ejemplo con context_size = 32,768:
‚îú‚îÄ‚îÄ Respuesta reservada: 10% (3,277 tokens)
‚îî‚îÄ‚îÄ Disponible: 90% (29,491 tokens)
    ‚îú‚îÄ‚îÄ RAG chunks: 54% del total (17,694)
    ‚îú‚îÄ‚îÄ Historial: 27% del total (8,847)
    ‚îî‚îÄ‚îÄ Sistema: 9% del total (2,950)

# Si el historial excede su presupuesto:
# - Se comprimen mensajes antiguos en res√∫menes
# - Se priorizan mensajes recientes y relevantes
# - SQLite guarda TODO, pero solo lo esencial va al LLM
```

### Ejemplo: Gesti√≥n de conversaci√≥n larga con res√∫menes

```
Pregunta 50 despu√©s de muchas interacciones:

SQLite tiene:           5,000 tokens en res√∫menes (50 interacciones x 100 tokens promedio)
                              ‚Üì
TokenBudgetManager selecciona 8,847 tokens:
  - √öltimos 5 res√∫menes: 500 tokens
  - Res√∫menes de la tarea actual: 2,000 tokens
  - Res√∫menes relacionados por b√∫squeda: 1,347 tokens
  - Mensajes recientes completos: 5,000 tokens
                              ‚Üì
Enviado a Ollama:       sistema (2,950) + RAG (17,694) + historial (8,847) = 29,491 ‚úì

*Nota: Los res√∫menes permiten mantener contexto de miles de interacciones en solo unos pocos tokens*
```

## Flujo Completo del Sistema

```mermaid
graph TD
    A[Usuario mensaje] --> B[API /v1/chat/completions]
    B --> C[ChatService.process_message]
    
    C --> D{¬øSession ID?}
    D -->|No| E[Crear nueva sesi√≥n]
    E --> F[Buscar tarea activa]
    E --> G[ConversationService.create_session]
    D -->|S√≠| H[Cargar contexto sesi√≥n]
    
    C --> I[Semantic.analyze_query_intent]
    I --> J[TokenBudgetManager.allocate]
    
    C --> K[Semantic.detect_task_context]
    K --> L{¬øNueva tarea?}
    L -->|S√≠| M[TaskService.create_task]
    L -->|No| N[TaskService.associate_session]
    
    C --> O[HybridSearch.search]
    O --> P{¬øQuery espec√≠fico?}
    P -->|S√≠| Q[ContextualCompressor]
    P -->|No| R[Chunks sin comprimir]
    
    C --> S[Semantic.build_dynamic_context]
    S --> T[System Prompt]
    
    T --> U[OllamaClient.generate]
    Q --> U
    R --> U
    U --> V[Respuesta generada]
    
    C --> W[Semantic.generate_summary]
    V --> W
    Q --> W
    R --> W
    W --> X[Resumen con entidades]
    
    C --> Y[Semantic.detect_technical_decision]
    V --> Y
    Y --> Z{¬øDecisi√≥n detectada?}
    Z -->|S√≠| AA[TaskService.save_technical_decision]
    
    X --> AB[ConversationService.save_conversation_turn]
    AB --> AC[SQLite + Weaviate]
    
    V --> AD[Respuesta al usuario]
```

## Flujo de Indexaci√≥n con EventBus

```mermaid
graph LR
    A[Git Hook/Manual] --> B[IndexingService.index_files]
    B --> C[Filtrado de archivos]
    C --> D[Chunking inteligente]
    D --> E[ChunkType detection]
    E --> F[EnrichmentService]
    F --> G[EmbeddingService]
    G --> H[Weaviate storage]
    
    B --> I[_notify_progress con task_id]
    I --> J[EventBus.publish ProgressEvent]
    J --> K[WebSocket.handle_progress_event]
    K --> L[Filtrado por task_id]
    L --> M[Notificaci√≥n al cliente]
```

### Ejemplo de c√≥digo - Indexaci√≥n inicial:

```python
# Script de instalaci√≥n inicial
async def index_project():
    indexing_service = IndexingService()
    
    # Obtener todos los archivos del proyecto
    project_files = get_all_project_files()
    
    # Filtrar archivos soportados
    supported_files = indexing_service._filter_files(project_files)
    
    # Indexar con progreso
    result = await indexing_service.index_files(
        files=supported_files,
        trigger="installation",
        task_id="initial-indexing"
    )
    
    print(f"Indexados: {len(result['indexed'])} archivos")
    print(f"Chunks creados: {result['chunks_created']}")
    print(f"Embeddings generados: {result['embeddings_generated']}")
```

## Flujo de Git Reactivo

```mermaid
graph TD
    A[Usuario hace pull/fetch] --> B[Git hooks trigger]
    B --> C[GitService.detect_changes_from_others]
    C --> D{¬øCambios detectados?}
    D -->|S√≠| E[Publicar CacheInvalidateEvent]
    E --> F[ConversationService recibe evento]
    F --> G[HybridSearch.invalidate_cache]
    
    D -->|S√≠| H[notify_in_chat]
    H --> I[Notificaci√≥n en chat]
```

### Ejemplo - Detecci√≥n de cambios y notificaci√≥n:

```python
# En ChatService cuando se crea nueva sesi√≥n
async def _handle_new_chat(self):
    # ... c√≥digo de inicializaci√≥n ...
    
    # Verificar cambios de otros desarrolladores
    if self.git_service:
        changes = await self.git_service.detect_changes_from_others()
        if changes:
            # Generar notificaci√≥n amigable
            notification = self.git_service.notify_in_chat(
                "others_changes",
                {"changes": changes}
            )
            
            # A√±adir al contexto inicial
            self.initial_context += f"\n\n{notification}"
            
            # Los eventos de invalidaci√≥n se publican autom√°ticamente
```

## Sistema de Invalidaci√≥n de Cache Coordinado

```mermaid
graph TB
    A[GitService detecta cambios] --> B[EventBus.publish CacheInvalidateEvent]
    B --> C[ConversationService suscrito]
    B --> D[IndexingService suscrito]
    B --> E[EnrichmentService suscrito]
    
    C --> F[HybridSearch.invalidate_cache]
    D --> G[Re-indexar archivos afectados]
    E --> H[Limpiar cache de metadata]
```

### Implementaci√≥n completa del flujo:

```python
# 1. GitService detecta cambios y publica evento
async def _publish_cache_invalidation(self, reason: str, files: List[str]):
    event = CacheInvalidateEvent(
        source="git_service",
        target_service="all",  # o ["conversation", "indexing"]
        key_pattern="|".join(f"*{file}*" for file in files),
        reason=reason
    )
    await self.event_bus.publish(event)

# 2. ConversationService est√° suscrito
def __init__(self):
    self._cache_subscription = event_bus.subscribe(
        EventType.CACHE_INVALIDATE,
        self._handle_cache_invalidation,
        filter=lambda e: e.target_service in ["conversation", "all"]
    )

# 3. ConversationService propaga a HybridSearch
async def _handle_cache_invalidation(self, event: CacheInvalidateEvent):
    logger.info(f"Invalidating cache: {event.reason}")
    
    if self.hybrid_search and hasattr(self.hybrid_search, 'invalidate_cache'):
        if event.key_pattern == "*":
            self.hybrid_search.invalidate_cache()
        else:
            self.hybrid_search.invalidate_cache(pattern=event.key_pattern)

# 4. HybridSearch invalida entradas espec√≠ficas
def invalidate_cache(self, pattern: Optional[str] = None):
    if not pattern or pattern == "*":
        self.cache.clear()
    else:
        # Invalidar solo entradas que coincidan
        keys_to_remove = [
            key for key in self.cache.keys() 
            if self._matches_pattern(key, pattern)
        ]
        for key in keys_to_remove:
            del self.cache[key]
```

## Casos de Uso Comunes

### 1. Inicio de Nueva Conversaci√≥n

```python
# Usuario inicia chat sin session_id
response = await chat_service.process_message(
    message="Quiero implementar autenticaci√≥n JWT",
    session_id=None
)

# ChatService autom√°ticamente:
# 1. Busca tarea activa
# 2. Si no hay, busca √∫ltima sesi√≥n
# 3. Crea nueva sesi√≥n con contexto previo
# 4. Detecta que es nueva tarea (IMPLEMENTATION)
# 5. Crea TaskCheckpoint asociado
```

### 2. Continuaci√≥n de Tarea Existente

```python
# Usuario contin√∫a trabajando (con session_id)
response = await chat_service.process_message(
    message="¬øC√≥mo qued√≥ el middleware de auth?",
    session_id="abc123"
)

# ChatService:
# 1. Carga contexto de la sesi√≥n
# 2. Encuentra la tarea asociada
# 3. Carga todas las sesiones de la tarea
# 4. Busca chunks relacionados con auth
# 5. Genera respuesta con contexto completo
```

### 3. B√∫squeda Sem√°ntica de Conversaciones

```python
# Buscar conversaciones sobre un tema
request = ConversationSearchRequest(
    query="implementaci√≥n de cache redis",
    limit=5,
    include_completed=True,
    date_from=datetime(2025, 1, 1)
)

results = await conversation_service.search_conversations(request)

# Retorna ConversationSearchResult con:
# - session_id
# - content (resumen)
# - similarity_score
# - metadata (fecha, tokens, etc.)
```

### 4. Detecci√≥n y Guardado de Decisi√≥n T√©cnica

```python
# En el flujo de chat
message = "@decision Vamos a usar Redis para cache en lugar de Memcached"

# ChatService detecta el marcador @decision
# Semantic analiza y extrae:
decision = DetectedDecision(
    decision_type="ARCHITECTURE",
    title="Redis para sistema de cache",
    description="Cambiar de Memcached a Redis",
    rationale="Redis ofrece persistencia y estructuras de datos",
    alternatives_considered=["Memcached", "Hazelcast"],
    impact_level=4
)

# ChatService completa con IDs y guarda
await task_service.save_technical_decision(
    TechnicalDecision(**decision.dict(), 
                     session_id=session_id,
                     task_id=task_id)
)
```

### 5. An√°lisis de Conflictos Potenciales

```python
# Antes de modificar archivos
files_to_modify = ["auth/middleware.py", "auth/jwt_handler.py"]

conflicts = await git_service.analyze_potential_conflicts(files_to_modify)

if conflicts["severity"] > 7:
    # Alta probabilidad de conflictos
    warning = git_service.notify_in_chat("potential_conflicts", conflicts)
    # Mostrar warning al usuario
```

## Performance Tips

### 1. Optimizaci√≥n de B√∫squedas

```python
# Usar compresi√≥n para queries espec√≠ficos
if query_type == "specific_file_question":
    # La compresi√≥n reduce chunks 60-80%
    compressed_chunks = await compressor.compress(chunks, query)
```

### 2. Batch Processing en Indexaci√≥n

```python
# Configurar workers seg√∫n CPU disponible
indexing_service = IndexingService(
    batch_size=50,  # M√°s archivos por batch
    concurrent_workers=8  # M√°s workers paralelos
)
```

### 3. Cache con TTL Apropiado

```python
# GitService usa 5 minutos por defecto
# Ajustar seg√∫n frecuencia de cambios
GIT_CACHE_TTL = 300  # segundos

# Para proyectos muy activos, reducir
GIT_CACHE_TTL = 60  # 1 minuto
```

### 4. L√≠mites de Sesiones Relacionadas

```python
# En .acolyte
limits:
  max_related_sessions: 5  # Reducir para menos contexto
  related_sessions_chain: 3  # Profundidad de b√∫squeda
  max_summary_turns: 4  # Turnos en resumen
```

### 5. Gesti√≥n de Memoria en Embeddings

```python
# Para proyectos grandes
embeddings:
  batch_size: 10  # Procesar menos archivos simult√°neamente
  max_tokens_per_batch: 50000  # L√≠mite estricto de memoria
```

## Flujo de Datos Cr√≠tico

```
Usuario ‚Üí API ‚Üí ChatService ‚Üí Semantic (an√°lisis)
                    ‚Üì
              ConversationService ‚Üê HybridSearch (b√∫squeda)
                    ‚Üì
              TaskService (contexto)
                    ‚Üì
              Ollama (generaci√≥n) ‚Üí Semantic (resumen)
                    ‚Üì
              ConversationService (persistir)
                    ‚Üì
              Usuario (respuesta)

Paralelo:
Git changes ‚Üí GitService ‚Üí EventBus ‚Üí Cache invalidation ‚Üí Fresh searches
```

## Secuencias de Llamadas T√≠picas

### Crear Nueva Tarea

```python
1. ChatService.process_message()
2. ‚îú‚îÄ‚îÄ Semantic.detect_task_context()
3. ‚îú‚îÄ‚îÄ TaskService.create_task()
4. ‚îú‚îÄ‚îÄ ConversationService.create_session()
5. ‚îî‚îÄ‚îÄ TaskService.associate_session_to_task()
```

### B√∫squeda con Compresi√≥n

```python
1. ChatService.process_message()
2. ‚îú‚îÄ‚îÄ Semantic.analyze_query_intent()
3. ‚îú‚îÄ‚îÄ HybridSearch.search()
4. ‚îú‚îÄ‚îÄ ContextualCompressor.compress()
5. ‚îî‚îÄ‚îÄ OllamaClient.generate()
```

### Invalidaci√≥n por Git

```python
1. Git hook ejecutado
2. ‚îú‚îÄ‚îÄ GitService.detect_changes_from_others()
3. ‚îú‚îÄ‚îÄ EventBus.publish(CacheInvalidateEvent)
4. ‚îú‚îÄ‚îÄ ConversationService._handle_cache_invalidation()
5. ‚îî‚îÄ‚îÄ HybridSearch.invalidate_cache()
```
