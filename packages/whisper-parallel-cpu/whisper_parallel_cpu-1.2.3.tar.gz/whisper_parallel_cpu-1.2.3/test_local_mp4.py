#!/usr/bin/env python3
"""
Comprehensive test script for the new WhisperModel functionality with video.mp4
Tests model reuse, parameters, model management, and performance comparison.
"""

import time
import os
from whisper_parallel_cpu import WhisperModel, transcribe, clear_contexts, get_context_count, list_models, download_model

def test_whisper_model_basic():
    """Test basic WhisperModel functionality"""
    print("1Ô∏è‚É£ Testing WhisperModel class:")
    
    video_file = "video.mp4"
    
    try:
        model = WhisperModel(model="base", use_gpu=False, threads=4)
        print(f"‚úÖ Created model: {model}")
        
        # First transcription (will load the model)
        print("üîÑ First transcription (loading model)...")
        start_time = time.time()
        result1 = model.transcribe(video_file)
        first_time = time.time() - start_time
        print(f"‚úÖ First transcription: {result1[:100]}...")
        print(f"‚è±Ô∏è  Time: {first_time:.2f}s")
        
        # Second transcription (reusing model)
        print("üîÑ Second transcription (reusing model)...")
        start_time = time.time()
        result2 = model.transcribe(video_file)
        second_time = time.time() - start_time
        print(f"‚úÖ Second transcription: {result2[:100]}...")
        print(f"‚è±Ô∏è  Time: {second_time:.2f}s")
        
        # Third transcription (reusing model)
        print("üîÑ Third transcription (reusing model)...")
        start_time = time.time()
        result3 = model.transcribe(video_file)
        third_time = time.time() - start_time
        print(f"‚úÖ Third transcription: {result3[:100]}...")
        print(f"‚è±Ô∏è  Time: {third_time:.2f}s")
        
        print(f"üìä Context count: {model.get_context_count()}")
        return first_time, second_time, third_time
        
    except Exception as e:
        print(f"‚ùå WhisperModel test failed: {e}")
        return None, None, None

def test_regular_transcribe():
    """Test regular transcribe function"""
    print("\n2Ô∏è‚É£ Testing regular transcribe function:")
    
    video_file = "video.mp4"
    
    try:
        print("üîÑ First transcription with transcribe()...")
        start_time = time.time()
        result4 = transcribe(video_file, model="base", use_gpu=False, threads=4)
        transcribe_time = time.time() - start_time
        print(f"‚úÖ transcribe() result: {result4[:100]}...")
        print(f"‚è±Ô∏è  Time: {transcribe_time:.2f}s")
        return transcribe_time
        
    except Exception as e:
        print(f"‚ùå transcribe() test failed: {e}")
        return None

def test_performance_comparison(first_time, second_time, third_time, transcribe_time):
    """Compare performance between methods"""
    print("\n3Ô∏è‚É£ Performance Comparison:")
    print(f"   WhisperModel first run:  {first_time:.2f}s")
    print(f"   WhisperModel reuse:      {second_time:.2f}s")
    print(f"   WhisperModel reuse:      {third_time:.2f}s")
    print(f"   transcribe() function:   {transcribe_time:.2f}s")
    
    if second_time and transcribe_time and second_time > 0 and transcribe_time > 0:
        speedup = transcribe_time / second_time
        print(f"   üöÄ Speedup with model reuse: {speedup:.2f}x")
    
    if first_time and second_time and first_time > 0 and second_time > 0:
        model_loading_time = first_time - second_time
        print(f"   üì¶ Model loading time: {model_loading_time:.2f}s")

def test_memory_management():
    """Test memory management functions"""
    print("\n4Ô∏è‚É£ Testing memory management:")
    
    try:
        initial_count = get_context_count()
        print(f"   Initial context count: {initial_count}")
        
        # Clear contexts
        clear_contexts()
        after_clear = get_context_count()
        print(f"   After clear_contexts(): {after_clear}")
        
        # Create new model
        video_file = "video.mp4"
        model2 = WhisperModel(model="base", use_gpu=False)
        model2.transcribe(video_file)
        after_new = get_context_count()
        print(f"   After new model: {after_new}")
        
        print("‚úÖ Memory management test passed!")
        
    except Exception as e:
        print(f"‚ùå Memory management test failed: {e}")

def test_context_manager():
    """Test WhisperModel as context manager"""
    print("\n5Ô∏è‚É£ Testing context manager:")
    
    video_file = "video.mp4"
    
    try:
        with WhisperModel(model="base", use_gpu=False) as model:
            print("‚úÖ Created model in context manager")
            result = model.transcribe(video_file)
            print(f"‚úÖ Transcription in context: {result[:50]}...")
        
        print("‚úÖ Context manager test passed!")
        return True
        
    except Exception as e:
        print(f"‚ùå Context manager test failed: {e}")
        return False

def test_model_parameters():
    """Test different model parameters"""
    print("\n6Ô∏è‚É£ Testing model parameters:")
    
    video_file = "video.mp4"
    
    # Test different models
    models_to_test = ["tiny", "base"]
    
    for model_name in models_to_test:
        try:
            print(f"   Testing {model_name} model:")
            model = WhisperModel(model=model_name, use_gpu=False, threads=4)
            print(f"   ‚úÖ Created: {model}")
            
            start_time = time.time()
            result = model.transcribe(video_file)
            elapsed = time.time() - start_time
            
            print(f"   ‚è±Ô∏è  Time: {elapsed:.2f}s")
            print(f"   üìù Result: {result[:50]}...")
            
        except Exception as e:
            print(f"   ‚ùå {model_name} model failed: {e}")
    
    # Test different thread counts
    thread_counts = [2, 4, 8]
    
    for threads in thread_counts:
        try:
            print(f"   Testing with {threads} threads:")
            model = WhisperModel(model="base", use_gpu=False, threads=threads)
            print(f"   ‚úÖ Created: {model}")
            
            start_time = time.time()
            result = model.transcribe(video_file)
            elapsed = time.time() - start_time
            
            print(f"   ‚è±Ô∏è  Time: {elapsed:.2f}s")
            
        except Exception as e:
            print(f"   ‚ùå {threads} threads failed: {e}")

def test_model_management():
    """Test model management functionality"""
    print("\n7Ô∏è‚É£ Testing model management:")
    
    try:
        # List available models
        print("   Listing available models:")
        list_models()
        
        # Test downloading a model
        print("\n   Testing model download:")
        try:
            download_model("tiny", force=False)  # Won't re-download if exists
            print("   ‚úÖ Model download test passed!")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Model download test: {e}")
        
        # Test with custom model path
        model_path = "models/ggml-base.en.bin"
        if os.path.exists(model_path):
            print(f"\n   Testing with custom model path: {model_path}")
            model = WhisperModel(model=model_path, use_gpu=False, threads=4)
            print(f"   ‚úÖ Custom path model created: {model}")
            
            video_file = "video.mp4"
            result = model.transcribe(video_file)
            print(f"   ‚úÖ Custom path transcription: {result[:50]}...")
        
        print("‚úÖ Model management test passed!")
        
    except Exception as e:
        print(f"‚ùå Model management test failed: {e}")

def main():
    """Run all tests"""
    print("üß™ Comprehensive WhisperModel Test with video.mp4")
    print("Make sure you have activated your virtual environment!")
    print("=" * 60)
    
    # Check if video file exists
    if not os.path.exists("video.mp4"):
        print("‚ùå Error: video.mp4 not found!")
        print("Please make sure video.mp4 is in the current directory.")
        return False
    
    # Run tests
    first_time, second_time, third_time = test_whisper_model_basic()
    transcribe_time = test_regular_transcribe()
    test_performance_comparison(first_time, second_time, third_time, transcribe_time)
    test_memory_management()
    context_success = test_context_manager()
    test_model_parameters()
    test_model_management()
    
    print("\n" + "=" * 60)
    print("üéâ All tests completed!")
    
    if context_success:
        print("‚úÖ The new model reuse functionality is working correctly!")
        print("\nüìã Summary:")
        print("   ‚Ä¢ WhisperModel class works with all parameters")
        print("   ‚Ä¢ Model reuse provides performance benefits")
        print("   ‚Ä¢ Memory management functions correctly")
        print("   ‚Ä¢ Context manager support works")
        print("   ‚Ä¢ Model management integration works")
        print("   ‚Ä¢ Backward compatibility maintained")
    else:
        print("‚ùå Some tests failed. Check the error messages above.")
    
    return context_success

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1) 