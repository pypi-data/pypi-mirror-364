# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "clients.baml": "client<llm> OpenAIClient {\n  provider openai-generic\n  options {\n    api_key env.BAML_API_KEY\n    base_url env.BAML_BASE_URL\n    model env.BAML_MODEL\n  }\n}\nclient<llm> GeminiClient{\n  provider google-ai\n  options {\n    api_key env.BAML_API_KEY\n    model env.BAML_MODEL\n  }\n}",
    "extract_command.baml": "class Command{\n    command string\n}\n\nclass ChatResponse{\n  query_response string\n}\n\nfunction GenerateChatResponseOpenAI(terminal_history: string, user_input: string) -> ChatResponse{\n  client OpenAIClient\n  prompt #\"\n    You are a helpful linux terminal assistant.\n    Examine any provided terminal history and the user's request, then output a response to the user's query. Return the response STRICTLY according to the format provided below.\n\n    Terminal History:\n    ---\n    {{ terminal_history }}\n    ---\n\n    User Input:\n    ---\n    {{ user_input }}\n    ---\n\n    {# special macro to print the output instructions. #}\n    {{ ctx.output_format }}\n\n    JSON:\n  \"#\n}\n\n\n\n\nfunction GenerateCommandOpenAI(terminal_history: string, user_input: string) -> Command{\n  // see ollama-clients.baml\n  client OpenAIClient\n\n\n  // The prompt uses Jinja syntax. Change the models or this text and watch the prompt preview change!\n  prompt #\"\n    Examine any provided terminal history and the user's request, then output a shell command according to the following schema. You are STRICTYLY FORBIDDEN from returning anything else other than the command that will fulfil the user's request. \n\n    Terminal History:\n    ---\n    {{ terminal_history }}\n    ---\n\n    User Input:\n    ---\n    {{ user_input }}\n    ---\n\n    {# special macro to print the output instructions. #}\n    {{ ctx.output_format }}\n\n    JSON:\n  \"#\n}\n\n\nfunction GenerateChatResponseGemeni(terminal_history: string, user_input: string) -> ChatResponse{\n  client GeminiClient\n  prompt #\"\n    You are a helpful linux terminal assistant.\n    Examine any provided terminal history and the user's request, then output a response to the user's query. Return the response STRICTLY according to the format provided below.\n\n    Terminal History:\n    ---\n    {{ terminal_history }}\n    ---\n\n    User Input:\n    ---\n    {{ user_input }}\n    ---\n\n    {# special macro to print the output instructions. #}\n    {{ ctx.output_format }}\n\n    JSON:\n  \"#\n}\n\n\n\n\nfunction GenerateCommandGemeni(terminal_history: string, user_input: string) -> Command{\n  // see ollama-clients.baml\n  client GeminiClient\n\n\n  // The prompt uses Jinja syntax. Change the models or this text and watch the prompt preview change!\n  prompt #\"\n    Examine any provided terminal history and the user's request, then output a shell command according to the following schema. You are STRICTYLY FORBIDDEN from returning anything else other than the command that will fulfil the user's request. \n\n    Terminal History:\n    ---\n    {{ terminal_history }}\n    ---\n\n    User Input:\n    ---\n    {{ user_input }}\n    ---\n\n    {# special macro to print the output instructions. #}\n    {{ ctx.output_format }}\n\n    JSON:\n  \"#\n}\n\ntest TestName {\n  functions [GenerateCommandOpenAI]\n  args {\n    terminal_history #\"\n     \n    \"#\n    user_input #\"\n      say hello\n    \"#\n  }\n}\n",
    "generator.baml": "generator target {\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // What interface you prefer to use for the generated code (sync/async)\n    // Both are generated regardless of the choice, just modifies what is exported\n    // at the top level\n    default_client_mode \"sync\"\n\n    // Version of runtime to generate code for (should match installed baml-py version)\n    version \"0.201.0\"\n}\n",
}

def get_baml_files():
    return _file_map