# TestTeller Agent Configuration Example
# Copy this file to .env and update the values as needed

# =============================================================================
# LLM PROVIDER CONFIGURATION (Required: Choose one)
# =============================================================================
# Available providers: gemini, openai, claude, llama
LLM_PROVIDER=gemini

# -----------------------------------------------------------------------------
# Google Gemini Configuration (Required for Gemini provider)
# -----------------------------------------------------------------------------
# Get your API key from: https://aistudio.google.com/
GOOGLE_API_KEY=your_gemini_api_key_here
GEMINI_EMBEDDING_MODEL=text-embedding-004
GEMINI_GENERATION_MODEL=gemini-2.0-flash

# -----------------------------------------------------------------------------
# OpenAI Configuration (Required for OpenAI provider, also needed for Claude embeddings)
# -----------------------------------------------------------------------------
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
OPENAI_GENERATION_MODEL=gpt-4o-mini

# -----------------------------------------------------------------------------
# Anthropic Claude Configuration (Required for Claude provider)
# -----------------------------------------------------------------------------
# Get your API key from: https://console.anthropic.com/
# Note: Claude also requires OPENAI_API_KEY for embeddings
CLAUDE_API_KEY=your_claude_api_key_here
CLAUDE_GENERATION_MODEL=claude-3-5-haiku-20241022

# -----------------------------------------------------------------------------
# Llama/Ollama Configuration (Required for Llama provider - local models)
# -----------------------------------------------------------------------------
# Install Ollama from: https://ollama.ai/
# No API key required - runs locally
LLAMA_EMBEDDING_MODEL=llama3.2:1b
LLAMA_GENERATION_MODEL=llama3.2:3b

# OLLAMA_BASE_URL configuration depends on where Ollama is running:
# 1. Ollama on Docker host machine (default): http://host.docker.internal:11434
# 2. Ollama in docker-compose.yml service: http://ollama:11434
# 3. Ollama on remote network: http://<IP_OR_HOSTNAME>:11434
# 4. For local development (not Docker): http://localhost:11434
OLLAMA_BASE_URL=http://host.docker.internal:11434

# =============================================================================
# OPTIONAL CONFIGURATIONS
# =============================================================================

# -----------------------------------------------------------------------------
# GitHub Integration (Optional)
# -----------------------------------------------------------------------------
# Personal Access Token for accessing private repositories
# Get from: https://github.com/settings/tokens
GITHUB_TOKEN=your_github_token_here

# -----------------------------------------------------------------------------
# Logging Configuration
# -----------------------------------------------------------------------------
LOG_LEVEL=ERROR
LOG_FORMAT=json

# -----------------------------------------------------------------------------
# ChromaDB Configuration
# -----------------------------------------------------------------------------
# Local ChromaDB settings
CHROMA_DB_HOST=localhost
CHROMA_DB_PORT=8000
CHROMA_DB_USE_REMOTE=false
CHROMA_DB_PERSIST_DIRECTORY=./chroma_data
DEFAULT_COLLECTION_NAME=testteller_collection

# -----------------------------------------------------------------------------
# Document Processing Configuration
# -----------------------------------------------------------------------------
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
CODE_EXTENSIONS=.py,.js,.ts,.java,.go,.rs,.cpp,.c,.cs,.rb,.php
TEMP_CLONE_DIR_BASE=./temp_cloned_repos

# -----------------------------------------------------------------------------
# Output Configuration
# -----------------------------------------------------------------------------
OUTPUT_FILE_PATH=./testteller_generated_tests/testteller-testcases.md

# Test case output format (md, pdf, docx)
TEST_OUTPUT_FORMAT=pdf

# -----------------------------------------------------------------------------
# Test Automation Configuration
# -----------------------------------------------------------------------------
# Default settings for test automation generation
AUTOMATION_OUTPUT_DIR=./testteller_automated_tests
AUTOMATION_LANGUAGE=python
AUTOMATION_FRAMEWORK=pytest

# Base URL for testing (used in generated tests)
BASE_URL=http://localhost:8000

# Test execution timeout (in milliseconds)
TEST_TIMEOUT=30000

# -----------------------------------------------------------------------------
# Dual-Feedback RAG Architecture Configuration (NEW!)
# -----------------------------------------------------------------------------
# Enable automatic storage of high-quality generated test cases
ENABLE_TEST_CASE_FEEDBACK=true

# Minimum quality score (0.0-1.0) for storing test cases in vector store
MIN_QUALITY_SCORE_FOR_STORAGE=0.7

# Maximum number of generated test cases to store in vector store
MAX_GENERATED_TESTS_TO_STORE=1000

# Days to retain generated test cases in vector store
GENERATED_TEST_RETENTION_DAYS=90

# -----------------------------------------------------------------------------
# RAG-Enhanced Automator Agent Configuration (NEW!)
# -----------------------------------------------------------------------------
# Number of context documents to retrieve for automation context discovery
AUTOMATOR_NUM_CONTEXT_DOCS=5

# Default programming language for automation
AUTOMATOR_DEFAULT_LANGUAGE=python

# Default test framework for automation
AUTOMATOR_DEFAULT_FRAMEWORK=pytest

# AI enhancement for generated tests (true/false)
AUTOMATION_ENHANCE_BY_DEFAULT=true

# Supported Languages and Frameworks:
# - Python: pytest, unittest, playwright, cucumber
# - JavaScript: jest, mocha, playwright, cypress, cucumber  
# - TypeScript: jest, mocha, playwright, cypress, cucumber
# - Java: junit5, junit4, testng, playwright, karate, cucumber

# Framework-specific configurations:
# For Playwright (Python/JS/TS):
# PLAYWRIGHT_BROWSER=chromium  # Options: chromium, firefox, webkit
# PLAYWRIGHT_HEADLESS=true

# For Selenium (when used):
# SELENIUM_BROWSER=chrome      # Options: chrome, firefox, safari, edge
# SELENIUM_HEADLESS=true

# For API testing:
# API_BASE_URL=http://localhost:8000/api
# API_TIMEOUT=10000

# For database testing:
# DB_CONNECTION_STRING=postgresql://user:pass@localhost:5432/testdb
# DB_TEST_SCHEMA=test_schema

# -----------------------------------------------------------------------------
# API Retry Configuration
# -----------------------------------------------------------------------------
API_RETRY_ATTEMPTS=3
API_RETRY_WAIT_SECONDS=2

# =============================================================================
# PROVIDER-SPECIFIC NOTES
# =============================================================================

# Gemini (Default):
# - Fast and cost-effective
# - Only requires GOOGLE_API_KEY
# - Good for general-purpose testing

# OpenAI:
# - High-quality responses
# - Only requires OPENAI_API_KEY
# - Best for complex scenarios

# Claude:
# - Advanced reasoning capabilities
# - Requires both CLAUDE_API_KEY and OPENAI_API_KEY (for embeddings)
# - Excellent for safety-critical applications

# Llama (Local):
# - Complete privacy, runs locally
# - No API key required
# - Requires Ollama installation and model downloads:
#   curl -fsSL https://ollama.ai/install.sh | sh
#   ollama serve
#   ollama pull llama3.2:3b
#   ollama pull llama3.2:1b
#
# OLLAMA DEPLOYMENT OPTIONS:
# 
# Option 1: Ollama on host machine (recommended for development)
#   - Install Ollama on your host machine
#   - Set OLLAMA_BASE_URL=http://host.docker.internal:11434
#
# Option 2: Ollama in Docker Compose (recommended for production)
#   - Uncomment the ollama service in docker-compose.yml
#   - Set OLLAMA_BASE_URL=http://ollama:11434
#   - Run: docker-compose up -d
#
# Option 3: Ollama on remote server
#   - Set OLLAMA_BASE_URL=http://your-ollama-server.com:11434
#   - Ensure the remote server is accessible from Docker network
#
# Option 4: Local development without Docker
#   - Set OLLAMA_BASE_URL=http://localhost:11434
