#! /usr/bin/env python3

# Import the fastest jsons library available
try:
    import orjson as json
except ImportError:
    try:
        import ujson as json
    except ImportError:
        try:
            import simplejson as json
        except ImportError:
            import json

import argparse
import sys
from collections import namedtuple
import pathlib
import lzma


# Version of the profiling files generated by preCICE
RUN_FILE_VERSION: int = 2
# Version of the merged profiling data generated by merge
MERGED_FILE_VERSION: int = 1


def mergedDict(dict1, dict2):
    merged = dict1.copy()
    merged.update(dict2)
    return merged


def warning(message, filename=None):
    prefix = f"{str(filename)}: " if filename else ""
    print(f"{prefix}\033[33mwarning:\033[0m {message}")


def readJSON(filename: pathlib.Path):
    assert filename.suffix == ".json"
    content = filename.read_text()
    try:
        return json.loads(content)  # try direct
    except:
        warning("File damaged. Attempting to terminate truncated event file.", filename)
        content += "]}"
        try:
            return json.loads(content)  # try terminated
        except:
            warning("Unable to load critically damaged file.", filename)
            return {}  # give up


def expandTXTRecord(s: str):
    parts = s[1:].rstrip().split(":")
    match s[0]:
        case "N":
            eid, name = int(parts[0]), parts[1]
            return {"et": "n", "eid": eid, "en": name}
        case "B":
            eid, ts = map(int, parts)
            return {"et": "b", "eid": eid, "ts": ts}
        case "E":
            eid, ts = map(int, parts)
            return {"et": "e", "eid": eid, "ts": ts}
        case "D":
            eid, ts, dn, dv = map(int, parts)
            return {"et": "d", "eid": eid, "ts": ts, "dn": dn, "dv": dv}
    assert False


def readTXT(filename: pathlib.Path):
    with filename.open("rb") as file:
        meta = json.loads(file.readline())
        assert "compression" in meta
        stream = lzma.LZMAFile(file) if meta["compression"] else file
        events = [expandTXTRecord(line.decode()) for line in stream]
        return {
            "meta": meta,
            "events": events,
        }


def readTimestamp(filename: pathlib.Path):
    if filename.suffix == ".json":
        meta = readJSON(filename)["meta"]
        return int(meta["unix_us"])

    assert filename.suffix == ".txt"
    with filename.open("rb") as file:
        meta = json.loads(file.readline())
        return int(meta["unix_us"])


def alignEvents(events):
    """Aligns passed events of multiple ranks and or participants.
    All ranks of a participant align at initialization, ensured by a barrier in preCICE.
    Primary ranks of all participants align after successfully establishing primary connections.
    """
    assert "events" in events and "eventDict" in events, "Not a loaded event file"
    assert len(events.get("events")) > 0, "No participants in the file"
    participants = events["events"].keys()
    grouped = events["events"]

    # Align ranks of each participant
    for participant, ranks in grouped.items():
        if len(ranks) == 1:
            continue
        print(f"Aligning {len(ranks)} ranks of {participant}")
        intraSyncID = [
            id
            for id, name in events["eventDict"].items()
            if "com.initializeIntraCom" in name
        ][0]
        syncs = {
            rank: e["ts"] + e["dur"]
            for rank, data in ranks.items()
            for e in data["events"]
            if e["eid"] == intraSyncID
        }

        firstSync = min(syncs.values())
        shifts = {rank: firstSync - tp for rank, tp in syncs.items()}

        for rank, data in ranks.items():
            for e in data["events"]:
                e["ts"] += shifts[rank]

    if len(grouped) == 1:
        return events

    # Align participants
    primaries = [name for name, ranks in events["events"].items() if 0 in ranks]

    for lonely in set(events["events"].keys()).difference(primaries):
        print(f"Cannot align {lonely} as event file of rank 0 is missing.")

    # Cannot align anything
    if len(primaries) == 1:
        return events

    # Find synchronization points
    syncEvents = [
        "m2n.acceptPrimaryRankConnection.",
        "m2n.requestPrimaryRankConnection.",
    ]
    # Event ID -> Remote name
    syncIDs = {
        id: name.rsplit(".", 1)[1]
        for id, name in events["eventDict"].items()
        if any([check in name for check in syncEvents])
    }
    syncs = {
        local: {
            remote: e["ts"] + e["dur"]
            for e in events["events"][local][0]["events"]
            for eid, remote in syncIDs.items()
            if eid == e["eid"]
        }
        for local in primaries
    }

    def hasSync(l, r):
        return (
            syncs.get(l)
            and syncs.get(l).get(r)
            and syncs.get(r)
            and syncs.get(r).get(l)
        )

    shifts = {
        (local, remote): syncs[local][remote] - syncs[remote][local]
        # all unique participant combinations
        for local in primaries
        for remote in primaries
        if local < remote
        if hasSync(local, remote)
    }
    for (local, remote), shift in shifts.items():
        print(f"Aligning {remote} ({shift}us) with {local}")
        for rank, data in events["events"][remote].items():
            data["meta"]["unix_us"] += shift
            for e in data["events"]:
                e["ts"] += shift

    return events


def groupEvents(events: [dict], initTime: int):

    # Expands event names
    def namedEvents():
        nameMap = {int(e["eid"]): e["en"] for e in events if e["et"] == "n"}
        for e in events:
            type = e["et"]
            if type != "n":
                e["eid"] = nameMap[e["eid"]]
                if type == "d":
                    e["dn"] = nameMap[e["dn"]]
                yield e

    completed = []
    active = {}  # name to event data
    stack = []

    for event in namedEvents():
        type = event["et"]

        name: str = event["eid"]
        assert isinstance(name, str)

        # Handle event starts
        if type == "b":
            # assert(name not in active.keys())
            if name in active.keys():
                print(f"Ignoring start of active event {name}")
            else:
                event["ts"] = int(event["ts"])
                fullName = "/".join(stack + [name])
                event["eid"] = fullName
                active[name] = event
                if name != "_GLOBAL":
                    stack.append(name)
        # Handle event stops
        elif type == "e":
            # assert(name in active.keys())
            if name not in active.keys():
                print(f"Ignoring end of inactive event {name}")
            else:
                begin = active[name]
                active.pop(name)
                begin["dur"] = int(event["ts"]) - begin["ts"]
                begin["ts"] = int(begin["ts"]) + initTime
                begin.pop("et")
                completed.append(begin)
                if name != "_GLOBAL":
                    assert (
                        stack[-1] == name
                    ), f"Expected to end event {name} but the currently active event is {stack[-1]}. Note that events need to follow a strict nesting and overlapping starts/stops are not permitted."
                    stack.pop()
        # Handle event data
        elif type == "d":
            if name not in active.keys():
                print(f"Dropping data event {name} as event isn't yet known.")
            else:
                d = active[name].get("data", {})
                dname = event["dn"]
                assert isinstance(dname, str)
                d[dname] = int(event["dv"])
                active[name]["data"] = d

    # Handle leftover events in case of a truncated input file
    if active:
        lastTS = min(map(lambda e: e["ts"] + e["dur"], completed))
        for event in active.values():
            name = event["eid"]  # This is a global id
            print(f"Truncating event without end {name}")
            begin = active[name]
            begin["ts"] = int(begin["ts"]) + initTime
            begin["dur"] = lastTS - begin["ts"]
            begin.pop("et")
            completed.append(begin)

    assert all((isinstance(e["eid"], str) for e in completed))

    return sorted(completed, key=lambda e: e["ts"])


def compressNames(events):
    allNames = set(
        (
            e["eid"]
            for participants in events.values()
            for ranks in participants.values()
            for rank in ranks.values()
            for e in ranks["events"]
        )
    )
    nameToId = {name: id for id, name in enumerate(allNames)}

    for p, ranks in events.items():
        for r, data in ranks.items():
            for e in data["events"]:
                name = e["eid"]
                assert isinstance(name, str)
                e["eid"] = nameToId[name]

    idToName = dict(zip(nameToId.values(), nameToId.keys()))
    return idToName


def loadProfilingOutputs(filenames: list[pathlib.Path]):
    # Load all jsons
    print("Loading event files")
    jsons = []
    for i, fn in enumerate(filenames):
        json = readJSON(fn) if fn.suffix == ".json" else readTXT(fn)

        # General checks
        if not json:
            warning(
                "The file is empty or was unable to be load and will be ignored.", fn
            )
            continue
        if "meta" not in json:
            warning("The file doesn't contain metadata and will be ignored.", fn)
            continue
        elif "events" not in json:
            warning("The file doesn't contain event data and will be ignored.", fn)
            continue
        else:
            version = json["meta"].get("file_version")
            if version is None:
                warning(
                    "The file doesn't contain a version (preCICE version v3.2 or earlier) and may be incompatible.",
                    fn,
                )
            elif version == 1:
                warning(
                    f"The file uses development version 1, upgrading to a newer preCICE version is highly recommended.",
                    fn,
                )
            elif version != RUN_FILE_VERSION:
                warning(
                    f"The file uses version {version}, which doesn't match the expected version {RUN_FILE_VERSION} and may be incompatible.",
                    fn,
                )

        jsons.append(json)

    if not jsons:
        print("No files loaded")
        sys.exit(1)

    # Grouping events
    print("Grouping events")
    events = {}
    for i, json in enumerate(jsons):
        name = json["meta"]["name"]
        rank = int(json["meta"]["rank"])
        unix_us = int(json["meta"]["unix_us"])
        events.setdefault(name, {})[rank] = {
            "meta": {
                "name": name,
                "rank": rank,
                "size": int(json["meta"]["size"]),
                "unix_us": unix_us,
                "tinit": json["meta"]["tinit"],
            },
            "events": groupEvents(json["events"], unix_us),
        }

    print("Compressing names")
    globalNameMap = compressNames(events)

    return {
        "file_version": MERGED_FILE_VERSION,
        "eventDict": globalNameMap,
        "events": events,
    }


def detectFiles(files: list[pathlib.Path]):
    def searchDir(directory: pathlib.Path):
        assert directory.is_dir()
        import re

        nameMatcher = r".+-\d+-\d+.(json|txt)"
        return [
            candidate
            for pattern in ("**/*.json", "**/*.txt")
            for candidate in path.rglob(pattern)
            if re.fullmatch(nameMatcher, candidate.name)
        ]

    resolved = []
    for path in files:
        if path.is_file():
            resolved.append(path)
            continue
        if path.is_dir():
            detected = searchDir(path)
            if len(detected) == 0:
                print(f"Nothing found in {path}")
            else:
                print(f"Found {len(detected)} files in {path}")
                resolved += detected
        else:
            print(f'Cannot interpret "{path}"')

    unique = list(set(resolved))
    if len(files) > 1:
        print(f"Found {len(unique)} profiling files in total")
    return unique


def findFilesOfLatestRun(name, sizes):
    assert len(sizes) > 1
    print(f"Found multiple runs for participant {name}")
    timestamps = []
    for size, ranks in sizes.items():
        assert len(ranks) > 0
        example = next(iter(ranks.values()))  # Get some file of this run
        timestamp = readTimestamp(example)
        timestamps.append((size, timestamp))

    # Find oldest size of newest timestamps
    size, _ = max(timestamps, key=lambda p: p[1])
    print(f"`-selected latest run of size {size}")

    return list(sizes[size].values())


def groupRuns(files: list[pathlib.Path]):
    PieceFile = namedtuple("PieceFile", ["name", "rank", "size", "filename"])

    def info(filename: pathlib.Path):
        parts = filename.stem.split("-")
        name = "-".join(parts[:-2])
        return PieceFile(name, int(parts[-2]), int(parts[-1]), filename)

    pieces = [info(filename) for filename in files]

    map = {}
    for n, r, s, fn in pieces:
        rankMap = map.setdefault(n, {}).setdefault(s, {})
        if r in rankMap:
            existing = rankMap[r]
            if existing.suffix == ".txt":
                warning(
                    f"Ignored new .json due to conflict with existing .txt '{existing}'",
                    fn,
                )
            else:
                warning(
                    f"Newer .txt replaces previously found .json file '{existing}'", fn
                )
                rankMap.update({r: fn})
        else:
            rankMap.update({r: fn})

    return map


def sanitizeFiles(files: list[pathlib.Path]):
    map = groupRuns(files)
    filesToLoad = []
    for name, sizes in map.items():
        if len(sizes) == 1:
            print(f"Found a single run for participant {name}")
            filesToLoad += [
                filename for _, ranks in sizes.items() for filename in ranks.values()
            ]
            continue

        filesToLoad = findFilesOfLatestRun(name, sizes)
    return filesToLoad


def runMerge(ns):
    return mergeCommand(ns.files, ns.output, not ns.no_align)


def mergeCommand(files, outfile, align):
    resolved = detectFiles(files)
    sanitized = sanitizeFiles(resolved)
    merged = loadProfilingOutputs(sanitized)

    if align:
        merged = alignEvents(merged)

    print(f"Writing to {outfile}")
    if json.__name__ == "orjson":
        data = json.dumps(merged, option=json.OPT_NON_STR_KEYS)
        outfile.write_bytes(data)
    else:
        data = json.dumps(merged)
        outfile.write_text(data)

    return 0


def makeMergeParser(add_help: bool = True):
    merge_help = "Merges preCICE profiling output files to a single file used by the other commands."
    merge = argparse.ArgumentParser(description=merge_help, add_help=add_help)
    merge.add_argument(
        "files",
        type=pathlib.Path,
        nargs="*",
        help="The profiling files to process, directories to search, or nothing to autodetect",
        default=["."],
    )
    merge.add_argument(
        "-o",
        "--output",
        type=pathlib.Path,
        default="profiling.json",
        help="The resulting profiling file.",
    )
    merge.add_argument(
        "-n", "--no-align", action="store_true", help="Don't align participants?"
    )
    return merge


def main():
    parser = makeMergeParser()
    ns = parser.parse_args()
    return runMerge(ns)


if __name__ == "__main__":
    sys.exit(main())
