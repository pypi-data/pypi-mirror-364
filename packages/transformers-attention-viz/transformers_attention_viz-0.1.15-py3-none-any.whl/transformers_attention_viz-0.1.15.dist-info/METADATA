Metadata-Version: 2.4
Name: transformers-attention-viz
Version: 0.1.15
Summary: Interactive attention visualization for multi-modal transformer models
Home-page: https://github.com/sisird864/transformers-attention-viz
Author: Sisir Doppalapudi
Author-email: Your Name <your.email@example.com>
License: MIT
Project-URL: Homepage, https://github.com/YOUR_USERNAME/transformers-attention-viz
Project-URL: Bug Tracker, https://github.com/YOUR_USERNAME/transformers-attention-viz/issues
Project-URL: Documentation, https://transformers-attention-viz.readthedocs.io
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Scientific/Engineering :: Visualization
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: torch>=1.9.0
Requires-Dist: transformers>=4.30.0
Requires-Dist: numpy>=1.19.0
Requires-Dist: matplotlib>=3.3.0
Requires-Dist: seaborn>=0.11.0
Requires-Dist: gradio>=3.0.0
Requires-Dist: pillow>=8.0.0
Requires-Dist: pandas>=1.3.0
Requires-Dist: scipy>=1.7.0
Requires-Dist: tqdm>=4.62.0
Requires-Dist: networkx>=2.6.0
Provides-Extra: dev
Requires-Dist: pytest>=6.0; extra == "dev"
Requires-Dist: pytest-cov>=2.0; extra == "dev"
Requires-Dist: black>=22.0; extra == "dev"
Requires-Dist: flake8>=4.0; extra == "dev"
Requires-Dist: isort>=5.0; extra == "dev"
Requires-Dist: mypy>=0.900; extra == "dev"
Requires-Dist: sphinx>=4.0; extra == "dev"
Requires-Dist: sphinx-rtd-theme>=1.0; extra == "dev"
Requires-Dist: nbsphinx>=0.8.0; extra == "dev"
Dynamic: author
Dynamic: home-page
Dynamic: license-file
Dynamic: requires-python

# üîç Transformers Attention Viz

Interactive attention visualization for multi-modal transformer models

[![PyPI version](https://badge.fury.io/py/transformers-attention-viz.svg)](https://badge.fury.io/py/transformers-attention-viz)
[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Transformers](https://img.shields.io/badge/transformers-4.30+-orange.svg)](https://github.com/huggingface/transformers)

**Visualize and understand cross-modal attention in vision-language models like BLIP and CLIP**

## üöÄ Try it Now!

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sisird864/transformers-attention-viz/blob/master/demo.ipynb) **Full Demo** - Explore all features

## üéØ Features

- üìä **Cross-Modal Attention**: Visualize how text tokens attend to image regions in BLIP
- üîÑ **Multi-Layer Support**: Analyze attention patterns across all transformer layers
- üìà **Attention Statistics**: Compute entropy, concentration, and top attended regions
- üé® **Publication Ready**: Export high-quality figures for papers (PNG, PDF, SVG)
- üöÄ **Easy Integration**: Works seamlessly with HuggingFace models
- üñ•Ô∏è **Interactive Dashboard**: Explore attention patterns in real-time (local only)

## üì¶ Installation

```bash
pip install transformers-attention-viz
```

## üí° Basic Usage

```python
from transformers_attention_viz import AttentionVisualizer
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image

# Load BLIP model (supports cross-modal attention)
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")

# Create visualizer
visualizer = AttentionVisualizer(model, processor)

# Load your image
image = Image.open("cat.jpg")
text = "a fluffy orange cat"

# Visualize cross-modal attention
viz = visualizer.visualize(
    image=image,
    text=text,
    visualization_type="heatmap",
    attention_type="cross"  # text -> image attention
)
viz.show()

# Get attention statistics
stats = visualizer.get_attention_stats(image, text, attention_type="cross")
print(f"Average entropy: {stats['entropy'].mean():.3f}")
print(f"Top attended regions: {stats['top_tokens'][:3]}")
```

## üì∏ Example Visualizations

### Cross-Modal Attention (BLIP)
Each text token gets its own heatmap showing attention to image patches:

```python
# Visualizing "a fluffy orange cat sitting on a surface"
# Generates separate heatmaps for each token
```
![BLIP Cross-Modal Attention](docs/images/blip_cross_attention_example.png)

### Attention Statistics
```python
# Example output:
Average entropy: 4.251
Top attended regions: 
  1. Patch_(24,4): 0.0429
  2. Patch_(20,1): 0.0395
  3. Patch_(23,4): 0.0391
```

## üõ†Ô∏è Advanced Features

### Multi-Layer Analysis
```python
# Visualize attention at different layers
viz = visualizer.visualize(
    image=image,
    text=text,
    layer_indices=[0, 5, 11],  # First, middle, last
    attention_type="cross"
)
```

### Export for Publications
```python
# Save high-quality figures
viz.save("attention_figure.png", dpi=300)  # For papers
viz.save("attention_figure.pdf")           # For LaTeX
viz.save("attention_figure.svg")           # For web
```

### Interactive Dashboard
```python
from transformers_attention_viz import launch_dashboard

# Launch interactive exploration tool (requires local environment)
launch_dashboard(model, processor)
# Opens at http://localhost:7860
```

## ü§ñ Supported Models

| Model | Cross-Modal Attention | Self-Attention | Status |
|-------|---------------------|----------------|---------|
| BLIP | ‚úÖ | ‚úÖ | Fully Supported |
| CLIP | ‚ùå | ‚úÖ | Self-Attention Only |
| BLIP-2 | ‚úÖ | ‚úÖ | Coming Soon |
| Flamingo | ‚úÖ | ‚úÖ | In Development |

## üìä Understanding the Visualizations

- **BLIP Cross-Modal Attention**: Shows how each text token attends to the 24√ó24 grid of image patches
- **Attention Entropy**: Lower entropy indicates more focused attention
- **Diffuse Attention**: BLIP often shows uniform attention, especially on simple images - this is normal behavior

## ü§ù Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

```bash
# Clone the repo
git clone https://github.com/sisird864/transformers-attention-viz.git
cd transformers-attention-viz

# Install in development mode
pip install -e ".[dev]"

# Run tests
pytest tests/
```

## üìñ Citation

If you use this tool in your research, please cite:

```bibtex
@software{transformers-attention-viz,
  author = {Sisir Doppalapudi},
  title = {Transformers Attention Viz: Interactive Attention Visualization for Multi-Modal Transformers},
  year = {2024},
  publisher = {GitHub},
  url = {https://github.com/sisird864/transformers-attention-viz}
}
```

## üöß Known Limitations

- **v0.1.15**:
  - Individual attention head visualization (aggregate_heads=False) not fully supported
  - Flow visualization has dimension compatibility issues with BLIP
  - BLIP text self-attention not captured (cross-modal and vision self-attention work fine)

## üõ§Ô∏è Roadmap

- [ ] Full support for individual attention heads
- [ ] Fix flow visualization for BLIP
- [ ] Add BLIP-2 support
- [ ] Add LLaVA support
- [ ] 3D attention visualization
- [ ] Attention pattern export to TensorBoard
- [ ] Real-time video attention tracking

## üìÑ License

MIT License - see [LICENSE](LICENSE) for details.

## üôè Acknowledgments

- HuggingFace team for the amazing Transformers library
- Salesforce Research for BLIP
- OpenAI for CLIP
- All contributors and users of this tool

## ‚≠ê Support

If you find this tool useful, please consider:
- Starring this repository
- Sharing it with colleagues
- Contributing improvements
- Citing it in your research

---

Made with ‚ù§Ô∏è for the ML research community
