Metadata-Version: 2.4
Name: nahiarhdNLP
Version: 1.2.0
Summary: Advanced Indonesian Natural Language Processing Library
Author-email: Raihan Hidayatullah Djunaedi <raihanhd.dev@gmail.com>
License: MIT
Project-URL: Homepage, https://example.com
Project-URL: Documentation, https://example.com
Project-URL: Repository, https://github.com/raihanhd12/nahiarhdNLP
Project-URL: Issues, https://github.com/raihanhd12/nahiarhdNLP/issues
Keywords: nlp,indonesian,natural-language-processing,text-processing,bahasa-indonesia
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Text Processing :: Linguistic
Classifier: Operating System :: OS Independent
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: pandas>=1.3.0
Requires-Dist: fsspec>=2021.10.1
Requires-Dist: huggingface_hub>=0.10.0
Requires-Dist: sastrawi>=1.0.1
Requires-Dist: datasets>=2.0.0
Requires-Dist: rich>=12.0.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: black>=22.0.0; extra == "dev"
Requires-Dist: isort>=5.10.0; extra == "dev"
Requires-Dist: flake8>=4.0.0; extra == "dev"
Requires-Dist: mypy>=0.991; extra == "dev"
Requires-Dist: pre-commit>=2.17.0; extra == "dev"
Provides-Extra: docs
Requires-Dist: sphinx>=4.0.0; extra == "docs"
Requires-Dist: sphinx-rtd-theme>=1.0.0; extra == "docs"
Requires-Dist: myst-parser>=0.17.0; extra == "docs"
Dynamic: license-file

# nahiarhdNLP - Advanced Indonesian Natural Language Processing Library

Advanced Indonesian Natural Language Processing Library dengan fitur preprocessing teks, normalisasi slang, konversi emoji, koreksi ejaan, dan banyak lagi.

## ğŸš€ Instalasi

```bash
pip install nahiarhdNLP
```

## ğŸ“¦ Import Library

```python
# Import module preprocessing
from nahiarhdNLP import preprocessing

# Import fungsi spesifik
from nahiarhdNLP.preprocessing import (
    # Fungsi pembersihan dasar
    remove_html, remove_url, remove_mentions, remove_hashtags,
    remove_numbers, remove_punctuation, remove_extra_spaces,
    remove_special_chars, remove_whitespace, to_lowercase,
    # Fungsi normalisasi dan koreksi
    replace_slang, replace_word_elongation, correct_spelling,
    # Fungsi emoji
    emoji_to_words, words_to_emoji,
    # Fungsi linguistic
    remove_stopwords, stem_text, tokenize,
    # Fungsi all-in-one
    clean_text,
    # Fungsi pipeline
    pipeline, preprocess
)

# Import kelas untuk penggunaan advanced
from nahiarhdNLP.preprocessing import (
    TextCleaner, SpellCorrector, StopwordRemover,
    Stemmer, EmojiConverter, Tokenizer, Pipeline
)

# Import dataset loader (dua cara)
from nahiarhdNLP.datasets import DatasetLoader
# atau
from nahiarhdNLP.datasets.loaders import DatasetLoader
```

## ğŸ“‹ Contoh Penggunaan

### 1. ğŸ§¹ TextCleaner - Membersihkan Teks

```python
from nahiarhdNLP.preprocessing import TextCleaner

cleaner = TextCleaner()

# Membersihkan URL
url_text = "kunjungi https://google.com sekarang!"
clean_result = cleaner.clean_urls(url_text)
print(clean_result)
# Output: "kunjungi  sekarang!"

# Membersihkan mentions
mention_text = "Halo @user123 apa kabar?"
clean_result = cleaner.clean_mentions(mention_text)
print(clean_result)
# Output: "Halo  apa kabar?"

# Membersihkan teks secara menyeluruh
messy_text = "Halooo!!! @user #trending https://example.com ğŸ˜€"
clean_result = cleaner.clean(messy_text)
print(clean_result)
# Output: teks yang sudah dibersihkan
```

### 2. âœï¸ SpellCorrector - Koreksi Ejaan & Normalisasi Slang

```python
from nahiarhdNLP.preprocessing import SpellCorrector

spell = SpellCorrector()

# Koreksi kata salah eja
word = "mencri"
corrected = spell.correct_word(word)
print(corrected)
# Output: "mencuri"

# Koreksi kalimat lengkap (termasuk normalisasi slang)
sentence = "gw lg mencri informsi"
corrected = spell.correct_sentence(sentence)
print(corrected)
# Output: "saya lagi mencuri informasi"
```

### 3. ğŸš« StopwordRemover - Menghapus Stopwords

```python
from nahiarhdNLP.preprocessing import StopwordRemover

stopword = StopwordRemover()
stopword._load_data()  # Load dataset stopwords

# Menghapus stopwords
text = "saya suka makan nasi goreng"
result = stopword.remove_stopwords(text)
print(result)
# Output: "suka makan nasi goreng"

# Menambah custom stopwords
stopword.add_custom_stopwords(["adalah", "akan"])
```

### 4. ğŸ˜€ EmojiConverter - Konversi Emoji

```python
from nahiarhdNLP.preprocessing import EmojiConverter

emoji = EmojiConverter()
emoji._load_data()  # Load dataset emoji

# Emoji ke teks
emoji_text = "ğŸ˜€ ğŸ˜‚ ğŸ˜"
text_result = emoji.emoji_to_text_convert(emoji_text)
print(text_result)
# Output: "wajah_gembira wajah_tertawa wajah_bercinta"

# Teks ke emoji
text = "wajah_gembira"
emoji_result = emoji.text_to_emoji_convert(text)
print(emoji_result)
# Output: "ğŸ˜€"
```

### 5. ğŸ”ª Tokenizer - Tokenisasi

```python
from nahiarhdNLP.preprocessing import Tokenizer

tokenizer = Tokenizer()

# Tokenisasi teks
text = "ini contoh tokenisasi"
tokens = tokenizer.tokenize(text)
print(tokens)
# Output: ['ini', 'contoh', 'tokenisasi']
```

### 6. ğŸŒ¿ Stemmer - Stemming (Memerlukan Sastrawi)

```python
from nahiarhdNLP.preprocessing import Stemmer

try:
    stemmer = Stemmer()
    text = "bermain-main dengan senang"
    result = stemmer.stem(text)
    print(result)
    # Output: "main main dengan senang"
except ImportError:
    print("Install Sastrawi dengan: pip install Sastrawi")
```

### 7. ğŸ› ï¸ Fungsi Individual

```python
from nahiarhdNLP.preprocessing import (
    # Fungsi pembersihan dasar
    remove_html, remove_url, remove_mentions, remove_hashtags,
    remove_numbers, remove_punctuation, remove_extra_spaces,
    remove_special_chars, remove_whitespace, to_lowercase,
    # Fungsi normalisasi dan koreksi
    replace_slang, replace_word_elongation, correct_spelling,
    # Fungsi emoji
    emoji_to_words, words_to_emoji,
    # Fungsi linguistic
    remove_stopwords, stem_text, tokenize,
    # Fungsi all-in-one
    clean_text
)

# ğŸ§¹ FUNGSI PEMBERSIHAN DASAR

# Menghapus HTML tags
html_text = "website <a href='https://google.com'>google</a>"
clean_result = remove_html(html_text)
print(clean_result)
# Output: "website google"

# Menghapus URL
url_text = "kunjungi https://google.com sekarang!"
clean_result = remove_url(url_text)
print(clean_result)
# Output: "kunjungi sekarang!"

# Menghapus mentions (@username)
mention_text = "Halo @user123 dan @admin apa kabar?"
clean_result = remove_mentions(mention_text)
print(clean_result)
# Output: "Halo dan apa kabar?"

# Menghapus hashtags (#tag)
hashtag_text = "Hari ini #senin #libur #weekend"
clean_result = remove_hashtags(hashtag_text)
print(clean_result)
# Output: "Hari ini"

# Menghapus angka
number_text = "Saya berumur 25 tahun dan punya 3 anak"
clean_result = remove_numbers(number_text)
print(clean_result)
# Output: "Saya berumur tahun dan punya anak"

# Menghapus tanda baca
punct_text = "Halo, apa kabar?! Semoga sehat selalu..."
clean_result = remove_punctuation(punct_text)
print(clean_result)
# Output: "Halo apa kabar Semoga sehat selalu"

# Menghapus spasi berlebih
space_text = "Halo    dunia   yang    indah"
clean_result = remove_extra_spaces(space_text)
print(clean_result)
# Output: "Halo dunia yang indah"

# Menghapus karakter khusus
special_text = "Halo @#$%^&*() dunia!!!"
clean_result = remove_special_chars(special_text)
print(clean_result)
# Output: "Halo dunia!!!"

# Membersihkan karakter whitespace (tab, newline, dll)
whitespace_text = "Halo\n\tdunia\r\nyang indah"
clean_result = remove_whitespace(whitespace_text)
print(clean_result)
# Output: "Halo dunia yang indah"

# Konversi ke huruf kecil
upper_text = "HALO Dunia Yang INDAH"
clean_result = to_lowercase(upper_text)
print(clean_result)
# Output: "halo dunia yang indah"

# âœ¨ FUNGSI NORMALISASI DAN KOREKSI

# Normalisasi slang (menggunakan SpellCorrector)
slang_text = "emg siapa yg nanya?"
normal_text = replace_slang(slang_text)
print(normal_text)
# Output: "memang siapa yang bertanya?"

# Mengatasi perpanjangan kata (word elongation)
elongation_text = "kenapaaa bangettt???"
clean_result = replace_word_elongation(elongation_text)
print(clean_result)
# Output: "kenapa banget??"

# Koreksi ejaan
spell_text = "saya mencri informsi pnting"
corrected = correct_spelling(spell_text)
print(corrected)
# Output: "saya mencuri informasi penting"

# ğŸ˜€ FUNGSI EMOJI

# Konversi emoji ke kata
emoji_text = "ğŸ˜€ ğŸ˜‚ ğŸ˜"
text_result = emoji_to_words(emoji_text)
print(text_result)
# Output: "wajah_gembira wajah_tertawa wajah_bercinta"

# Konversi kata ke emoji
text_to_emoji = "wajah_gembira wajah_sedih"
emoji_result = words_to_emoji(text_to_emoji)
print(emoji_result)
# Output: "ğŸ˜€ ğŸ˜¢"

# ğŸ”¬ FUNGSI LINGUISTIC

# Menghapus stopwords
stopword_text = "saya sangat suka sekali makan nasi goreng"
clean_result = remove_stopwords(stopword_text)
print(clean_result)
# Output: "suka makan nasi goreng"

# Stemming teks (memerlukan Sastrawi)
try:
    stem_text_input = "bermain-main dengan gembira"
    stemmed = stem_text(stem_text_input)
    print(stemmed)
    # Output: "main main dengan gembira"
except ImportError:
    print("Install Sastrawi: pip install Sastrawi")

# Tokenisasi teks
tokenize_text = "Saya suka makan nasi goreng"
tokens = tokenize(tokenize_text)
print(tokens)
# Output: ['Saya', 'suka', 'makan', 'nasi', 'goreng']

# ğŸ¯ FUNGSI ALL-IN-ONE

# Cleaning menyeluruh (gabungan semua fungsi cleaning)
messy_text = "Halooo!!! @user #trending https://example.com ğŸ˜€ 123"
cleaned = clean_text(messy_text)
print(cleaned)
# Output: "haloo" (teks yang sudah dibersihkan menyeluruh)
```

### 8. ğŸ”€ Pipeline - Preprocessing Sekaligus

Pipeline memungkinkan Anda menjalankan beberapa fungsi preprocessing sekaligus dengan konfigurasi yang fleksibel.

```python
from nahiarhdNLP.preprocessing import Pipeline, pipeline, preprocess

# ğŸ—ï¸ MENGGUNAKAN KELAS PIPELINE

# Buat pipeline dengan konfigurasi default
pipe = Pipeline()
result = pipe.process("Halooo @user https://example.com gw lg nyari info ğŸ˜€")
print(result)
# Output: "halo saya lagi mencari informasi"

# Buat pipeline dengan konfigurasi custom
config = {
    'remove_html': True,
    'remove_url': True,
    'remove_mentions': True,
    'remove_hashtags': True,
    'normalize_slang': True,
    'correct_spelling': True,
    'remove_stopwords': True,
    'stem_text': False,
    'to_lowercase': True,
    'tokenize': False
}

pipe = Pipeline(config)
messy_text = "gw lg mencri informsi pnting @user #trending https://example.com"
result = pipe.process(messy_text)
print(result)
# Output: "saya mencari informasi penting"

# Update konfigurasi pipeline
pipe.update_config({'tokenize': True, 'remove_stopwords': False})
tokens = pipe.process("Saya suka makan nasi goreng")
print(tokens)
# Output: ['saya', 'suka', 'makan', 'nasi', 'goreng']

# Lihat konfigurasi aktif
print("Konfigurasi:", pipe.get_config())
print("Steps aktif:", pipe.get_enabled_steps())

# Reset ke konfigurasi default
pipe.reset_config()

# ğŸš€ FUNGSI PIPELINE (SIMPLE)

# Gunakan konfigurasi default
result = pipeline("Halooo @user https://example.com gw lg nyari info ğŸ˜€")
print(result)
# Output: "halo saya lagi mencari informasi"

# Gunakan konfigurasi custom
config = {
    'remove_url': True,
    'normalize_slang': True,
    'to_lowercase': True,
    'remove_mentions': True
}
result = pipeline("Gw lg browsing https://google.com @admin", config)
print(result)
# Output: "saya lagi browsing"

# Dengan tokenisasi
config = {'normalize_slang': True, 'tokenize': True}
tokens = pipeline("gw suka makan nasi", config)
print(tokens)
# Output: ['saya', 'suka', 'makan', 'nasi']

# âš™ï¸ FUNGSI PREPROCESS (DETAIL CONTROL)

# Preprocess dengan parameter eksplisit
result = preprocess(
    "Halooo @user123 #trending https://example.com gw lg nyari info ğŸ˜€!!!",
    remove_url=True,
    remove_mentions=True,
    remove_hashtags=True,
    remove_punctuation=True,
    normalize_slang=True,
    correct_spelling=True,
    to_lowercase=True,
    remove_stopwords=True
)
print(result)
# Output: "halo saya mencari informasi"

# Preprocess dengan tokenisasi
tokens = preprocess(
    "Saya suka makan nasi goreng pedas",
    remove_stopwords=True,
    tokenize=True
)
print(tokens)
# Output: ['suka', 'makan', 'nasi', 'goreng', 'pedas']

# Preprocess minimal (hanya cleaning dasar)
result = preprocess(
    "Halooo @user!!! 123",
    remove_mentions=True,
    remove_numbers=True,
    remove_punctuation=True,
    replace_word_elongation=True,
    to_lowercase=True,
    # Nonaktifkan normalisasi advanced
    normalize_slang=False,
    correct_spelling=False,
    remove_stopwords=False
)
print(result)
# Output: "halo"
```

#### ğŸ“‹ Konfigurasi Pipeline Available

```python
# Semua opsi konfigurasi yang tersedia
available_options = {
    # Basic cleaning
    'remove_html': True,           # Hapus HTML tags
    'remove_url': True,            # Hapus URL
    'remove_mentions': True,       # Hapus @mentions
    'remove_hashtags': True,       # Hapus #hashtags
    'remove_numbers': False,       # Hapus angka
    'remove_punctuation': False,   # Hapus tanda baca
    'remove_special_chars': True,  # Hapus karakter khusus
    'remove_whitespace': True,     # Hapus whitespace berlebih
    'remove_extra_spaces': True,   # Hapus spasi berlebih

    # Text normalization
    'to_lowercase': True,          # Ubah ke huruf kecil
    'replace_word_elongation': True, # Normalisasi kata berulang (halooo -> halo)
    'normalize_slang': True,       # Normalisasi slang (gw -> saya)
    'correct_spelling': True,      # Koreksi ejaan

    # Emoji handling
    'emoji_to_words': False,       # Ubah emoji ke kata
    'words_to_emoji': False,       # Ubah kata ke emoji

    # Linguistic processing
    'remove_stopwords': False,     # Hapus stopwords
    'stem_text': False,           # Lakukan stemming

    # Tokenization
    'tokenize': False,            # Tokenisasi (return list)
}
```

### 9. ğŸ“Š Dataset Loader

```python
from nahiarhdNLP.datasets import DatasetLoader

loader = DatasetLoader()

# Load stopwords dari CSV lokal
stopwords = loader.load_stopwords_dataset()
print(f"Jumlah stopwords: {len(stopwords)}")

# Load slang dictionary dari CSV lokal
slang_dict = loader.load_slang_dataset()
print(f"Jumlah slang: {len(slang_dict)}")

# Load emoji dictionary dari CSV lokal
emoji_dict = loader.load_emoji_dataset()
print(f"Jumlah emoji: {len(emoji_dict)}")

# Load wordlist dari JSON lokal
wordlist = loader.load_wordlist_dataset()
print(f"Jumlah kata: {len(wordlist)}")
```

> **Catatan:** Semua dataset (stopword, slang, emoji, wordlist) di-load langsung dari file CSV/JSON di folder `nahiarhdNLP/datasets/`. Tidak ada proses cache atau download dari HuggingFace.

## ğŸš¨ Error Handling

```python
try:
    from nahiarhdNLP.preprocessing import SpellCorrector
    spell = SpellCorrector()
    result = spell.correct_sentence("test")
except ImportError:
    print("Package nahiarhdNLP belum terinstall")
    print("Install dengan: pip install nahiarhdNLP")
except Exception as e:
    print(f"Error: {e}")
```

## ğŸ’¡ Tips Penggunaan

1. **Untuk preprocessing sekaligus**: Gunakan `Pipeline`, `pipeline()`, atau `preprocess()`
2. **Untuk cleaning dasar**: Gunakan `clean_text()` atau kelas `TextCleaner`
3. **Untuk kontrol penuh**: Gunakan kelas individual (`TextCleaner`, `SpellCorrector`, dll)
4. **Untuk spell correction + slang**: Gunakan `SpellCorrector` yang menggabungkan kedua fitur
5. **Untuk stemming**: Install Sastrawi terlebih dahulu: `pip install Sastrawi`
6. **Untuk load dataset**: Gunakan `DatasetLoader` dari `nahiarhdNLP.datasets`
7. **Untuk inisialisasi kelas**: Jangan lupa panggil `_load_data()` untuk kelas yang memerlukan dataset
8. **Untuk kustomisasi pipeline**: Gunakan kelas `Pipeline` dengan konfigurasi dictionary
9. **Untuk penggunaan sederhana**: Gunakan fungsi `pipeline()` atau `preprocess()`

## âš¡ Performance & Dataset

Mulai versi terbaru, nahiarhdNLP **menggunakan dataset lokal** yang sudah disediakan:

- **Stopwords**: File `stop_word.csv`
- **Slang Dictionary**: File `slang.csv`
- **Emoji Mapping**: File `emoji.csv`
- **Wordlist**: File `wordlist.json`
- **KBBI Dictionary**: File `kata_dasar_kbbi.csv`

Semua dataset tersimpan di folder `nahiarhdNLP/datasets/` dan diakses melalui `DatasetLoader`.

## ğŸ“¦ Dependencies

Package ini membutuhkan:

- `pandas` - untuk load dan proses dataset CSV/JSON
- `sastrawi` - untuk stemming (opsional)
- `rich` - untuk output formatting (opsional)

## ğŸ”§ Struktur Modul

```
nahiarhdNLP/
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ loaders.py          # DatasetLoader class
â”‚   â”œâ”€â”€ emoji.csv           # Dataset emoji
â”‚   â”œâ”€â”€ slang.csv           # Dataset slang
â”‚   â”œâ”€â”€ stop_word.csv       # Dataset stopwords
â”‚   â”œâ”€â”€ wordlist.json       # Dataset wordlist
â”‚   â””â”€â”€ kata_dasar_kbbi.csv # Dataset KBBI
â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ cleaning/
â”‚   â”‚   â””â”€â”€ text_cleaner.py # TextCleaner class
â”‚   â”œâ”€â”€ linguistic/
â”‚   â”‚   â”œâ”€â”€ stemmer.py      # Stemmer class
â”‚   â”‚   â””â”€â”€ stopwords.py    # StopwordRemover class
â”‚   â”œâ”€â”€ normalization/
â”‚   â”‚   â”œâ”€â”€ emoji.py        # EmojiConverter class
â”‚   â”‚   â””â”€â”€ spell_corrector.py # SpellCorrector class
â”‚   â”œâ”€â”€ tokenization/
â”‚   â”‚   â””â”€â”€ tokenizer.py    # Tokenizer class
â”‚   â””â”€â”€ utils.py            # Fungsi utility individual
â””â”€â”€ demo.py                 # File demo penggunaan
```

## ğŸ†• Perubahan Versi 1.1.1

- âœ… **[BARU]** Penambahan fitur **Pipeline** untuk preprocessing sekaligus
- âœ… **[BARU]** Kelas `Pipeline` dengan konfigurasi fleksibel
- âœ… **[BARU]** Fungsi `pipeline()` untuk penggunaan sederhana
- âœ… **[BARU]** Fungsi `preprocess()` dengan parameter eksplisit
- âœ… Menggabungkan spell correction dan slang normalization dalam `SpellCorrector`
- âœ… Semua dataset menggunakan file lokal (CSV/JSON)
- âœ… Struktur yang lebih terorganisir dengan pemisahan kelas dan fungsi
- âœ… Penambahan `DatasetLoader` untuk manajemen dataset terpusat
- âŒ Menghapus dependency HuggingFace untuk dataset

## ğŸ› Troubleshooting

**Error saat import dataset:**

```python
# Pastikan memanggil _load_data() untuk kelas yang memerlukan dataset
stopword = StopwordRemover()
stopword._load_data()  # Penting!
```

**Error Sastrawi tidak ditemukan:**

```bash
pip install Sastrawi
```

**Error pandas tidak ditemukan:**

```bash
pip install pandas
```
