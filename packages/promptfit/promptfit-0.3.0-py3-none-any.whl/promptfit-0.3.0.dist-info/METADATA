Metadata-Version: 2.4
Name: promptfit
Version: 0.3.0
Summary: promptfit - Modular toolkit for optimizing LLM prompts: estimate token usage, rank by semantic relevance, and compress with LLMs to fit any token budget. Perfect for RAG, few-shot, and instruction-heavy GenAI workflows.
Author-email: Vedant Laxman Chandore <vedantchandore5@email.com>
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: cohere
Requires-Dist: scikit-learn
Requires-Dist: tiktoken
Requires-Dist: python-dotenv
Requires-Dist: nltk
Requires-Dist: spacy
Requires-Dist: rich
Requires-Dist: typer
Requires-Dist: pytest

# promptfit

[![PyPI version](https://badge.fury.io/py/promptfit.svg)](https://pypi.org/project/promptfit/)
[![GitHub stars](https://img.shields.io/github/stars/VedantChandore/promptfit?style=social)](https://github.com/VedantChandore/promptfit)


---

---

## ğŸ“£ Author

**Vedant Laxman Chandore**  
[GitHub](https://github.com/VedantChandore)

##  The Core Problem
Modern LLMs (Cohere, OpenAI, Gemini, Anthropic, etc.) are powerful, but their **token limits** make it hard to fit rich, multi-section promptsâ€”especially for Retrieval-Augmented Generation (RAG), few-shot learning, and instruction-heavy use cases. Developers waste time manually trimming prompts, risking loss of important context, incomplete responses, or costly token overages.

**promptfit** solves this by automating prompt analysis, compression, and optimizationâ€”so you get the most value from every token, every time.

---

## âœ¨ Features

- **ğŸ”¢ Token Budget Estimator:** Analyze and estimate token usage for prompt templates, sections, and variablesâ€”before sending to an LLM.
- **ğŸ§­ Semantic Relevance Scoring:** Split prompts into sections, generate embeddings (Cohere), and rank by cosine similarity to your query or task.
- **âœ‚ï¸ Smart Prompt Pruner:** Drop or trim low-salience sections first, keeping only the most relevant content to fit your token budget.
- **âœï¸ Paraphrasing Module:** Use Cohereâ€™s LLM to rewrite and compress over-budget prompts, preserving key instructions and meaning.
- **ğŸ“¦ Modular Design:** Each feature is a standalone moduleâ€”use them independently or together.
- **ğŸ§ª Test-Driven:** Comprehensive unit tests with mocked or live Cohere API responses.
- **ğŸ” Secure API Key Handling:** Loads your Cohere API key from a `.env` file or environment variable.
- **ğŸ–¥ï¸ CLI Support:** Optimize prompts directly from the command line.

---

## ğŸ› ï¸ Tech Stack

- **Language:** Python 3.10+
- **LLM:** [Cohere command-r-plus](https://docs.cohere.com/docs/models-overview)
- **Embeddings:** embed-english-v3.0
- **Tokenizer:** Cohereâ€™s estimator (or manual fallback)
- **Libraries:**
  - `cohere`, `scikit-learn`, `tiktoken`, `python-dotenv`, `nltk`/`spacy`, `rich`, `typer`, `pytest`

---

## ğŸ“¦ Installation

```bash
pip install promptfit
```

---

## ğŸš€ Demo Usage

### **Python API**

```python
import os
import time
# 1. Set your Cohere key (or have it in your env)
os.environ["COHERE_API_KEY"] = "Kwi33HNnmXRDCkO4j7FndNP3LATOoKX3yvoOdztK"

from promptfit.token_budget import estimate_tokens
from promptfit.utils import split_sentences
from promptfit.embedder import get_embeddings
from promptfit.relevance import compute_cosine_similarities
from promptfit.optimizer import optimize_prompt

# 2. Your actual long prompt
prompt = """
You are a customer-support assistant. A user reports that their device fails
intermittently under cold conditions, the battery drains within two hours, and
previous support tickets went unanswered. Theyâ€™ve provided logs and screenshots.
Please summarize the issues, note their emotional tone, propose immediate
fixes, and suggest long-term retention strategies.
"""

query = "Summarize issues, emotional tone, action items, and retention strategies."

print("=== PROMPT ===")
print(prompt)

# 3. Split into sentences
sentences = split_sentences(prompt)
print("=== SENTENCES ===")
for i, s in enumerate(sentences, 1):
    print(f"{i}: {s!r}")
print()

# 4. Compute embeddings (first the query, then the sentences)
all_texts = [query] + sentences
embs = get_embeddings(all_texts)

# 5. Compute cosine similarities between query and each sentence
query_emb = embs[0]
sent_embs = embs[1:]
scores = compute_cosine_similarities(query_emb, sent_embs)

# 6. Display relevance scores
print("=== RELEVANCE SCORES OF COSINE SIMILARITY===")
for sent, score in zip(sentences, scores):
    print(f"{score:.4f}â€‚â€“â€‚{sent!r}")
print()

# 7. Show total token count before optimization
orig_tokens = estimate_tokens(prompt)
print(f"Original prompt â‰ˆ {orig_tokens} tokens\n")

# 8. Run optimizer with timing
budget = 40
start_time = time.time()
optimized = optimize_prompt(prompt, query, max_tokens=budget)
end_time = time.time()
opt_tokens = estimate_tokens(optimized)

tokens_saved = orig_tokens - opt_tokens
percent_saved = (tokens_saved / orig_tokens) * 100

# 9. Output final result with efficiency stats
print(f"Optimized prompt ({opt_tokens} tokens â‰¤ {budget} budget):\n")
print(optimized)
print("\n--- Efficiency Stats ---")
print(f"Token reduction: {orig_tokens - opt_tokens} tokens")
print(f"Reduction percentage: {(orig_tokens - opt_tokens) / orig_tokens * 100:.1f}%")
print(f"Optimization time: {end_time - start_time:.2f} seconds")
print(f"Tokens Saved: {tokens_saved}")

```

### **Command Line**

```bash
python -m promptfit.cli "YOUR_PROMPT" "YOUR_QUERY" --max-tokens 120
```

### **Full Demo Script**
See [`demo/demo_usage.py`](demo/demo_usage.py) for a comprehensive example covering:
- Token estimation
- Embedding generation
- Relevance ranking
- Pruning and paraphrasing
- End-to-end optimization

---

## ğŸ—ï¸ Environment Setup

- Store your `COHERE_API_KEY` in a `.env` file in your project root:
  ```
  COHERE_API_KEY=your-real-api-key-here
  ```
- Or set it in your shell:
  ```bash
  export COHERE_API_KEY=your-real-api-key-here
  ```

---

## ğŸ“š Directory Structure

```
promptfit/
â”‚
â”œâ”€â”€ __init__.py
â”œâ”€â”€ token_budget.py
â”œâ”€â”€ embedder.py
â”œâ”€â”€ relevance.py
â”œâ”€â”€ optimizer.py
â”œâ”€â”€ paraphraser.py
â”œâ”€â”€ cli.py
â”œâ”€â”€ utils.py
â”œâ”€â”€ config.py
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_token_budget.py
â”‚   â”œâ”€â”€ test_relevance.py
â”‚   â”œâ”€â”€ test_optimizer.py
â”‚   â””â”€â”€ test_paraphraser.py
â”‚
â””â”€â”€ demo/
    â””â”€â”€ demo_usage.py
```

---

## ğŸ’¡ Why Use promptfit?

- **Save tokens, save money:** Only send the most relevant, concise prompts to your LLM.
- **Prevent errors:** Never exceed token limits or lose critical context.
- **Automate prompt engineering:** Focus on your app, not manual prompt trimming.
- **Works with any LLM:** Designed for Cohere, but easily adaptable to OpenAI, Gemini, Anthropic, and more.

---

## ğŸ“ License

MIT License

---

## ğŸ¤ Contributing

Pull requests, issues, and stars are welcome! For major changes, please open an issue first to discuss what youâ€™d like to change.

---

## ğŸ“£ Author

**Vedant Laxman Chandore**  
[GitHub](https://github.com/VedantChandore)

---
<<<<<<<

*Built for the next generation of GenAI and LLM developers. Optimize your prompts, maximize your results!* 
=======

>>>>>>> 
