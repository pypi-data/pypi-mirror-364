{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1D spectral analysis and light curve calculation from DL3 (energy-dependent angular cuts)\n",
    "\n",
    "Originally created by Chaitanya Priyadarshi for the 1st LST-1 analysis school and later adapted to work with Gammapy v1.1 and the latest DL3 files produced by lstchain v0.10 for the 2nd LST-1 analysis school.\n",
    "\n",
    "⚠️ **Important**: Gammapy v1.2 can also be used, but in this case LST-1 DL3 files must be produced with lstchain >= v0.10.9 to include the needed metadata to use Gammapy v1.2.\n",
    "\n",
    "Even if Gammapy can handle gzipped DL3 files, it is faster to use unzipped DL3 files.\n",
    "\n",
    "This is a combination of Gammapy tutorials for the **1D spectral analysis following the ON-OFF forward-folding method and light curve calculation** for point-like-source observations in wobble mode.\n",
    "It requires point-like IRFs.\n",
    "\n",
    "Original notebooks can be accessed at:\n",
    "\n",
    " - https://docs.gammapy.org/1.1/tutorials/analysis-1d/spectral_analysis_rad_max.html (from DL3 with energy-dependent angular cuts, the *approach followed in this notebook*)\n",
    " - https://docs.gammapy.org/1.1/tutorials/analysis-1d/spectral_analysis.html (from DL3 with global gammaness and angular cuts)\n",
    " - https://docs.gammapy.org/1.1/tutorials/analysis-time/light_curve.html\n",
    " - https://docs.gammapy.org/1.1/tutorials/analysis-time/light_curve_flare.html\n",
    "\n",
    "\n",
    "It reduces a set of DL3 files (with energy-dependent angular cuts) into an energy-binned dataset, fits a spectral model to this dataset, calculates spectral flux points and computes the light curve.\n",
    "\n",
    "We also describe how to write all these objects to files, read them back, inspect and plot the results.\n",
    "\n",
    "-----------\n",
    "## Content\n",
    "\n",
    "### 0. Inspect the DL3 file content\n",
    "### 1. Read the DL3 index files and load the data\n",
    "### 2. Selection filters for the observations\n",
    "### 3. Define Target position and energy ranges for reconstructed events\n",
    "### 4. Define the base Map geometries for creating the SpectrumDataset\n",
    "### 5. Data reduction chain\n",
    "### 6. Generate the Spectrum Dataset for all observations\n",
    "### 7. Some plots with the given Dataset\n",
    "### 8. Write all datasets into OGIP files\n",
    "### 9. Get Pivot energy to fix the reference energy and define the Spectrum Model\n",
    "### 10. Spectral Fitting\n",
    "### 11. Check the Flux points\n",
    "### 12. SED plots\n",
    "### 13. Light curve\n",
    "### 14. Save the SED and LC Flux Points and Model to separate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "import numpy as np\n",
    "from regions import PointSkyRegion\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gammapy.data import DataStore, EventList\n",
    "from gammapy.datasets import (\n",
    "    Datasets,\n",
    "    FluxPointsDataset,\n",
    "    SpectrumDataset,\n",
    ")\n",
    "from gammapy.estimators import FluxPointsEstimator, LightCurveEstimator, FluxPoints\n",
    "from gammapy.makers import (\n",
    "    ReflectedRegionsBackgroundMaker,\n",
    "    SafeMaskMaker,\n",
    "    SpectrumDatasetMaker,\n",
    "    WobbleRegionsFinder\n",
    ")\n",
    "from gammapy.maps import MapAxis, RegionGeom, Map\n",
    "from gammapy.modeling import Fit\n",
    "from gammapy.modeling.models import (\n",
    "    Models,\n",
    "    PowerLawSpectralModel,\n",
    "    LogParabolaSpectralModel,\n",
    "    create_crab_spectral_model,\n",
    "    SkyModel,\n",
    ")\n",
    "from gammapy.visualization import plot_spectrum_datasets_off_regions\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.time import Time\n",
    "import astropy.units as u\n",
    "\n",
    "from gammapy import __version__ as gammapy_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Using Gammapy {gammapy_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Input and output settings (SET BY THE USER)\n",
    "Here we use some DL3 Crab data from the 2024 LST analysis school as an example.\n",
    "\n",
    "If left unchanged, the following cell will create an analysis directory in your workspace. Please change it at your convenience.\n",
    "\n",
    "Intermediate reduced datasets files are also stored following the OGIP standards.\n",
    "See https://gamma-astro-data-formats.readthedocs.io/en/v0.3/spectra/ogip/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the input DL3 files.\n",
    "base_dir = Path(\"/fefs/aswg/workspace/analysis-school-2024\")\n",
    "dl3_path = base_dir / \"DL3/Crab\"\n",
    "\n",
    "# Define and create the analysis directory where products will be saved.\n",
    "# It will be placed in your workspace, but you can change it accordingly.\n",
    "username = os.getenv(\"USER\")\n",
    "analysis_dir = Path(f\"/fefs/aswg/workspace/{username}/analysis-school-2024/high_level_analysis\")\n",
    "\n",
    "# Create also another directory to store the intermediate reduced dataset files in OGIP format.\n",
    "ogip_path = analysis_dir / \"OGIP\"\n",
    "ogip_path.mkdir(exist_ok=True, parents=True)  # All parent directories will be created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# 0. Inspect the DL3 file content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open one of the DL3 files from the directory\n",
    "filename = dl3_path / \"dl3_LST-1.Run15996.fits\"\n",
    "events = EventList.read(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the event list (here the first 10 events)\n",
    "events.table[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can have a look of the events with\n",
    "events.peek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can select events based any parameter on the table above, e.g. energy:\n",
    "selected_energy = events.select_energy([500 * u.GeV, 1 * u.TeV])\n",
    "selected_energy.peek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or gammaness:\n",
    "gh_range = [0.9, 1]\n",
    "selected_events_gh = events.select_parameter(parameter=\"GAMMANESS\", band=gh_range)\n",
    "selected_events_gh.peek()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# 1. Read the DL3 index files and load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Let's load now all the DL3 files together.\n",
    "If the DL3 index files are not present, run the `lstchain_create_dl3_index_files` for the given DL3 files.\n",
    "\n",
    "`lstchain_create_dl3_index_files -d $dl3_path`  (by default create the index files in the same directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_datastore = DataStore.from_dir(dl3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's have a first look at the first 5 entries of the table from all observations present in the index files. \n",
    "# It contains run-wise information.\n",
    "total_datastore.obs_table[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Also you can have a look at the HDU table, which contains information on the data \n",
    "# and instrument response function files for each observation\n",
    "total_datastore.hdu_table[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# 2. Selection filters for the observations\n",
    "\n",
    "Based on the run-wise information from the previous table, filters can be applied to select just a subset of the observation list based on source name, zenith angle or livetime.\n",
    "\n",
    "This list of observations based on similar filters can be also obtained beforehand by using the [data quality notebook](https://github.com/cta-observatory/cta-lstchain/blob/main/notebooks/data_quality.ipynb) (recommended). Then you can directly pass this list of `obs_id` to `get_observations` method of `DataStore` object below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the object name from the OBS Table, assuming all the DL3 files are of the same single source.\n",
    "# If not, then select a single object, to produce the relevant Spectrum Dataset\n",
    "\n",
    "obj_name = np.unique(total_datastore.obs_table[\"OBJECT\"])[0]\n",
    "print(\"The source is\", obj_name)\n",
    "\n",
    "max_zen = 60  # in deg for a maximum limit on zenith pointing of observations\n",
    "min_time = 60  # in seconds for minimum livetime of each observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_obs_list = total_datastore.obs_table[\"OBS_ID\"].data\n",
    "observations_total = total_datastore.get_observations(\n",
    "    total_obs_list, \n",
    "    required_irf=\"point-like\",  # By default is \"full-enclosure\" : [\"events\", \"gti\", \"aeff\", \"edisp\", \"psf\", \"bkg\"]\n",
    "    skip_missing=False  # Skip missing observations within the list provided earlier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d_time = total_datastore.obs_table[\"LIVETIME\"] > min_time\n",
    "d_zen = total_datastore.obs_table[\"ZEN_PNT\"] < max_zen\n",
    "d_obj = total_datastore.obs_table[\"OBJECT\"] == obj_name\n",
    "\n",
    "obs_table_selected = total_datastore.obs_table[d_zen & d_obj & d_time]\n",
    "obs_id_list = obs_table_selected[\"OBS_ID\"]\n",
    "\n",
    "observations_sel = total_datastore.get_observations(\n",
    "    obs_id_list, \n",
    "    required_irf=\"point-like\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Observation runs selected are:', obs_id_list.data)\n",
    "print(f'Total livetime of all observations: {total_datastore.obs_table[\"LIVETIME\"].to(u.h).sum():.1f}')\n",
    "print(f'Total livetime of selected observations {obs_table_selected[\"LIVETIME\"].data.sum()/3600:.1f} hrs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# 3. Define Target position and energy ranges for reconstructed events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_position = SkyCoord.from_name(obj_name, frame='icrs')\n",
    "target_position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "If we were using global theta cut, this is the way of getting the theta cut (`RAD_MAX` key) used for the IRF production.\n",
    "\n",
    "Here we will use energy-dependent cuts, therefore the following is not executed in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "```\n",
    "# Find the fixed global theta cut used for creating the IRFs\n",
    "\n",
    "theta_cut = observations_sel[0].aeff.meta[\"RAD_MAX\"]\n",
    "print(\"Theta cut applied for creating the IRF in the selected DL3 file,\", theta_cut)\n",
    "\n",
    "# Converting the value into astropy.units to be used for defining the ON region with CircleSkyRegion\n",
    "on_region_radius = u.Quantity(theta_cut)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The metadata of the DL3 contains the efficiency of the cuts used for the DL3 production.\n",
    "observations_sel[0].aeff.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the minimum, maximum energies in TeV units, and number of bins per decade, to create the \n",
    "# required reconstructed and true energy ranges.\n",
    "# For Light Curve estimation and spectral fitting, flux calculation can only be performed within \n",
    "# the energy edges provided for the reconstructed events.\n",
    "# For example, if the reconstructed energy edges are [0.01, 0.1, 1, 10] TeV and you want LC in \n",
    "# [0.05, 10] TeV energy range, then, reproduce the Dataset objects with those reconstructed energy edges.\n",
    "\n",
    "e_reco_min = 0.05  # 0.01\n",
    "e_reco_max = 50\n",
    "\n",
    "e_true_min = 0.01\n",
    "e_true_max = 100\n",
    "\n",
    "# Using bins per decade\n",
    "e_reco_bin_p_dec = 8\n",
    "e_true_bin_p_dec = 10\n",
    "\n",
    "energy_axis = MapAxis.from_energy_bounds(\n",
    "    e_reco_min, e_reco_max, \n",
    "    nbin=e_reco_bin_p_dec, per_decade=True, \n",
    "    unit=\"TeV\", name=\"energy\"\n",
    ")\n",
    "energy_axis_true = MapAxis.from_energy_bounds(\n",
    "    e_true_min, e_true_max, \n",
    "    nbin=e_true_bin_p_dec, per_decade=True, \n",
    "    unit=\"TeV\", name=\"energy_true\"\n",
    ")\n",
    "\n",
    "# Select the minimum and maximum energy edges for the SED, from the energy_axis to be used in the Dataset\n",
    "# Here we use a different minimum energy than energy_axis, but the same energy bins.\n",
    "# For analyzers who do not want energy bins per decade, or some custom bins for energy_axis, \n",
    "# make appropriate changes in each axis.\n",
    "e_fit_min = energy_axis.edges[1].value\n",
    "e_fit_max = energy_axis.edges[-1].value\n",
    "e_fit_bin_p_dec = e_reco_bin_p_dec\n",
    "\n",
    "# Just to have a separate MapAxis for spectral fit energy range\n",
    "energy_fit_edges = MapAxis.from_energy_bounds(\n",
    "    e_fit_min, e_fit_max, \n",
    "    nbin=e_fit_bin_p_dec, per_decade=True, \n",
    "    unit=\"TeV\"\n",
    ").edges\n",
    "\n",
    "\n",
    "print(\"Spectral Fit will be done in energy edges:\\n\", energy_fit_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can access the energy edges and center values\n",
    "energy_axis_true.edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "# 4. Define the base Map geometries for creating the SpectrumDataset\n",
    "\n",
    "In case of using global angular cut `CircleSkyRegion` should be used instead of `PointSkyRegion` (see [\"Spectral analysis\" Gammapy tutorial](https://docs.gammapy.org/1.2/tutorials/analysis-1d/spectral_analysis.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "on_region = PointSkyRegion(target_position)  \n",
    "\n",
    "# This will create the base geometry in which to bin the events based on their reconstructed positions\n",
    "on_geom = RegionGeom.create(\n",
    "    on_region, \n",
    "    axes=[energy_axis]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "# 5. Data Reduction chain\n",
    "Create some Dataset and Data Reduction Makers:\n",
    "\n",
    "In the case of energy-dependent angular cuts we have to use the `WobbleRegionsFinder`, \n",
    "to determine the OFF positions, depending on the number of regions specified.\n",
    "\n",
    "Their sizes will be defined by the theta values in RAD_MAX_2D table based on the estimated energy binning.\n",
    "The same logic applies to the size of the ON region.\n",
    "\n",
    "Background maker will use the WobbleRegionsFinder, assuming 1 OFF region for the background estimation\n",
    "Using more off regions depends on the maximum value allowed for RAD_MAX_2D.\n",
    "Gammapy will not allow the use of more off regions since at low energy they will overlap.\n",
    "\n",
    "One way to overcome this problem is to restrict the value of theta angular cut up to a maximum value (e.g. 0.2 deg)\n",
    "when producing the IRFs and DL3 files with lstchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geom is the target geometry in reco energy for counts and background maps\n",
    "# energy_axis_true is the true energy axis for the IRF maps\n",
    "dataset_empty = SpectrumDataset.create(\n",
    "    geom=on_geom, \n",
    "    energy_axis_true=energy_axis_true\n",
    ")\n",
    "# When not including a PSF IRF, put the containment_correction as False\n",
    "dataset_maker = SpectrumDatasetMaker(\n",
    "    containment_correction=False, \n",
    "    selection=[\"counts\", \"exposure\", \"edisp\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "The following makers can be tuned to check the final Dataset to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_finder = WobbleRegionsFinder(n_off_regions=1)\n",
    "bkg_maker = ReflectedRegionsBackgroundMaker(region_finder=region_finder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maker for safe energy range for the events, which can be based on effective area, energy bias, etc.\n",
    "# Here we set a lower energy threshold corresponding to 5% of the maximum value of the effective area.\n",
    "safe_mask_maker = SafeMaskMaker(\n",
    "    methods=[\"aeff-max\"], \n",
    "    aeff_percent=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively you can make a custom safe energy range for the events.\n",
    "#safe_min_energy = 50 * u.GeV\n",
    "#safe_max_energy = 20 * u.TeV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "For other arguments and options, check the SafeMaskMaker documentation:\n",
    "https://docs.gammapy.org/1.1/api/gammapy.makers.SafeMaskMaker.html#gammapy.makers.SafeMaskMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "# 6. Generate the Spectrum Dataset for all observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The final object will be stored as a Datasets object\n",
    "\n",
    "datasets = Datasets()\n",
    "\n",
    "# Create a counts map for visualisation later\n",
    "counts = Map.create(skydir=target_position, width=3)\n",
    "\n",
    "for obs_id, observation in zip(obs_id_list, observations_sel):\n",
    "    dataset = dataset_maker.run(\n",
    "        dataset_empty.copy(name=str(obs_id)), \n",
    "        observation\n",
    "    )\n",
    "    dataset_on_off = bkg_maker.run(\n",
    "        dataset=dataset, \n",
    "        observation=observation\n",
    "    )\n",
    "    counts.fill_events(observation.events)\n",
    "    \n",
    "    # Check the LC and SEDs by applying the safe mask to see the distinction.\n",
    "    dataset_on_off = safe_mask_maker.run(dataset_on_off, observation)\n",
    "    \n",
    "    # Or use custom safe energy range\n",
    "    #dataset_on_off.mask_safe = dataset_on_off.counts.geom.energy_mask(\n",
    "    #    energy_min=safe_min_energy, energy_max=safe_max_energy\n",
    "    #)\n",
    "    \n",
    "    datasets.append(dataset_on_off)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Have a look at the information from the first dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datasets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## 6.1 Stack datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "At this point, all the datasets can be stacked in a single dataset (SpectrumDatasetOnOff object), which is usually faster to handle.\n",
    "\n",
    "You can do it like follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_dataset = Datasets(datasets).stack_reduce()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "Alternatively, you can continue directly using the individual `Datasets` as produced in the above loop. Then you would perform a joint likelihood fit to all observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "# 7. Some plots with the given Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "Plot the target position and center of the OFF regions used for the calculation of the gamma-ray excess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = counts.plot(cmap=\"viridis\", stretch=\"sinh\")\n",
    "on_geom.plot_region(ax=ax, kwargs_point={\"color\": \"k\", \"marker\": \"*\"})\n",
    "plot_spectrum_datasets_off_regions(ax=ax, datasets=datasets)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49",
   "metadata": {},
   "source": [
    "# For source-dependent analysis, check the reconstructed position of all the events, \n",
    "# to be sure of the type of dataset we have\n",
    "for o in observations_wob:\n",
    "    table=o.events.table\n",
    "    plt.plot((table[\"RA\"]*24/360),(table[\"DEC\"]), '.')\n",
    "plt.grid()\n",
    "plt.gca().invert_xaxis()\n",
    "plt.xlabel(\"RA (deg)\")\n",
    "plt.ylabel(\"Dec (deg)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_table = datasets.info_table(cumulative=True)\n",
    "info_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot temporal evolution of excess events and significance value\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(\n",
    "    info_table[\"livetime\"].to(\"h\"), info_table[\"excess\"], marker=\"o\", ls=\"none\"\n",
    ")\n",
    "plt.plot(info_table[\"livetime\"].to(\"h\")[-1:1], info_table[\"excess\"][-1:1], 'r')\n",
    "plt.xlabel(\"Livetime (h)\")\n",
    "plt.ylabel(\"Excess\")\n",
    "plt.grid()\n",
    "plt.title('Excess vs Livetime')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(\n",
    "    np.sqrt(info_table[\"livetime\"].to(\"h\")),\n",
    "    info_table[\"sqrt_ts\"],\n",
    "    marker=\"o\",\n",
    "    ls=\"none\",\n",
    ")\n",
    "plt.grid()\n",
    "plt.xlabel(r\"Sqrt Livetime ($\\sqrt{h}$)\")\n",
    "plt.ylabel(\"sqrt_ts\")\n",
    "plt.title('Significance vs Square root of Livetime')\n",
    "plt.subplots_adjust(wspace=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Plot the counts+excess, exposure and energy migration of each selected dataset.\n",
    "# For simplicity we show them for the first two observations.\n",
    "for data in datasets[:2]:\n",
    "    plt.figure(figsize=(21, 5.5))\n",
    "    plt.subplot(131)\n",
    "    data.plot_counts()\n",
    "    data.plot_excess()\n",
    "    plt.grid(which=\"both\")\n",
    "    plt.title(f'Run {data.name} Counts and Excess')\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    data.exposure.plot()\n",
    "    plt.grid(which='both')\n",
    "    plt.title(f'Run {data.name} Exposure')\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    if data.edisp is not None:\n",
    "        kernel = data.edisp.get_edisp_kernel()\n",
    "        kernel.plot_matrix(add_cbar=True)\n",
    "        plt.title(f'Run {data.name} Energy Dispersion')\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "# 8. Write all datasets into OGIP files\n",
    "\n",
    "These files are the datasets (events binned following the energy axes and geometry defined at the beginning). It corresponds to the DL4 data level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    dataset.write(\n",
    "        filename=ogip_path / f\"obs_{dataset.name}.fits.gz\", overwrite=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the OGIP files to include the source object name in its headers, to be used for further analysis\n",
    "for obs in obs_id_list:\n",
    "    file = ogip_path/f\"obs_{obs}.fits.gz\"\n",
    "    \n",
    "    d1 = fits.open(file)\n",
    "    d1.writeto(file, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "# 9. Get Pivot energy to fix the reference energy and define the Spectrum Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "As of Gammapy v1.2 it is possible to calculate the pivot energy for any spectral model not just for the power law like it is show below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find pivot (decorrelation) assuming a Power Law model to get the reference energy for a Log Parabola model (Gammapy < v1.2)\n",
    "def get_pivot_energy(datasets, e_ref, obj_name):\n",
    "    \"\"\"Calculate the decorrelation energy of the fit assuming a Power Law spectral model.\n",
    "    \n",
    "    This method (analytical calculation for a power law) is further explained \n",
    "    in doi:10.1088/0004-637X/707/2/1310\n",
    "    \"\"\"\n",
    "    # As of Gammapy v1.2, the decorrelation energy can be calculated for any spectral model,\n",
    "    # so you can change the model below to the one you use later.\n",
    "    # For previous versions the calculation of the pivot energy was only available\n",
    "    # (analytically) for the power law model.\n",
    "    spectral_model = PowerLawSpectralModel(\n",
    "        index=2, amplitude=2e-11 * u.Unit(\"cm-2 s-1 TeV-1\"), reference=e_ref\n",
    "    )\n",
    "\n",
    "    model = SkyModel(spectral_model=spectral_model, name=obj_name)\n",
    "    model_check = model.copy()\n",
    "\n",
    "    # Stacked dataset method\n",
    "    stacked_dataset = Datasets(datasets).stack_reduce()\n",
    "    stacked_dataset.models = model_check\n",
    "\n",
    "    fit_stacked = Fit()\n",
    "    result_stacked = fit_stacked.run(datasets=stacked_dataset)\n",
    "\n",
    "    return model_check.spectral_model.pivot_energy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a reference energy close to the expected decorrelation energy\n",
    "ref = get_pivot_energy(datasets, 0.4 * u.TeV, obj_name)\n",
    "print(f\"Decorrelation energy used as reference energy: {ref.to(u.GeV):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the spectral model to be fitted and later used to estimate the LC. \n",
    "# In this example, we assume a log parabola. One can try different Spectral Models as well.\n",
    "# Be careful in the choice of Spectral Model being used for the 2 examples presented here\n",
    "\n",
    "# Crab\n",
    "spectral_model_lp = LogParabolaSpectralModel(\n",
    "        amplitude = 5e-12 * u.Unit('cm-2 s-1 TeV-1'),\n",
    "        reference = ref,\n",
    "        alpha = 2 * u.Unit(''),\n",
    "        beta = 0.1 * u.Unit('')\n",
    ")\n",
    "\n",
    "# Usually, setting minimum and maximum values for amplitude and assuming that \n",
    "# alpha and beta cannot be negative, (or the index in case of a simple power law),\n",
    "# later helps to get ULs when using FluxPointEstimator in case of undetected sources\n",
    "spectral_model_lp.amplitude.min = 1e-17\n",
    "spectral_model_lp.amplitude.max = 1e-8\n",
    "\n",
    "spectral_model_lp.alpha.min = 0\n",
    "spectral_model_lp.beta.min = 0\n",
    "\n",
    "model_lp = SkyModel(spectral_model=spectral_model_lp, name=obj_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the appropriate models, as per the selection of the source/dataset\n",
    "model_lp.to_dict()['spectral']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "# 10. Spectral Fitting\n",
    "One can check for a more comprehensive tutorial on Modelling and Fitting in the Gammapy tutorials\n",
    "* https://docs.gammapy.org/1.1/tutorials/api/fitting.html\n",
    "* https://docs.gammapy.org/1.1/tutorials/api/model_management.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the stacked analysis method, where we stack together all Datasets into 1 Dataset object and add the model afterward\n",
    "stacked_dataset.models = model_lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model to the dataset\n",
    "fit = Fit()\n",
    "result = fit.run(datasets=stacked_dataset)\n",
    "model_best = model_lp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "Have a look at the fitting results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best.parameters[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best.to_dict()['spectral']['parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "## Compute the Flux Points after Fitting the model\n",
    "\n",
    "See the various attributes of the `Estimator` in Gammapy documentation (https://docs.gammapy.org/1.3/tutorials/api/estimators.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpe = FluxPointsEstimator(\n",
    "    energy_edges=energy_fit_edges, \n",
    "    reoptimize = False,  # Re-optimizing other free model parameters (not belonging to the source)\n",
    "    source=obj_name,\n",
    "    selection_optional=\"all\"  # Estimates asymmetric errors, upper limits and fit statistic profiles\n",
    ")\n",
    "\n",
    "flux_points = fpe.run(datasets=stacked_dataset)\n",
    "\n",
    "flux_points_dataset = FluxPointsDataset(\n",
    "    data=flux_points, models=model_best\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "### Calculating upper limits for dim sources\n",
    "\n",
    "To avoid NaN values when calculating flux point upper limits for dim sources, you may need to widen the default\n",
    "normalization range that `FluxPointsEstimator` and `LightCurveEstimator` can scan (by default set between 0.2 and 5).\n",
    "\n",
    "In Gammapy 1.3 you can change it like this:\n",
    "\n",
    "```python\n",
    " norm = Parameter(name=\"norm\", value=1.0)\n",
    "```\n",
    "\n",
    "Include in the `FluxPointsEstimator` definition as one of its arguments and then set the value of the parameter:\n",
    "\n",
    "```python\n",
    "fpe = FluxPointsEstimator(\n",
    "    ...,\n",
    "    norm=norm,\n",
    ")\n",
    "fpe.norm.scan_min = 0.05\n",
    "fpe.norm.scan_max = 20\n",
    "```\n",
    "\n",
    "Finally run the estimator:\n",
    "\n",
    "```python\n",
    "flux_points = fpe.run(datasets=datasets)\n",
    "```\n",
    "\n",
    "In older versions of Gammapy (<v1.3) you can directly set `norm_min` and `norm_max` in the `FluxPointsEstimator` definition:\n",
    "\n",
    "```python\n",
    "fpe = FluxPointsEstimator(\n",
    "    ...,\n",
    "    norm_min=0.05,\n",
    "    norm_max=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "# 11. Check the Flux points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Flux table\n",
    "# sed_type options are {“likelihood”, “dnde”, “e2dnde”, “flux”, “eflux”} with \"likelihood\" being default\n",
    "# format options are {“gadf-sed”, “lightcurve”, “binned-time-series”, “profile”} with \"gadf-sed\" being default\n",
    "flux_points.to_table(formatted=True, sed_type=\"e2dnde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Statistic array\n",
    "print(flux_points_dataset.stat_array())\n",
    "\n",
    "# Total statistics sum\n",
    "print(flux_points_dataset.stat_sum(), np.nansum(flux_points_dataset.stat_array()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best.parameters.to_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "# 12. SED Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_label=\"Crab MAGIC LP (JHEAp 2015)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_points.to_table(formatted=True, sed_type=\"e2dnde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting plot axes limits and other args\n",
    "\n",
    "# Here we plot only in the energy range from where we have the first flux point, and before the first UL.\n",
    "# This is based on the selection we used earlier on the dataset and the Flux points estimation we get.\n",
    "# One should adjust the limit as per selections used earlier.\n",
    "\n",
    "positive_flux = flux_points.to_table(formatted=True, sed_type=\"e2dnde\")[\"e2dnde\"] > 0\n",
    "e_plot_min = flux_points.to_table(formatted=True, sed_type=\"e2dnde\")[\"e_min\"].quantity[positive_flux][0]\n",
    "\n",
    "non_ul = flux_points.to_table(formatted=True, sed_type=\"e2dnde\")[\"is_ul\"] == 0\n",
    "e_plot_max = flux_points.to_table(formatted=True, sed_type=\"e2dnde\")[\"e_max\"].quantity[non_ul][-1]\n",
    "\n",
    "sed_kwargs = {\n",
    "    \"sed_type\": \"e2dnde\",\n",
    "    \"energy_bounds\": [e_plot_min, e_plot_max],\n",
    "}\n",
    "\n",
    "# Using the energy range used in the MAGIC reference\n",
    "ds_magic_ref_kwargs = {\n",
    "    \"sed_type\": \"dnde\",\n",
    "    \"energy_bounds\": [50 * u.GeV, 30 * u.TeV],\n",
    "}\n",
    "sed_magic_ref_kwargs = {\n",
    "    \"sed_type\": \"e2dnde\",\n",
    "    \"energy_bounds\": [50 * u.GeV, 30 * u.TeV],\n",
    "}\n",
    "sed_plot_kwargs = {\n",
    "    \"label\": \"LST-1\",\n",
    "}\n",
    "plot_ts_kwargs = {\n",
    "    \"color\": \"darkorange\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate & plot Crab reference flux\n",
    "# https://doi.org/10.1016/j.jheap.2015.01.002\n",
    "crab = create_crab_spectral_model(\"magic_lp\")\n",
    "crab.amplitude.error = 0.03e-11 * u.Unit(\"cm-2 s-1 TeV-1\")\n",
    "crab.alpha.error = 0.01\n",
    "crab.beta.error = 0.01/np.log(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "ax = flux_points.plot(sed_type=\"e2dnde\", **plot_ts_kwargs)\n",
    "\n",
    "flux_points.plot_ts_profiles(ax=ax, sed_type=\"e2dnde\")\n",
    "plt.xlim(e_plot_min.value, e_plot_max.value)\n",
    "\n",
    "plt.grid(which='both')\n",
    "plt.title('TS Profiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model covariance matrix plot\n",
    "model_best.covariance.plot_correlation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_sed = plt.figure(figsize=(8,8))\n",
    "\n",
    "gs2 = GridSpec(7, 1)\n",
    "\n",
    "gs2.update(hspace=0.1)\n",
    "args1 = [gs2[:5,:]]\n",
    "args2 = [gs2[5:,:]]\n",
    "\n",
    "fig_gs1 = fig_sed.add_subplot(*args1)\n",
    "fig_gs2 = fig_sed.add_subplot(*args2)\n",
    "\n",
    "FluxPointsDataset(data=flux_points, models=model_best).plot_spectrum(\n",
    "    ax=fig_gs1, \n",
    "    kwargs_fp=sed_plot_kwargs, \n",
    ")\n",
    "\n",
    "create_crab_spectral_model(\"magic_lp\").plot(\n",
    "    ax=fig_gs1, **sed_magic_ref_kwargs, label=ref_label\n",
    ")\n",
    "\n",
    "fig_gs1.legend()\n",
    "fig_gs1.set_xlim(e_plot_min.value, e_plot_max.value)\n",
    "fig_gs1.set_ylim(2e-12, 2e-10)\n",
    "fig_gs1.tick_params(labelbottom=False)\n",
    "\n",
    "fig_gs1.grid(which='both')\n",
    "fig_gs1.set_title('SED')\n",
    "\n",
    "flux_points_dataset.plot_residuals(ax=fig_gs2, method='diff/model')\n",
    "fig_gs2.grid(which='both')\n",
    "fig_gs2.set_xlim(e_plot_min.value, e_plot_max.value);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,7))\n",
    "gs = GridSpec(7, 1)\n",
    "\n",
    "args1 = [gs[:5,:]]\n",
    "args2 = [gs[5:,:]]\n",
    "kwargs_res = {\"method\": \"diff/sqrt(model)\"}\n",
    "\n",
    "fig_gs1 = fig.add_subplot(*args1)\n",
    "fig_gs2 = fig.add_subplot(*args2)\n",
    "\n",
    "stacked_dataset.plot_excess(fig_gs1)\n",
    "fig_gs1.grid(which=\"both\")\n",
    "fig_gs1.set_ylabel(\"Excess\")\n",
    "\n",
    "stacked_dataset.plot_residuals_spectral(fig_gs2, **kwargs_res, region=stacked_dataset.counts.geom.region)\n",
    "fig_gs2.grid(which=\"both\")\n",
    "\n",
    "fig_gs2.set_ylabel(\"Residuals \\n (data-model)/sqrt(model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "# 13. Light curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the energy range for the light curve. It has to match the edges of the \n",
    "# reconstructed energy binning set for the SED! So if you want to calculate it between\n",
    "# certain energy values, you must first define the energy axis accordingly.\n",
    "e_lc_min = energy_axis.edges[4]\n",
    "e_lc_max = energy_axis.edges[-1]\n",
    "print(f\"LC will be estimated from {e_lc_min.to(u.GeV):.0f} and {e_lc_max:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the GTI parameters of each observation to create time intervals for plotting LC\n",
    "t_start = []\n",
    "t_stop = []\n",
    "tot_time = []\n",
    "\n",
    "for obs in observations_sel:\n",
    "    gti = obs.gti\n",
    "    \n",
    "    t_start.append(gti.time_start[0])\n",
    "    t_stop.append(gti.time_stop[0])\n",
    "    tot_time.append(gti.time_sum.value)\n",
    "\n",
    "t_start = np.sort(np.array(t_start))\n",
    "t_stop = np.sort(np.array(t_stop))\n",
    "tot_time = np.array(tot_time)\n",
    "\n",
    "t_start = Time(t_start)\n",
    "t_stop = Time(t_stop)\n",
    "\n",
    "t_day = np.unique(np.rint(t_start.mjd))\n",
    "\n",
    "# To make the range night-wise, keep the MJD range in half-integral values\n",
    "t_range = [Time([t - 0.5, t + 0.5], format=\"mjd\", scale=\"utc\") for t in t_day]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LC Estimator on a run-by-run basis and nightly\n",
    "lc_maker_1d = LightCurveEstimator(\n",
    "    energy_edges=[e_lc_min, e_lc_max], \n",
    "    reoptimize=False, # Re-optimizing other free model parameters (not belonging to the source)\n",
    "    source=obj_name, \n",
    "    selection_optional=\"all\" # Estimates asymmetric errors, upper limits and fit statistic profiles\n",
    ")\n",
    "\n",
    "lc_maker_night_wise = LightCurveEstimator(\n",
    "    energy_edges=[e_lc_min, e_lc_max], \n",
    "    time_intervals=t_range,\n",
    "    reoptimize=False, \n",
    "    source=obj_name,\n",
    "    selection_optional=\"all\"\n",
    ")\n",
    "\n",
    "# Assigning the best fit model for each dataset\n",
    "for data in datasets:\n",
    "    data.models = model_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_1d = lc_maker_1d.run(datasets)\n",
    "lc_night = lc_maker_night_wise.run(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are more than 1 night of data, one can see the integrated light curve for each night\n",
    "lc_night.to_table(sed_type=\"flux\", format=\"lightcurve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the various column data of the Light Curve object\n",
    "lc_1d.to_table(sed_type=\"flux\", format=\"lightcurve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Crab reference flux\n",
    "flux_crab, flux_crab_error = crab.integral_error(e_lc_min, e_lc_max)\n",
    "print(f\"Crab reference flux integrated between {e_lc_min.to(u.GeV):.0f} and {e_lc_max:.0f}: {flux_crab.value:.2e} +/- {flux_crab_error:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting settings\n",
    "lc_kwargs = {\n",
    "    \"marker\": \"o\",\n",
    "}\n",
    "\n",
    "fig_lc, (ax1, ax2) = plt.subplots(2, sharex=True, sharey=True, figsize=(8,10))\n",
    "fig_lc.suptitle(\n",
    "    f'Light curve for {obj_name}: {e_lc_min.to(u.GeV):.0f} < E < {e_lc_max.to(u.TeV):.0f} '\n",
    "    f'\\nRun-wise (top panel), night-wise (bottom panel), '\n",
    "    f'{tot_time.sum()/3600:.1f} hrs, {len(t_day)} nights'\n",
    ")\n",
    "\n",
    "# Run-wise\n",
    "lc_1d.plot(\n",
    "    ax=ax1,\n",
    "    sed_type=\"flux\",\n",
    "    label=\"LST-1 (run-wise)\",\n",
    "    **lc_kwargs\n",
    ")\n",
    "ax1.set_xlabel(None)\n",
    "\n",
    "# Night-wise\n",
    "lc_night.plot(\n",
    "    ax=ax2,\n",
    "    sed_type=\"flux\",\n",
    "    axis_name=\"time\",\n",
    "    label=\"LST-1 (night-wise)\",\n",
    "    **lc_kwargs\n",
    ")\n",
    "\n",
    "# Plot reference Crab Nebula flux\n",
    "for ax in (ax1, ax2):\n",
    "    ax.axhline(\n",
    "        flux_crab.to_value(\"cm-2 s-1\"), c='red', ls='--', \n",
    "        label=ref_label\n",
    "    )\n",
    "    ax.axhspan(\n",
    "        (flux_crab - flux_crab_error).to_value(\"cm-2 s-1\"), \n",
    "        (flux_crab + flux_crab_error).to_value(\"cm-2 s-1\"), \n",
    "        alpha=0.2, color='tab:orange'\n",
    "    ) \n",
    "\n",
    "ax2.set_yscale('linear')\n",
    "ax2.set_ylim(0, 1.5 * np.nanmax(lc_1d.flux.data))\n",
    "ax1.grid(which='both')\n",
    "ax2.grid(which='both')\n",
    "ax1.legend()\n",
    "ax2.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "# 14. Save the SED and LC Flux Points and Model to separate files\n",
    "This way, one can plot different SEDs and LCs together.\n",
    "\n",
    "## Write spectral model to a file\n",
    "\n",
    "By default, it also stores the covariance file together with the model (same file path as the model but adding the `_covariance.dat` suffix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_model_file = analysis_dir / f'{obj_name}_dataset_{datasets[0].name}_to_{datasets[-1].name}_spectral_model.yml'\n",
    "stacked_dataset.models.write(spectral_model_file, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "## Write the SED and LC flux points tables to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_points_file = analysis_dir / f'{obj_name}_dataset_{datasets[0].name}_to_{datasets[-1].name}_SED_flux_points.fits'\n",
    "lightcurve_file = analysis_dir / f'{obj_name}_dataset_{datasets[0].name}_to_{datasets[-1].name}_LC_flux_points.fits'\n",
    "\n",
    "flux_points.write(flux_points_file, sed_type=\"e2dnde\", format=\"gadf-sed\", overwrite=True)\n",
    "lc_1d.write(lightcurve_file, sed_type=\"flux\", format=\"lightcurve\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "### Example on reading back and plotting the LC and SED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_model = Models.read(spectral_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "Then use the only model contained in the Models object we have created: `flux_model[0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_points = FluxPoints.read(\n",
    "    flux_points_file,\n",
    "    format=\"gadf-sed\",\n",
    "    reference_model=flux_model[0]\n",
    ")\n",
    "flux_points_lc = FluxPoints.read(\n",
    "    lightcurve_file,\n",
    "    sed_type=\"flux\", \n",
    "    format=\"lightcurve\",\n",
    "    reference_model=flux_model[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.set_title(\n",
    "    f'LC: {flux_points_lc.to_table(sed_type=\"flux\", format=\"lightcurve\")[\"e_min\"].to(u.GeV)[0][0]:.0f} < '\n",
    "    f'E < {flux_points_lc.to_table(sed_type=\"flux\", format=\"lightcurve\")[\"e_max\"].to(u.TeV)[0][0]:.0f}'\n",
    ")\n",
    "flux_points_lc.plot(ax=ax1, sed_type=\"flux\", label=\"LST-1 (run-wise)\")\n",
    "\n",
    "ax1.axhline(\n",
    "    flux_crab.to_value(\"cm-2 s-1\"), c='red', ls='--', label=ref_label\n",
    ")\n",
    "ax1.axhspan(\n",
    "    (flux_crab - flux_crab_error).to_value(\"cm-2 s-1\"), \n",
    "    (flux_crab + flux_crab_error).to_value(\"cm-2 s-1\"), \n",
    "    alpha=0.2, color='tab:orange'\n",
    ")\n",
    "ax1.legend()\n",
    "ax1.grid(which=\"both\")\n",
    "\n",
    "ax2.set_title(\"SED\")\n",
    "flux_points.plot(ax=ax2, sed_type=\"e2dnde\", **sed_plot_kwargs)\n",
    "flux_points.reference_model.spectral_model.plot(ax=ax2, **sed_kwargs)\n",
    "flux_points.reference_model.spectral_model.plot_error(ax=ax2, **sed_kwargs)\n",
    "create_crab_spectral_model(\"magic_lp\").plot(\n",
    "    ax=ax2,\n",
    "    label=ref_label,\n",
    "    **sed_magic_ref_kwargs,\n",
    ")\n",
    "ax2.legend()\n",
    "ax2.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Perform the 1D spectral analysis for BLLac with DL3 files in `/fefs/aswg/workspace/analysis-school-2024/DL3/BLLac` and calculate the light curve with a 10-minute temporal binning above 100 GeV."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
