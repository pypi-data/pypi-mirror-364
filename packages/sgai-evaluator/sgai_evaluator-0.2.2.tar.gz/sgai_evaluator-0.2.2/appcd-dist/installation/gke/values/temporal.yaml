nameOverride: ""
fullnameOverride: ""

# Chart debug mode
# (eg. disable helm hook delete policy)
debug: false

# Custom Service account management
serviceAccount:
  # Whether to create service account or not
  create: false

  # Name of the service account, default: temporal.fullname
  name:

  # extraAnnotations would let users add additional annotations
  extraAnnotations:

server:
  enabled: true
  sidecarContainers: {}
  dynamicConfig:
    frontend.keepAliveMaxConnectionAge:
    - value: "5m"
      constraints: {}
  image:
    repository: temporalio/server
    tag: 1.22.4
    pullPolicy: IfNotPresent

  # Global default settings (can be overridden per service)
  replicaCount: 2
  metrics:
    # Annotate pods directly with Prometheus annotations.
    # Use this if you installed Prometheus from a Helm chart.
    annotations:
      enabled: true
    # Additional tags to be added to Prometheus metrics
    tags: {}
    # Enable Prometheus ServiceMonitor
    # Use this if you installed the Prometheus Operator (https://github.com/coreos/prometheus-operator).
    serviceMonitor:
      enabled: ${enable_ops}
      interval: 30s
      # Set additional lables to all the ServiceMonitor resources
      additionalLabels:
        release: kube-prometheus-stack
      # Set Prometheus metric_relabel_configs via ServiceMonitor
      # Use metricRelabelings to adjust metric and label names as needed
      metricRelabelings:
      # - action: replace
      #   sourceLabels:
      #   - exported_namespace
      #   targetLabel: temporal_namespace
      # - action: replace
      #   regex: service_errors_(.+)
      #   replacement: ${1}
      #   sourceLabels:
      #   - __name__
      #   targetLabel: temporal_error_kind
      # - action: replace
      #   regex: service_errors_.+
      #   replacement: temporal_service_errors
      #   sourceLabels:
      #   - __name__
      #   targetLabel: __name__
    prometheus:
      timerType: histogram
  podAnnotations:
    reloader.stakater.com/auto: "true"
  podLabels: {}
  secretLabels: {}
  secretAnnotations: {}
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 128Mi
  nodeSelector: {}
  tolerations: []
  affinity: {}
  additionalVolumes: []
  additionalVolumeMounts: []
  additionalEnv: []
  securityContext:
    fsGroup: 1000
    runAsUser: 1000

  config:
    logLevel: "info"

    # IMPORTANT: This value cannot be changed, once it's set.
    numHistoryShards: 512
    persistence:
      defaultStore: default
      additionalStores: {}

      default:
        driver: "sql"
        sql:
          database: "temporal"
          driver: "postgres"
          maxConnLifetime: "1h"
          existingSecret: temporal-default-store
          host: ${postgres_host}
          maxConns: 20
          port: ${postgres_port}
          sslmode: "require"
          user: ${postgres_user}
          tls:
            enableHostVerification: false

      visibility:
        driver: "sql"
        sql:
          driver: "postgres"
          database: "temporalvisibility"
          existingSecret: temporal-visibility-store
          host: ${postgres_host}
          maxConnLifetime: "1h"
          maxConns: 20
          port: ${postgres_port}
          user: ${postgres_user}
          tls:
            enableHostVerification: false
  frontend:
    service:
      annotations: {}
      type: ClusterIP
      port: 7233
    metrics:
      annotations:
        enabled: true
      serviceMonitor: {}
      # enabled: false
      prometheus: {}
      # timerType: histogram
    podAnnotations:
      reloader.stakater.com/auto: "true"
    podLabels: {}
    resources:
      requests:
        cpu: 100m
        memory: 512Mi
    nodeSelector: {}
    tolerations: []
    affinity: {}
    additionalEnv: []
    containerSecurityContext: {}
    topologySpreadConstraints: {}
    podDisruptionBudget: {}
    resources:
      limits:
        cpu: 100m
        memory: 128Mi
      requests:
        cpu: 100m
        memory: 128Mi


  history:
    service:
      # type: ClusterIP
      port: 7234
    metrics:
      annotations:
        enabled: true
      serviceMonitor: {}
      # enabled: false
      prometheus: {}
      # timerType: histogram
    podAnnotations:
      reloader.stakater.com/auto: "true"
    podLabels: {}
    resources:
      requests:
        cpu: 100m
        memory: 512Mi
      limits:
        cpu: 100m
        memory: 512Mi
    nodeSelector: {}
    tolerations: []
    affinity: {}
    additionalEnv: []
    containerSecurityContext: {}
    topologySpreadConstraints: {}
    podDisruptionBudget: {}

  matching:
    service:
      # type: ClusterIP
      port: 7235
    metrics:
      annotations:
        enabled: false
      serviceMonitor: {}
      # enabled: false
      prometheus: {}
      # timerType: histogram
    podAnnotations:
      reloader.stakater.com/auto: "true"
    podLabels: {}
    resources:
      limits:
        cpu: 100m
        memory: 512Mi
      requests:
        cpu: 100m
        memory: 512Mi
    nodeSelector: {}
    tolerations: []
    affinity: {}
    additionalEnv: []
    containerSecurityContext: {}
    topologySpreadConstraints: {}
    podDisruptionBudget: {}

  worker:
    service:
      # type: ClusterIP
      port: 7239
    metrics:
      annotations:
        enabled: true
      serviceMonitor: {}
      # enabled: false
      prometheus: {}
      # timerType: histogram
    podAnnotations:
      reloader.stakater.com/auto: "true"
    podLabels: {}
    resources:
      limits:
        cpu: 100m
        memory: 512Mi
      requests:
        cpu: 100m
        memory: 512Mi
    nodeSelector: {}
    tolerations: []
    affinity: {}
    additionalEnv: []
    containerSecurityContext: {}
    topologySpreadConstraints: {}
    podDisruptionBudget: {}

admintools:
  enabled: true
  image:
    repository: temporalio/admin-tools
    tag: 1.22.4
    pullPolicy: IfNotPresent

  service:
    type: ClusterIP
    port: 22
    annotations: {}
  podLabels: {}
  podAnnotations:
    reloader.stakater.com/auto: "true"
  nodeSelector: {}
  tolerations: []
  affinity: {}
  additionalEnv: []
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 128Mi
  containerSecurityContext: {}
  securityContext: {}
  podDisruptionBudget: {}

web:
  enabled: true
  additionalEnv:
    - name: TEMPORAL_UI_PUBLIC_PATH
      value: /internal/workload
  config:
    # server/config.yml file content
    auth:
      enabled: false
    routing:
      default_to_namespace: ${namespace}
      issue_report_link: "https://stackgen.com/#support"

  replicaCount: 2
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 128Mi

  image:
    repository: temporalio/ui
    tag: 2.27.3
    pullPolicy: IfNotPresent

  service:
    # set type to NodePort if access to web needs access from outside the cluster
    # for more info see https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
    type: ClusterIP
    port: 8080
    annotations: {}
    # loadBalancerIP:

  ingress:
    enabled: true
    className: nginx
    annotations:
      nginx.org/mergeable-ingress-type: minion
      nginx.org/location-snippets: |
        rewrite /internal/workload/(.*) /$1  break;
    hosts:
      - ${domain}/internal/workload

  podAnnotations:
    reloader.stakater.com/auto: "true"
  podLabels: {}

  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 128Mi

  nodeSelector: {}

  tolerations: []

  affinity: {}

  additionalVolumes: []
  additionalVolumeMounts: []

  containerSecurityContext: {}

  securityContext: {}

schema:
  setup:
    enabled: true
    backoffLimit: 100
  update:
    enabled: true
    backoffLimit: 100
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 128Mi
  containerSecurityContext: {}
  securityContext: {}

elasticsearch:
  enabled: false
  replicas: 3
  persistence:
    enabled: false
  imageTag: 7.17.3
  host: elasticsearch-master-headless
  scheme: http
  port: 9200
  version: "v7"
  logLevel: "error"
  username: ""
  password: ""
  visibilityIndex: "temporal_visibility_v1_dev"

prometheus:
  enabled: false
  nodeExporter:
    enabled: false

grafana:
  enabled: false
  replicas: 1
  testFramework:
    enabled: false
  rbac:
    create: false
    pspEnabled: false
    namespaced: true
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: "default"
          orgId: 1
          folder: ""
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/default
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: TemporalMetrics
          type: prometheus
          url: http://{{ .Release.Name }}-prometheus-server
          access: proxy
          isDefault: true
  dashboards:
    default:
      server-general-github:
        url: https://raw.githubusercontent.com/temporalio/dashboards/helm/server/server-general.json
        datasource: TemporalMetrics
      sdk-general-github:
        url: https://raw.githubusercontent.com/temporalio/dashboards/helm/sdk/sdk-general.json
        datasource: TemporalMetrics
      misc-advanced-visibility-specific-github:
        url: https://raw.githubusercontent.com/temporalio/dashboards/helm/misc/advanced-visibility-specific.json
        datasource: TemporalMetrics
      misc-clustermonitoring-kubernetes-github:
        url: https://raw.githubusercontent.com/temporalio/dashboards/helm/misc/clustermonitoring-kubernetes.json
        datasource: TemporalMetrics
      misc-frontend-service-specific-github:
        url: https://raw.githubusercontent.com/temporalio/dashboards/helm/misc/frontend-service-specific.json
        datasource: TemporalMetrics
      misc-history-service-specific-github:
        url: https://raw.githubusercontent.com/temporalio/dashboards/helm/misc/history-service-specific.json
        datasource: TemporalMetrics
      misc-matching-service-specific-github:
        url: https://raw.githubusercontent.com/temporalio/dashboards/helm/misc/matching-service-specific.json
        datasource: TemporalMetrics
      misc-worker-service-specific-github:
        url: https://raw.githubusercontent.com/temporalio/dashboards/helm/misc/worker-service-specific.json
        datasource: TemporalMetrics

cassandra:
  enabled: false
  persistence:
    enabled: false
  image:
    repo: cassandra
    tag: 3.11.3
    pullPolicy: IfNotPresent
  config:
    cluster_size: 3
    ports:
      cql: 9042
    num_tokens: 4
    max_heap_size: 512M
    heap_new_size: 128M
    seed_size: 0
  service:
    type: ClusterIP

mysql:
  enabled: false
