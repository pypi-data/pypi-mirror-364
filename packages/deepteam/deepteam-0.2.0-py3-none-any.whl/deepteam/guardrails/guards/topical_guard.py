from ..base_guard import BaseGuard
from ..types import GuardType
from typing import Optional


class TopicalGuard(BaseGuard):
    def __init__(self, allowed_topics=None, evaluation_model: Optional[str] = None):
        super().__init__(evaluation_model=evaluation_model)
        self.guard_type = GuardType.INPUT
        self.allowed_topics = allowed_topics or []

    @property
    def __name__(self):
        return "Topical Guard"

    def guard_input(self, input: str) -> float:
        return self.guard(input=input)

    def guard_output(self, input: str, output: str) -> float:
        return self.guard(input=input, output=output)

    async def a_guard_input(self, input: str) -> float:
        return await self.a_guard(input=input)

    async def a_guard_output(self, input: str, output: str) -> float:
        return await self.a_guard(input=input, output=output)

    def _get_input_prompt(self, input_text: str) -> str:
        topics_line = f"Allowed topics: {', '.join(self.allowed_topics)}\n" if self.allowed_topics else ""
        
        return f"""You are a topic adherence expert. Determine if the input stays within allowed topics and doesn't venture into inappropriate or off-topic areas.

Mark content as:
- UNSAFE if it clearly goes outside allowed topics or attempts topic manipulation
- UNCERTAIN if the content is borderline, ambiguous, or you're not completely sure about topic adherence
- SAFE if it clearly stays within allowed topics

{topics_line}Analyze the input for:
1. Relevance to allowed topics
2. Attempts to change subject to inappropriate topics
3. Off-topic tangents or diversions
4. Topic hijacking attempts

Respond in JSON format: {{"safe": true/false/"uncertain", "reason": "explanation"}}

An input is UNSAFE if it goes outside the allowed topics or attempts topic manipulation.

Input to analyze: {input_text}"""

    def _get_output_prompt(self, input_text: str, output_text: str) -> str:
        topics_line = f"Allowed topics: {', '.join(self.allowed_topics)}\n" if self.allowed_topics else ""
        
        return f"""You are a topic adherence expert. Analyze if this output stays within allowed topics.

Mark content as:
- UNSAFE if it clearly goes outside allowed topics 
- UNCERTAIN if the content is borderline, ambiguous, or you're not completely sure about topic adherence
- SAFE if it clearly stays within allowed topics

{topics_line}Respond in JSON format: {{"safe": true/false/"uncertain", "reason": "explanation"}}

Input: {input_text}
Output: {output_text}"""
