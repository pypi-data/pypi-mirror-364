Metadata-Version: 2.4
Name: polarisllm
Version: 1.1.0
Summary: ğŸŒŸ The Ultimate Multi-Model LLM Runtime Platform - Deploy, manage, and serve 300+ language models with OpenAI-compatible APIs. Built on ms-swift for production-ready performance.
Home-page: https://github.com/polarisllm/polarisLLM
Author: PolarisLLM Team
Author-email: elon@polariscloud.ai
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: fastapi>=0.104.1
Requires-Dist: uvicorn[standard]>=0.24.0
Requires-Dist: aiohttp>=3.9.0
Requires-Dist: pydantic>=2.5.0
Requires-Dist: pyyaml>=6.0
Requires-Dist: psutil>=5.9.0
Requires-Dist: python-multipart>=0.0.6
Requires-Dist: rich>=13.0.0
Provides-Extra: dev
Requires-Dist: pytest; extra == "dev"
Requires-Dist: black; extra == "dev"
Requires-Dist: flake8; extra == "dev"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# ğŸŒŸ PolarisLLM Runtime Engine

**The Ultimate Multi-Model LLM Runtime Platform**

PolarisLLM is a production-ready, high-performance runtime engine that transforms how you deploy and serve Large Language Models. Built on the robust ms-swift framework, it provides seamless OpenAI-compatible APIs while enabling dynamic multi-model serving, making it the perfect solution for developers, researchers, and enterprises who need flexible, scalable LLM infrastructure.

## ğŸ¯ **Why PolarisLLM?**

**ğŸš€ Turn Any Server Into an LLM Powerhouse**
- Deploy multiple models simultaneously on a single machine
- Switch between models without restarts or downtime
- Support for 300+ models including Qwen, Llama, DeepSeek, Mistral, and more

**âš¡ Production-Ready Performance**
- Built on battle-tested ms-swift framework
- Automatic resource management and optimization
- Real-time health monitoring and auto-recovery

**ğŸ”Œ Drop-in OpenAI Compatibility**
- Use existing OpenAI client libraries without modification
- Seamless integration with popular frameworks like LangChain, LlamaIndex
- Perfect for migration from proprietary APIs to self-hosted solutions

## âœ¨ **Key Features**

### ğŸ›ï¸ **Dynamic Model Management**
- **Hot-swap models** without server restarts
- **Concurrent serving** of multiple models on different ports
- **Intelligent resource allocation** and memory management
- **Auto-scaling** based on demand

### ğŸ”— **Universal Compatibility**
- **OpenAI API compatible** - works with existing tools and libraries
- **300+ supported models** from HuggingFace and ModelScope
- **Multi-modal support** - text, vision, code, and audio models
- **Streaming responses** for real-time applications

### ğŸ› ï¸ **Developer Experience**
- **Rich CLI interface** with beautiful status displays
- **RESTful admin APIs** for programmatic control
- **YAML configuration** for easy model definitions
- **Comprehensive logging** and error handling

### ğŸ—ï¸ **Production Ready**
- **Docker containerization** with GPU support
- **Health checks** and automatic recovery
- **Resource monitoring** and performance metrics
- **Horizontal scaling** support

## ğŸ“¦ **Installation**

### ğŸ‰ **Quick Install (Recommended)**
```bash
# Install from PyPI - that's it!
pip install polarisllm

# Start the engine
polaris start
```

### ğŸ³ **Docker Installation**
```bash
# Run with Docker
docker run -p 7860:7860 polarisllm/polarisllm

# Or with docker-compose
git clone https://github.com/polarisllm/polarisLLM.git
cd polarisLLM
docker-compose up -d
```

### ğŸ› ï¸ **Development Installation**
```bash
git clone https://github.com/polarisllm/polarisLLM.git
cd polarisLLM
pip install -e .
pip install ms-swift[llm] --upgrade
```

## ğŸš€ **Quick Start Guide**

### **Step 1: Install & Start** âš¡
```bash
pip install polarisllm
polaris start
```
*Server starts on `http://localhost:7860` with beautiful web interface*

### **Step 2: Load Your First Model** ğŸ¤–
```bash
# Load a powerful 7B chat model
polaris load qwen2.5-7b-instruct

# Or load a vision model for image understanding
polaris load deepseek-vl-7b-chat

# Check status
polaris status
```

### **Step 3: Start Chatting** ğŸ’¬
```bash
# Using curl
curl -X POST "http://localhost:7860/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5-7b-instruct", 
    "messages": [{"role": "user", "content": "Explain quantum computing"}],
    "stream": true
  }'
```

### **Step 4: Use with Your Favorite Tools** ğŸ”§
```python
# Works with OpenAI Python client
import openai

client = openai.OpenAI(
    base_url="http://localhost:7860/v1",
    api_key="not-required"  # No API key needed!
)

response = client.chat.completions.create(
    model="qwen2.5-7b-instruct",
    messages=[{"role": "user", "content": "Write a Python function to sort a list"}]
)
print(response.choices[0].message.content)
```

## ğŸ® **Real-World Examples**

### **Example 1: Multi-Model AI Assistant**
```bash
# Load different specialized models
polaris load qwen2.5-7b-instruct      # General chat
polaris load deepseek-coder-6.7b      # Code generation  
polaris load deepseek-vl-7b-chat      # Vision understanding

# Use different models for different tasks
curl -X POST "http://localhost:7860/v1/chat/completions" \
  -d '{"model": "deepseek-coder-6.7b", "messages": [{"role": "user", "content": "Write a REST API in FastAPI"}]}'
```

### **Example 2: LangChain Integration**
```python
from langchain.llms import OpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# Connect to your local PolarisLLM
llm = OpenAI(
    openai_api_base="http://localhost:7860/v1",
    openai_api_key="not-required",
    model_name="qwen2.5-7b-instruct"
)

# Use with LangChain as usual
prompt = PromptTemplate(template="Explain {topic} in simple terms")
chain = LLMChain(llm=llm, prompt=prompt)
result = chain.run(topic="machine learning")
```

### **Example 3: Batch Processing**
```python
import asyncio
import aiohttp

async def process_documents():
    models = ["qwen2.5-7b-instruct", "deepseek-coder-6.7b"]
    
    for model in models:
        async with aiohttp.ClientSession() as session:
            async with session.post(
                "http://localhost:7860/v1/chat/completions",
                json={
                    "model": model,
                    "messages": [{"role": "user", "content": "Analyze this document..."}]
                }
            ) as response:
                result = await response.json()
                print(f"{model}: {result}")
```

## ğŸ› ï¸ CLI Usage

The CLI provides convenient model management:

```bash
# Check runtime status
python cli.py status

# List available models
python cli.py list

# List running models
python cli.py list --running

# Load a model with custom arguments
python cli.py load qwen2.5-7b-instruct --swift-args max_length=4096

# Unload a model
python cli.py unload qwen2.5-7b-instruct

# Get model information
python cli.py info deepseek-vl-7b-chat
```

## ğŸ¤– **Supported Models (300+)**

PolarisLLM supports the entire ms-swift model ecosystem. Here are some popular choices:

### **ğŸ¯ General Chat Models**
- **Qwen2.5-7B-Instruct**: Alibaba's flagship model - excellent for general tasks
- **Llama-3.1-8B-Instruct**: Meta's latest - great reasoning capabilities  
- **Mistral-7B-Instruct**: Efficient and fast - perfect for production
- **DeepSeek-V2.5**: Advanced reasoning and long context support

### **ğŸ’» Code Generation Models** 
- **DeepSeek-Coder-6.7B**: State-of-the-art code generation
- **CodeQwen1.5-7B**: Multi-language programming support
- **Qwen2.5-Coder-7B**: Latest coding model with enhanced capabilities

### **ğŸ‘ï¸ Vision-Language Models**
- **DeepSeek-VL-7B-Chat**: Advanced vision understanding
- **Qwen2-VL-7B-Instruct**: Multi-modal reasoning 
- **LLaVA-NeXT**: Image analysis and description

### **ğŸµ Multi-Modal Models**
- **Qwen2-Audio**: Speech and audio understanding
- **Qwen2.5-Omni**: Text, image, and audio in one model

*See the complete list of 300+ supported models in our [Model Catalog](https://github.com/polarisllm/models)*

## ğŸ”§ Configuration

### Runtime Configuration (`config/runtime.yaml`)

```yaml
host: "0.0.0.0"
port_range_start: 8000
port_range_end: 8100
max_concurrent_models: 5
model_timeout: 300
env_vars:
  CUDA_VISIBLE_DEVICES: "0"
  HF_HUB_CACHE: "./cache/huggingface"
```

### Model Configuration (`config/models/*.yaml`)

```yaml
name: "custom-model"
model_id: "path/to/model"
model_type: "qwen2_5"
template: "qwen2_5"
description: "Custom model description"
tags: ["chat", "custom"]
swift_args:
  max_length: 8192
  temperature: 0.7
```

## ğŸŒ API Endpoints

### OpenAI Compatible Endpoints

- `POST /v1/chat/completions` - Create chat completion
- `GET /v1/models` - List available models

### Admin Endpoints

- `POST /admin/models/load` - Load a model
- `POST /admin/models/{model_name}/unload` - Unload a model
- `GET /admin/models/{model_name}/status` - Get model status
- `GET /admin/status` - Get runtime status
- `GET /admin/models/available` - List available model configurations
- `GET /admin/models/running` - List running models

### Utility Endpoints

- `GET /health` - Health check
- `GET /` - API information

## ğŸ³ Docker Deployment

### Basic Deployment

```bash
docker-compose up -d
```

### With GPU Support

1. Install nvidia-docker2
2. Uncomment GPU section in docker-compose.yml
3. Start with GPU access:

```bash
docker-compose up -d
```

### With Redis Cache

```bash
docker-compose --profile with-cache up -d
```

## ğŸ“Š Monitoring

### Health Checks

The runtime includes built-in health monitoring:

```bash
curl http://localhost:7860/health
```

### Resource Monitoring

View real-time resource usage:

```bash
python cli.py status
```

### Logs

```bash
# Local logs
tail -f polaris.log

# Docker logs
docker-compose logs -f polaris-runtime
```

## ğŸ”Œ Integration Examples

### Python Client

```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:7860/v1",
    api_key="not-required"
)

response = client.chat.completions.create(
    model="deepseek-vl-7b-chat",
    messages=[
        {"role": "user", "content": "Hello!"}
    ]
)

print(response.choices[0].message.content)
```

### JavaScript Client

```javascript
import OpenAI from 'openai';

const client = new OpenAI({
    baseURL: 'http://localhost:7860/v1',
    apiKey: 'not-required'
});

const completion = await client.chat.completions.create({
    model: 'qwen2.5-7b-instruct',
    messages: [
        { role: 'user', content: 'Hello!' }
    ]
});

console.log(completion.choices[0].message.content);
```

## ğŸ›¡ï¸ Production Considerations

1. **Resource Management**: Monitor GPU/CPU usage and memory consumption
2. **Load Balancing**: Use reverse proxy for multiple runtime instances
3. **Security**: Add authentication for admin endpoints
4. **Logging**: Configure structured logging for production monitoring
5. **Scaling**: Use Kubernetes for large-scale deployments

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸŒŸ **Use Cases**

### **ğŸ¢ Enterprise & Startups**
- **Private AI Infrastructure**: Keep models in-house for data privacy
- **Cost Optimization**: Reduce API costs by 90% compared to cloud providers
- **Multi-tenant Applications**: Serve different models to different customers
- **A/B Testing**: Compare model performance with easy switching

### **ğŸ‘¨â€ğŸ’» Developers & Researchers**
- **Local Development**: Test AI features without API costs
- **Model Comparison**: Evaluate different models on the same dataset
- **Fine-tuning Pipeline**: Deploy custom fine-tuned models
- **Prototype Rapidly**: Build AI applications with zero setup friction

### **ğŸ“š Educational & Training**
- **AI Courses**: Provide students with hands-on LLM experience
- **Research Projects**: Access to latest models for academic research
- **Hackathons**: Quick setup for AI-focused competitions

## ğŸ† **Why Choose PolarisLLM Over Alternatives?**

| Feature | PolarisLLM | Ollama | text-generation-webui | OpenAI API |
|---------|------------|---------|----------------------|------------|
| **Multi-model serving** | âœ… Concurrent | âš ï¸ Sequential | âŒ Single | âœ… Multiple |
| **OpenAI compatibility** | âœ… Full | âŒ Limited | âŒ None | âœ… Native |
| **Model variety** | âœ… 300+ models | âš ï¸ GGUF only | âš ï¸ Limited | âš ï¸ Proprietary |
| **Production ready** | âœ… Yes | âš ï¸ Basic | âŒ No | âœ… Yes |
| **Self-hosted** | âœ… Yes | âœ… Yes | âœ… Yes | âŒ No |
| **Cost** | âœ… Free | âœ… Free | âœ… Free | ğŸ’° Expensive |
| **Setup time** | âœ… < 2 minutes | âš ï¸ 5-10 min | âŒ 30+ min | âœ… Instant |

## ğŸ¤ **Community & Support**

### **Getting Help**
- ğŸ“– **Documentation**: Comprehensive guides and API reference
- ğŸ’¬ **GitHub Discussions**: Community Q&A and feature requests  
- ğŸ› **Issue Tracking**: Bug reports and feature requests
- ğŸ“§ **Email Support**: contact@polarisllm.dev

### **Contributing**
- ğŸ´ **Fork & PR**: Contributions welcome!
- ğŸ§ª **Testing**: Help test new models and features
- ğŸ“ **Documentation**: Improve guides and examples
- ğŸŒ **Translation**: Help localize for global users

### **Stay Updated**
- â­ **Star us on GitHub**: Get notifications for releases
- ğŸ¦ **Follow @PolarisLLM**: Latest updates and tips
- ğŸ“° **Newsletter**: Monthly model updates and tutorials

## ğŸ”„ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CLI Client    â”‚    â”‚   FastAPI Server â”‚    â”‚  Runtime Core   â”‚
â”‚                 â”‚â”€â”€â”€â”€â”‚                  â”‚â”€â”€â”€â”€â”‚                 â”‚
â”‚ - Model Mgmt    â”‚    â”‚ - OpenAI API     â”‚    â”‚ - Model Manager â”‚
â”‚ - Status Check  â”‚    â”‚ - Admin API      â”‚    â”‚ - Process Mgmt  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                        â”‚
                                â”‚                        â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  Model Instance â”‚    â”‚  Model Instance   â”‚
                       â”‚                 â”‚    â”‚                   â”‚
                       â”‚ - ms-swift      â”‚    â”‚ - ms-swift        â”‚
                       â”‚ - Port 8000     â”‚    â”‚ - Port 8001       â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ‰ Acknowledgments

- Built on the excellent [ms-swift](https://github.com/modelscope/swift) framework
- Inspired by OpenAI's API design
- Thanks to the open-source LLM community
