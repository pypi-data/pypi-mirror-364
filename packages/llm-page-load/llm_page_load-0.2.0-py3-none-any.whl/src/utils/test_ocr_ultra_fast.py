#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¶…é«˜æ•ˆçš„OCRå¼€å§‹å¸§æ£€æµ‹æµ‹è¯•
"""

import os
import sys
import re
import cv2
import numpy as np
import time
import gc
import json
import threading
import datetime
import uuid
import requests
import base64
from concurrent.futures import ThreadPoolExecutor, as_completed
# from paddleocr import PaddleOCR  # æ³¨é‡Šæ‰æœ¬åœ°PaddleOCR

# ä¿®å¤tqdmçš„AttributeErroré—®é¢˜
import warnings
warnings.filterwarnings("ignore", message=".*'tqdm' object has no attribute 'pos'.*")

# æ·»åŠ ground truthè®¡ç®—ç›¸å…³çš„è¾…åŠ©å‡½æ•°
def parse_case_key_for_gt(case_key: str) -> tuple:
    """
    è§£æcase_keyï¼Œæå–è§†é¢‘åå’Œæ—¶é—´ä¿¡æ¯
    æ ¼å¼: "2025-07-03_1751514078822,VID_20250703_113456/1_15s"
    """
    try:
        # åˆ†å‰²è§†é¢‘åå’Œæ—¶é—´
        video_part, time_part = case_key.split('/')
        
        # è§£ææ—¶é—´éƒ¨åˆ† "1_37s" -> 1.37
        time_str = time_part.replace('s', '')
        if '_' in time_str:
            # å¤„ç† "1_37s" æ ¼å¼ï¼Œè½¬æ¢ä¸º 1.37
            action_time_str, decimal_str = time_str.split('_')
            action_time = float(action_time_str) + float(decimal_str) / 100.0
        else:
            action_time = float(time_str)
            
        return video_part, action_time
    except Exception as e:
        print(f"è§£æcase_keyå¤±è´¥: {case_key}, é”™è¯¯: {e}")
        return None, None

def find_action_info_file_for_gt(video_name: str, action_time: float, processed_dir: str) -> str:
    """
    åœ¨processedç›®å½•ä¸­æŸ¥æ‰¾å¯¹åº”çš„action_info.jsonæ–‡ä»¶
    æ”¯æŒä¸¤ç§æ•°æ®æ ¼å¼ï¼š
    1. downloaded_videos_processed (æ–°æ ¼å¼)
    2. all_marked_data_processed_action_items (æ—§æ ¼å¼)
    """
    # æ—¶é—´å­—ç¬¦ä¸²ç”Ÿæˆæ–¹å¼ä¿®æ­£ï¼Œé¿å…å››èˆäº”å…¥
    def get_time_str(action_time):
        int_part = int(action_time)
        decimal_part = int(round((action_time - int_part) * 100))
        return f"{int_part}_{decimal_part}s"

    # é¦–å…ˆåœ¨downloaded_videos_processedä¸­æŸ¥æ‰¾
    if os.path.exists(processed_dir):
        # æŸ¥æ‰¾åŒ¹é…çš„è§†é¢‘ç›®å½•
        for item in os.listdir(processed_dir):
            item_path = os.path.join(processed_dir, item)
            if not os.path.isdir(item_path):
                continue
                
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡è§†é¢‘ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
            if video_name == item:
                # æŸ¥æ‰¾å¯¹åº”æ—¶é—´ç‚¹çš„action_info.json
                for subitem in os.listdir(item_path):
                    subitem_path = os.path.join(item_path, subitem)
                    if not os.path.isdir(subitem_path):
                        continue
                        
                    # æ£€æŸ¥æ—¶é—´æ˜¯å¦åŒ¹é…
                    time_str = get_time_str(action_time)
                    if time_str in subitem:
                        action_info_path = os.path.join(subitem_path, "action_info.json")
                        if os.path.exists(action_info_path):
                            return action_info_path
    
    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œåœ¨all_marked_data_processed_action_itemsä¸­æŸ¥æ‰¾
    old_format_dir = "all_marked_data_processed_action_items"
    if os.path.exists(old_format_dir):
        # æŸ¥æ‰¾åŒ¹é…çš„è§†é¢‘ç›®å½•
        for item in os.listdir(old_format_dir):
            item_path = os.path.join(old_format_dir, item)
            if not os.path.isdir(item_path):
                continue
                
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡è§†é¢‘ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
            if video_name == item:
                # æŸ¥æ‰¾å¯¹åº”æ—¶é—´ç‚¹çš„action_info.json
                for subitem in os.listdir(item_path):
                    subitem_path = os.path.join(item_path, subitem)
                    if not os.path.isdir(subitem_path):
                        continue
                        
                    # æ£€æŸ¥æ—¶é—´æ˜¯å¦åŒ¹é…
                    time_str = get_time_str(action_time)
                    if time_str in subitem:
                        action_info_path = os.path.join(subitem_path, "action_info.json")
                        if os.path.exists(action_info_path):
                            return action_info_path
    
    return None

def calculate_ground_truth_start_frame_for_gt(action_info_path: str) -> int:
    """
    ä»action_info.jsonè®¡ç®—å¼€å§‹å¸§çš„ground truthå¸§ç´¢å¼•
    å¯¹äºå¼€å§‹å¸§æ£€æµ‹ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°åŠ¨ä½œå¼€å§‹çš„æ—¶é—´ç‚¹
    """
    try:
        with open(action_info_path, 'r', encoding='utf-8') as f:
            action_info = json.load(f)
        
        # æå–å…³é”®ä¿¡æ¯
        original_action = action_info.get('original_action_item', {})
        extraction_params = action_info.get('extraction_parameters', {})
        
        # å°è¯•å¤šç§æ—¶é—´å­—æ®µï¼Œä¼˜å…ˆçº§ï¼šaction_time > marked_start_time > marked_response_time
        action_time = original_action.get('action_time')
        marked_start_time = original_action.get('marked_start_time')
        marked_response_time = original_action.get('marked_response_time')
        
        extract_start_time_sec = extraction_params.get('extract_start_time_sec')
        fps = extraction_params.get('fps', 30)
        
        # é€‰æ‹©åˆé€‚çš„æ—¶é—´ä½œä¸ºå¼€å§‹æ—¶é—´
        start_time = None
        if action_time is not None:
            start_time = action_time
        elif marked_start_time is not None:
            start_time = marked_start_time
        elif marked_response_time is not None:
            start_time = marked_response_time
        
        if start_time is None or extract_start_time_sec is None:
            print(f"ç¼ºå°‘å¿…è¦çš„æ—¶é—´ä¿¡æ¯: action_time={action_time}, marked_start_time={marked_start_time}, marked_response_time={marked_response_time}, extract_start_time_sec={extract_start_time_sec}")
            return None
        
        # è®¡ç®—å¼€å§‹å¸§çš„ground truthå¸§ç´¢å¼•
        gt_start_frame = int(round((start_time - extract_start_time_sec) * fps))
        
        # ç¡®ä¿å¸§ç´¢å¼•ä¸ä¸ºè´Ÿæ•°
        if gt_start_frame < 0:
            print(f"è­¦å‘Š: è®¡ç®—çš„å¸§ç´¢å¼•ä¸ºè´Ÿæ•° ({gt_start_frame})ï¼Œè®¾ç½®ä¸º0")
            gt_start_frame = 0
        
        return gt_start_frame
        
    except Exception as e:
        print(f"è®¡ç®—å¼€å§‹å¸§ground truthå¤±è´¥: {e}")
        return None

# çº¿ç¨‹å®‰å…¨çš„è®¡æ•°å™¨
class ThreadSafeCounter:
    def __init__(self):
        self._value = 0
        self._lock = threading.Lock()
    
    def increment(self):
        with self._lock:
            self._value += 1
    
    def get_value(self):
        with self._lock:
            return self._value
    
    def set_value(self, value):
        with self._lock:
            self._value = value

# å®æ—¶ç»“æœä¿å­˜å™¨
class RealTimeResultSaver:
    def __init__(self, output_dir):
        self.output_dir = output_dir
        self.results_file = os.path.join(output_dir, "results.json")
        self.log_file = os.path.join(output_dir, "real_time_log.txt")
        self.results_buffer = []  # ç»“æœç¼“å†²åŒº
        self.buffer_size = 10     # ç¼“å†²åŒºå¤§å°
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–ç»“æœæ–‡ä»¶
        with open(self.results_file, "w", encoding="utf-8") as f:
            json.dump({"results": [], "summary": {}}, f, indent=2, ensure_ascii=False)
        
        with open(self.log_file, "w", encoding="utf-8") as f:
            f.write(f"=== å®æ—¶æ—¥å¿— - {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\n")
    
    def save_result(self, result):
        """ä¿å­˜å•ä¸ªç»“æœ"""
        # æ·»åŠ æ–°ç»“æœåˆ°ç¼“å†²åŒºï¼ˆä¸åŒ…å«frame_textsï¼Œé¿å…æ–‡ä»¶è¿‡å¤§ï¼‰
        result_copy = result.copy()
        if "frame_texts" in result_copy:
            del result_copy["frame_texts"]  # ä»ä¸»ç»“æœä¸­ç§»é™¤è¯¦ç»†æ–‡æœ¬
        
        self.results_buffer.append(result_copy)
        
        # å½“ç¼“å†²åŒºæ»¡æ—¶ï¼Œæ‰¹é‡å†™å…¥æ–‡ä»¶
        if len(self.results_buffer) >= self.buffer_size:
            self._flush_results_buffer()
        
        # å•ç‹¬ä¿å­˜å¸§æ–‡æœ¬ä¿¡æ¯åˆ°æµ‹ä¾‹ç›®å½•
        if "frame_texts" in result and result["frame_texts"]:
            self.save_frame_texts_to_case_dir(result["case_key"], result["frame_texts"])
    
    def _flush_results_buffer(self):
        """åˆ·æ–°ç»“æœç¼“å†²åŒºåˆ°æ–‡ä»¶"""
        if not self.results_buffer:
            return
            
        # è¯»å–ç°æœ‰ç»“æœ
        with open(self.results_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        # æ·»åŠ ç¼“å†²åŒºä¸­çš„ç»“æœ
        data["results"].extend(self.results_buffer)
        
        # å†™å›æ–‡ä»¶
        with open(self.results_file, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        # æ¸…ç©ºç¼“å†²åŒº
        self.results_buffer.clear()
    
    def save_frame_texts_to_case_dir(self, case_key, frame_texts):
        """ä¿å­˜å•ä¸ªcaseçš„å¸§æ–‡æœ¬ä¿¡æ¯åˆ°æµ‹ä¾‹ç›®å½•"""
        # åˆ›å»ºæµ‹ä¾‹ç›®å½•
        case_dir = os.path.join(self.output_dir, "case_details", case_key.replace("/", "_"))
        os.makedirs(case_dir, exist_ok=True)
        
        # ä¿å­˜å¸§æ–‡æœ¬æ–‡ä»¶
        frame_texts_file = os.path.join(case_dir, "frame_texts.json")
        case_summary = {
            "case_key": case_key,
            "total_frames": len(frame_texts),
            "frames_with_coordinates": sum(1 for f in frame_texts if f.get("has_coordinates", False)),
            "frames_with_errors": sum(1 for f in frame_texts if "error" in f),
            "processing_timestamp": datetime.datetime.now().isoformat(),
            "frame_details": frame_texts
        }
        
        with open(frame_texts_file, "w", encoding="utf-8") as f:
            json.dump(case_summary, f, indent=2, ensure_ascii=False)
        
        # åˆ›å»ºç®€åŒ–çš„ç»Ÿè®¡æ–‡ä»¶
        stats_file = os.path.join(case_dir, "stats.txt")
        with open(stats_file, "w", encoding="utf-8") as f:
            f.write(f"=== {case_key} ç»Ÿè®¡ä¿¡æ¯ ===\n")
            f.write(f"æ€»å¸§æ•°: {len(frame_texts)}\n")
            f.write(f"åŒ…å«åæ ‡çš„å¸§æ•°: {case_summary['frames_with_coordinates']}\n")
            f.write(f"æœ‰é”™è¯¯çš„å¸§æ•°: {case_summary['frames_with_errors']}\n")
            f.write(f"åæ ‡æ£€æµ‹æˆåŠŸç‡: {case_summary['frames_with_coordinates']/len(frame_texts)*100:.1f}%\n")
            f.write(f"å¤„ç†æ—¶é—´: {case_summary['processing_timestamp']}\n")
            
            # æ˜¾ç¤ºå‰å‡ å¸§çš„æ–‡æœ¬ç¤ºä¾‹
            f.write(f"\n=== å‰5å¸§æ–‡æœ¬ç¤ºä¾‹ ===\n")
            for i, frame_text in enumerate(frame_texts[:5]):
                f.write(f"å¸§ {frame_text['frame_number']} (æ—¶é—´æˆ³: {frame_text['timestamp']:.2f}s):\n")
                f.write(f"  OCRæ–‡æœ¬: {frame_text['ocr_texts']}\n")
                f.write(f"  æœ‰åæ ‡: {frame_text['has_coordinates']}\n")
                if frame_text['coordinate_texts']:
                    f.write(f"  åæ ‡æ–‡æœ¬: {frame_text['coordinate_texts']}\n")
                if 'error' in frame_text:
                    f.write(f"  é”™è¯¯: {frame_text['error']}\n")
                f.write("\n")
    
    def update_summary(self, summary):
        """æ›´æ–°ç»Ÿè®¡æ‘˜è¦"""
        # å…ˆåˆ·æ–°ç¼“å†²åŒº
        self._flush_results_buffer()
        
        with open(self.results_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        data["summary"] = summary
        
        with open(self.results_file, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
    
    def log_message(self, message):
        """è®°å½•æ—¥å¿—æ¶ˆæ¯"""
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        with open(self.log_file, "a", encoding="utf-8") as f:
            f.write(f"[{timestamp}] {message}\n")
    
    def save_config(self, config):
        """ä¿å­˜é…ç½®"""
        config_file = os.path.join(self.output_dir, "config.json")
        with open(config_file, "w", encoding="utf-8") as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
    
    def finalize(self):
        """å®Œæˆæ—¶åˆ·æ–°æ‰€æœ‰ç¼“å†²åŒº"""
        self._flush_results_buffer()

def create_experiment_output_dir():
    """åˆ›å»ºå®éªŒè¾“å‡ºç›®å½•"""
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    output_dir = f"experiment_ocr_start_frame_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    return output_dir

def check_words(text):
    """ä½¿ç”¨ç”¨æˆ·æä¾›çš„æ­£åˆ™è¡¨è¾¾å¼æ£€æŸ¥åæ ‡ä¿¡æ¯ï¼ˆå¼€å§‹å¸§æ£€æµ‹ï¼Œä¸¥æ ¼ç‰ˆæœ¬ï¼‰"""
    # ä¸¥æ ¼ç‰ˆæœ¬ï¼šå¿…é¡»æ˜¯X:æˆ–Y:ï¼Œä¸”åé¢è·Ÿéé›¶æ•°å­—
    pattern = re.compile(r'(?i)\b[XY]:\s*-?(?!0\.?0*$)\d+\.?\d*\b')
    
    # ä½¿ç”¨searchè€Œä¸æ˜¯fullmatchï¼Œå› ä¸ºæ–‡æœ¬å¯èƒ½åŒ…å«å¤šä¸ªåæ ‡
    match = pattern.search(text)
    return match is not None

# æ·»åŠ HTTP OCRæœåŠ¡è°ƒç”¨å‡½æ•°
def call_ocr_service(image, url="http://10.164.6.121:8417/paddelocr"):
    """
    è°ƒç”¨HTTP OCRæœåŠ¡
    
    Args:
        image: OpenCVå›¾åƒæ•°ç»„
        url: OCRæœåŠ¡URL
        
    Returns:
        list: è¯†åˆ«çš„æ–‡æœ¬åˆ—è¡¨
    """
    try:
        # å°†OpenCVçš„BGRæ ¼å¼è½¬æ¢ä¸ºRGBæ ¼å¼
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # ç¼–ç ä¸ºJPEGæ ¼å¼
        _, buffer = cv2.imencode('.jpg', image_rgb)
        img_base64 = base64.b64encode(buffer).decode('utf-8')
        
        # å‡†å¤‡è¯·æ±‚æ•°æ®
        data = {'image': img_base64}
        
        # å‘é€è¯·æ±‚
        response = requests.post(url, data=data, timeout=30)
        
        # æ£€æŸ¥å“åº”
        if response.status_code == 200:
            result = response.json()
            if result.get('success'):
                # æå–æ–‡æœ¬
                texts = []
                for item in result.get('results', []):
                    text = item.get('text', '')
                    if text.strip():  # åªæ·»åŠ éç©ºæ–‡æœ¬
                        texts.append(text)
                return texts
            else:
                print(f"OCRæœåŠ¡è¿”å›é”™è¯¯: {result.get('error')}")
                return []
        else:
            print(f"HTTPè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return []
            
    except Exception as e:
        print(f"è°ƒç”¨OCRæœåŠ¡å‡ºé”™: {e}")
        return []

def llm_narrow_start_frame_ocr_ultra_fast(
    video_path: str, 
    llm_client=None,  # æ·»åŠ LLMå®¢æˆ·ç«¯å‚æ•°
    do_evaluation: bool = False,
    max_frames: int = None,
    skip_frames: int = 1,  # æ›´æ¿€è¿›çš„è·³å¸§
    scale_factor: float = 0.25,  # æ›´å°çš„ç¼©æ”¾
    early_stop: bool = True,  # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåæ ‡å°±åœæ­¢
    save_frame_texts: bool = True,  # æ˜¯å¦ä¿å­˜æ¯å¸§æ–‡æœ¬
    ui_infer_config: dict = None  # UI inferé…ç½®å‚æ•°
):
    """
    è¶…é«˜æ•ˆçš„OCRå¼€å§‹å¸§æ£€æµ‹æ–¹æ³•
    ä½¿ç”¨æ¿€è¿›çš„å†…å­˜ä¼˜åŒ–ç­–ç•¥
    æ”¯æŒè®¾å¤‡ç±»å‹æ£€æµ‹ï¼šå®‰å“è®¾å¤‡ä½¿ç”¨OCRæ–¹æ¡ˆï¼Œå…¶ä»–è®¾å¤‡ä½¿ç”¨UI inferæ–¹æ¡ˆ
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹ï¼ˆç”¨äºUI inferæ–¹æ¡ˆï¼‰
        do_evaluation: æ˜¯å¦è¿›è¡Œè¯„ä¼°è¾“å‡º
        max_frames: æœ€å¤§å¤„ç†å¸§æ•°ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰å¸§
        skip_frames: è·³å¸§æ•°ï¼Œ3è¡¨ç¤ºæ¯3å¸§å¤„ç†1å¸§
        scale_factor: å›¾ç‰‡ç¼©æ”¾å› å­ï¼Œ0.25è¡¨ç¤ºç¼©å°åˆ°åŸæ¥çš„1/4
        early_stop: æ˜¯å¦æ‰¾åˆ°ç¬¬ä¸€ä¸ªåæ ‡å°±åœæ­¢
        save_frame_texts: æ˜¯å¦ä¿å­˜æ¯å¸§æå–çš„æ–‡æœ¬
        ui_infer_config: UI inferé…ç½®å‚æ•°
    
    Returns:
        dict: åŒ…å«æ£€æµ‹ç»“æœçš„å­—å…¸
    """
    
    # å¯¼å…¥è®¾å¤‡ç±»å‹æ£€æµ‹å‡½æ•°
    from .llm_narrow_core import detect_android_by_first_frame, llm_narrow_start_frame_with_ui_infer
    
    print(f"=== å¼€å§‹å¸§æ£€æµ‹: {os.path.basename(video_path)} ===")
    
    # 1. é¦–å…ˆæ£€æµ‹è®¾å¤‡ç±»å‹
    print("æ­£åœ¨æ£€æµ‹è®¾å¤‡ç±»å‹...")
    is_android = detect_android_by_first_frame(video_path)
    
    if is_android:
        print("æ£€æµ‹åˆ°å®‰å“è®¾å¤‡ï¼Œä½¿ç”¨OCRæ–¹æ¡ˆ")
        return _llm_narrow_start_frame_ocr_ultra_fast_android(
            video_path, do_evaluation, max_frames, skip_frames, 
            scale_factor, early_stop, save_frame_texts
        )
    else:
        print("æ£€æµ‹åˆ°éå®‰å“è®¾å¤‡ï¼Œä½¿ç”¨UI Inferæ–¹æ¡ˆ")
        if llm_client is None:
            return {
                'success': False,
                'error': 'éå®‰å“è®¾å¤‡éœ€è¦LLMå®¢æˆ·ç«¯ï¼Œä½†æœªæä¾›',
                'start_frame': None,
                'candidates': []
            }
        
        # ä½¿ç”¨UI inferæ–¹æ¡ˆ
        ui_config = ui_infer_config or {}
        return llm_narrow_start_frame_with_ui_infer(
            video_path=video_path,
            llm_client=llm_client,
            do_evaluation=do_evaluation,
            activity_threshold=ui_config.get('activity_threshold', -0.001),
            merge_window=ui_config.get('merge_window', 3),
            start_threshold=ui_config.get('start_threshold', -0.0001),
            end_threshold=ui_config.get('end_threshold', -0.00003),
            ssim_threshold=ui_config.get('ssim_threshold', 0.995),
            model=ui_config.get('model', "anthropic.claude-3.7-sonnet"),
            max_voting_rounds=ui_config.get('max_voting_rounds', 10),
            temperature=ui_config.get('temperature', 0.1),
            remove_background=ui_config.get('remove_background', False),
            enable_diff=ui_config.get('enable_diff', False),
            ui_infer_max_retries=ui_config.get('ui_infer_max_retries', 3),
            ui_infer_retry_delay=ui_config.get('ui_infer_retry_delay', 2)
        )


def _llm_narrow_start_frame_ocr_ultra_fast_android(
    video_path: str, 
    do_evaluation: bool = False,
    max_frames: int = None,
    skip_frames: int = 1,  # æ›´æ¿€è¿›çš„è·³å¸§
    scale_factor: float = 0.25,  # æ›´å°çš„ç¼©æ”¾
    early_stop: bool = True,  # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåæ ‡å°±åœæ­¢
    save_frame_texts: bool = True  # æ˜¯å¦ä¿å­˜æ¯å¸§æ–‡æœ¬
):
    """
    å®‰å“è®¾å¤‡çš„OCRå¼€å§‹å¸§æ£€æµ‹æ–¹æ³•ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
    """
    
    # åˆå§‹åŒ–PaddleOCRï¼Œä½¿ç”¨æ›´è½»é‡çš„é…ç½®
    # ocr = PaddleOCR( # æ³¨é‡Šæ‰æœ¬åœ°PaddleOCR
    #     use_doc_orientation_classify=False, 
    #     use_doc_unwarping=False, 
    #     use_textline_orientation=False
    # )
    
    # æ‰“å¼€è§†é¢‘æ–‡ä»¶
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return {
            'success': False,
            'error': f'æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}',
            'start_frame': None,
            'candidates': [],
            'frame_texts': []
        }
    
    # è·å–è§†é¢‘ä¿¡æ¯
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    if do_evaluation:
        print(f"è§†é¢‘ä¿¡æ¯:")
        print(f"  æ€»å¸§æ•°: {total_frames}")
        print(f"  FPS: {fps:.2f}")
        print(f"  åˆ†è¾¨ç‡: {width}x{height}")
        print(f"  è·³å¸§æ•°: {skip_frames}")
        print(f"  ç¼©æ”¾å› å­: {scale_factor}")
        print(f"  æ—©æœŸåœæ­¢: {early_stop}")
        if max_frames:
            print(f"  æœ€å¤§å¤„ç†å¸§æ•°: {max_frames}")
    
    candidates = []
    frame_count = 0
    processed_frames = 0
    start_time = time.time()
    
    # è®°å½•æ¯å¸§æå–çš„æ–‡æœ¬
    frame_texts = []
    
    # ç›´æ¥åœ¨å†…å­˜ä¸­å¤„ç†å¸§ï¼Œä¸ä¿å­˜ä¸´æ—¶æ–‡ä»¶
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§å¸§æ•°é™åˆ¶
        if max_frames and processed_frames >= max_frames:
            break
        
        # è·³å¸§å¤„ç†
        if frame_count % skip_frames != 0:
            frame_count += 1
            continue
        
        processed_frames += 1
        
        if do_evaluation and processed_frames % 5 == 0:
            elapsed_time = time.time() - start_time
            fps_processed = processed_frames / elapsed_time if elapsed_time > 0 else 0
            print(f"å¤„ç†å¸§ {processed_frames}: å¸§å· {frame_count}, å¤„ç†é€Ÿåº¦: {fps_processed:.1f} fps")
        
        try:
            # å›¾ç‰‡ç¼©æ”¾ä»¥å‡å°‘å†…å­˜ä½¿ç”¨å’Œæé«˜å¤„ç†é€Ÿåº¦
            if scale_factor != 1.0:
                new_width = int(width * scale_factor)
                new_height = int(height * scale_factor)
                frame = cv2.resize(frame, (new_width, new_height))
            
            # å°†OpenCVçš„BGRæ ¼å¼è½¬æ¢ä¸ºRGBæ ¼å¼
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            if do_evaluation:
                print(f"  å¸§ {frame_count} - å¼€å§‹OCRè¯†åˆ«...")
                print(f"  å›¾ç‰‡å°ºå¯¸: {frame_rgb.shape}")
                print(f"  å›¾ç‰‡æ•°æ®ç±»å‹: {frame_rgb.dtype}")
            
            # è°ƒç”¨HTTP OCRæœåŠ¡
            try:
                rec_texts = call_ocr_service(frame, url="http://10.164.6.121:8417/paddelocr")
                if do_evaluation:
                    print(f"  å¸§ {frame_count} - OCRè¯†åˆ«å®Œæˆ")
                    print(f"  æå–çš„æ–‡æœ¬: {rec_texts}")
            except Exception as ocr_error:
                if do_evaluation:
                    print(f"  å¸§ {frame_count} - OCRè¯†åˆ«å¤±è´¥: {ocr_error}")
                    print(f"  OCRé”™è¯¯ç±»å‹: {type(ocr_error).__name__}")
                    import traceback
                    traceback.print_exc()
                raise ocr_error
            
            # è®°å½•æ¯å¸§çš„æ–‡æœ¬ä¿¡æ¯
            frame_text_info = {
                'frame_number': frame_count,
                'frame_index': processed_frames - 1,
                'timestamp': frame_count / fps if fps > 0 else 0,
                'ocr_texts': rec_texts,
                'has_coordinates': False,
                'coordinate_texts': []
            }
            
            # åˆ¤æ–­æ˜¯å¦å­˜åœ¨ç‚¹å‡»åæ ‡
            has_coordinates = False
            coordinate_texts = []
            for text in rec_texts:
                if check_words(text):
                    has_coordinates = True
                    coordinate_texts.append(text)
                    break
            
            frame_text_info['has_coordinates'] = has_coordinates
            frame_text_info['coordinate_texts'] = coordinate_texts
            
            # ä¿å­˜å¸§æ–‡æœ¬ä¿¡æ¯
            if save_frame_texts:
                frame_texts.append(frame_text_info)
            
            if has_coordinates:
                candidates.append({
                    'frame_number': frame_count,
                    'frame_index': processed_frames - 1,
                    'timestamp': frame_count / fps if fps > 0 else 0,
                    'ocr_texts': rec_texts,
                    'coordinate_texts': coordinate_texts
                })
                
                if do_evaluation:
                    print(f"  æ£€æµ‹åˆ°åæ ‡ä¿¡æ¯: {coordinate_texts}")
                    print(f"  æ‰€æœ‰OCRæ–‡æœ¬: {rec_texts}")
                    print(f"  æ—¶é—´æˆ³: {frame_count / fps:.2f}s")
                
                # æ—©æœŸåœæ­¢ï¼šæ‰¾åˆ°ç¬¬ä¸€ä¸ªåæ ‡å°±åœæ­¢
                if early_stop:
                    break
        
        except Exception as e:
            if do_evaluation:
                import traceback
                print(f"  å¤„ç†å¸§ {frame_count} æ—¶å‡ºé”™: {e}")
                print(f"  é”™è¯¯ç±»å‹: {type(e).__name__}")
                print(f"  é”™è¯¯è¯¦æƒ…:")
                traceback.print_exc()
            
            # è®°å½•é”™è¯¯å¸§ä¿¡æ¯
            if save_frame_texts:
                frame_texts.append({
                    'frame_number': frame_count,
                    'frame_index': processed_frames - 1,
                    'timestamp': frame_count / fps if fps > 0 else 0,
                    'ocr_texts': [],
                    'has_coordinates': False,
                    'coordinate_texts': [],
                    'error': str(e)
                })
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼Œé‡Šæ”¾å†…å­˜
        if processed_frames % 10 == 0:
            gc.collect()
        
        frame_count += 1
    
    # é‡Šæ”¾è§†é¢‘èµ„æº
    cap.release()
    
    # é€‰æ‹©ç¬¬ä¸€ä¸ªæ£€æµ‹åˆ°åæ ‡çš„å¸§ä½œä¸ºå¼€å§‹å¸§
    start_frame = candidates[0] if candidates else None
    
    total_time = time.time() - start_time
    
    result = {
        'success': True,
        'start_frame': start_frame,
        'candidates': candidates,
        'total_frames': total_frames,
        'processed_frames': processed_frames,
        'processing_time': total_time,
        'fps_processed': processed_frames / total_time if total_time > 0 else 0,
        'video_info': {
            'fps': fps,
            'width': width,
            'height': height,
            'duration': total_frames / fps if fps > 0 else 0
        },
        'frame_texts': frame_texts if save_frame_texts else [],
        'method_used': 'ocr_ultra_fast_android'
    }
    
    # è¯„ä¼°æ¨¡å¼è¾“å‡º
    if do_evaluation:
        print(f"\n=== æ£€æµ‹ç»“æœ ===")
        print(f"è§†é¢‘æ€»å¸§æ•°: {total_frames}")
        print(f"å®é™…å¤„ç†å¸§æ•°: {processed_frames}")
        print(f"å¤„ç†æ€»æ—¶é—´: {total_time:.2f} ç§’")
        print(f"å¤„ç†é€Ÿåº¦: {processed_frames / total_time:.1f} fps")
        print(f"æ£€æµ‹åˆ°åæ ‡çš„å¸§æ•°: {len(candidates)}")
        print(f"è®°å½•å¸§æ–‡æœ¬æ•°: {len(frame_texts)}")
        
        if start_frame:
            print(f"\nå¼€å§‹å¸§:")
            print(f"  å¸§å·: {start_frame['frame_number']}")
            print(f"  æ—¶é—´æˆ³: {start_frame['timestamp']:.2f}s")
            print(f"  åæ ‡æ–‡æœ¬: {start_frame['coordinate_texts']}")
            print(f"  æ‰€æœ‰OCRæ–‡æœ¬: {start_frame['ocr_texts']}")
        else:
            print("æœªæ£€æµ‹åˆ°åŒ…å«åæ ‡ä¿¡æ¯çš„å¸§")
    
    return result

def find_video_files(base_dir: str):
    """æŸ¥æ‰¾ç›®å½•ä¸­çš„è§†é¢‘æ–‡ä»¶"""
    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.webm']
    video_files = []
    
    for root, dirs, files in os.walk(base_dir):
        for file in files:
            if any(file.lower().endswith(ext) for ext in video_extensions):
                video_files.append(os.path.join(root, file))
    
    return video_files

def scan_and_select_cases_from_directory(dataset_root_path):
    """
    æ‰«ædataset_root_pathç›®å½•ï¼Œè®©ç”¨æˆ·é€‰æ‹©è¦è¿è¡Œçš„case
    
    Args:
        dataset_root_path: æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„
        
    Returns:
        list: é€‰ä¸­çš„æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨ï¼Œæ ¼å¼ä¸º ["case_name/segment_name", ...]
    """
    print(f"\næ­£åœ¨æ‰«æç›®å½•: {dataset_root_path}")
    
    if not os.path.exists(dataset_root_path):
        print(f"é”™è¯¯ï¼šç›®å½• {dataset_root_path} ä¸å­˜åœ¨")
        return []
    
    # è·å–æ‰€æœ‰caseç›®å½•
    case_dirs = []
    for item in os.listdir(dataset_root_path):
        item_path = os.path.join(dataset_root_path, item)
        if os.path.isdir(item_path) and not item.startswith('.'):
            case_dirs.append(item)
    
    case_dirs.sort()
    
    if not case_dirs:
        print("æœªæ‰¾åˆ°ä»»ä½•caseç›®å½•")
        return []
    
    print(f"\næ‰¾åˆ° {len(case_dirs)} ä¸ªcaseç›®å½•:")
    for i, case_dir in enumerate(case_dirs, 1):
        print(f"{i:2d}. {case_dir}")
    
    # è®©ç”¨æˆ·é€‰æ‹©case
    while True:
        try:
            selection = input(f"\nè¯·é€‰æ‹©caseç¼–å· (1-{len(case_dirs)}) æˆ–è¾“å…¥'all'é€‰æ‹©æ‰€æœ‰case: ").strip().lower()
            
            if selection == 'all':
                selected_case_dirs = case_dirs
                break
            else:
                case_idx = int(selection) - 1
                if 0 <= case_idx < len(case_dirs):
                    selected_case_dirs = [case_dirs[case_idx]]
                    break
                else:
                    print(f"æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥1-{len(case_dirs)}ä¹‹é—´çš„æ•°å­—")
        except ValueError:
            print("è¾“å…¥æ— æ•ˆï¼Œè¯·è¾“å…¥æ•°å­—æˆ–'all'")
    
    # å¯¹äºé€‰ä¸­çš„æ¯ä¸ªcaseï¼Œè·å–å…¶segment
    all_test_cases = []
    for case_dir in selected_case_dirs:
        case_path = os.path.join(dataset_root_path, case_dir)
        
        # è·å–è¯¥caseä¸‹çš„æ‰€æœ‰segment
        segments = []
        for item in os.listdir(case_path):
            item_path = os.path.join(case_path, item)
            if os.path.isdir(item_path) and not item.startswith('.'):
                segments.append(item)
        
        segments.sort()
        
        if not segments:
            print(f"è­¦å‘Šï¼šcase {case_dir} ä¸‹æ²¡æœ‰æ‰¾åˆ°segment")
            continue
        
        print(f"\ncase {case_dir} ä¸­æ‰¾åˆ° {len(segments)} ä¸ªsegment:")
        for i, segment in enumerate(segments, 1):
            print(f"  {i:2d}. {segment}")
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªcaseè¢«é€‰ä¸­ï¼Œè®©ç”¨æˆ·é€‰æ‹©segment
        if len(selected_case_dirs) == 1:
            while True:
                try:
                    seg_selection = input(f"è¯·é€‰æ‹©segmentç¼–å· (1-{len(segments)}) æˆ–è¾“å…¥'all'é€‰æ‹©æ‰€æœ‰segment: ").strip().lower()
                    
                    if seg_selection == 'all':
                        selected_segments = segments
                        break
                    else:
                        seg_idx = int(seg_selection) - 1
                        if 0 <= seg_idx < len(segments):
                            selected_segments = [segments[seg_idx]]
                            break
                        else:
                            print(f"æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥1-{len(segments)}ä¹‹é—´çš„æ•°å­—")
                except ValueError:
                    print("è¾“å…¥æ— æ•ˆï¼Œè¯·è¾“å…¥æ•°å­—æˆ–'all'")
        else:
            # å¤šä¸ªcaseæ—¶ï¼Œé»˜è®¤é€‰æ‹©æ‰€æœ‰segment
            selected_segments = segments
        
        # æ·»åŠ åˆ°æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
        for segment in selected_segments:
            test_case_path = f"{case_dir}/{segment}"
            all_test_cases.append(test_case_path)
    
    return all_test_cases

def process_single_test_case_ocr(args):
    """
    å¤„ç†å•ä¸ªæµ‹è¯•ç”¨ä¾‹çš„å‡½æ•°ï¼Œç”¨äºå¹¶å‘æ‰§è¡ŒOCRå¼€å§‹å¸§æ£€æµ‹
    
    Args:
        args: åŒ…å«æµ‹è¯•ç”¨ä¾‹ä¿¡æ¯å’Œç»“æœä¿å­˜å™¨çš„å…ƒç»„
        
    Returns:
        dict: å¤„ç†ç»“æœ
    """
    test_case, dataset_root_path, case_index, total_cases, result_saver, ocr_config, llm_client = args
    
    # ä¸ºæ¯ä¸ªçº¿ç¨‹ç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦
    thread_id = threading.current_thread().ident
    unique_id = f"{thread_id}_{uuid.uuid4().hex[:8]}"
    
    case_key = test_case["case_key"]
    gt_start_frame = test_case.get("gt_start_frame", None)
    
    message = f"[çº¿ç¨‹{unique_id}] å¤„ç†ç¬¬ {case_index}/{total_cases} ä¸ªæµ‹è¯•ç”¨ä¾‹: {case_key}"
    print(message)
    result_saver.log_message(message)
    
    if gt_start_frame is not None:
        message = f"[çº¿ç¨‹{unique_id}] Ground Truthå¼€å§‹å¸§: {gt_start_frame}"
        print(message)
        result_saver.log_message(message)
    
    # æ„å»ºsegment_dirè·¯å¾„
    if os.path.isabs(case_key) or case_key.startswith(dataset_root_path):
        segment_dir = case_key
    else:
        segment_dir = os.path.join(dataset_root_path, case_key)
    
    if not os.path.exists(segment_dir):
        message = f"[çº¿ç¨‹{unique_id}] è­¦å‘Š: ç›®å½•ä¸å­˜åœ¨ {segment_dir}"
        print(message)
        result_saver.log_message(message)
        
        result = {
            "case_key": case_key,
            "gt_start_frame": gt_start_frame,
            "pred_start_frame": None,
            "diff": None,
            "match_type": "failed",
            "error": "ç›®å½•ä¸å­˜åœ¨",
            "timestamp": datetime.datetime.now().isoformat(),
            "thread_id": unique_id,
            "method_used": "ocr_ultra_fast"
        }
        result_saver.save_result(result)
        return result
    
    try:
        # æŸ¥æ‰¾è§†é¢‘æ–‡ä»¶
        video_files = find_video_files(segment_dir)
        if not video_files:
            message = f"[çº¿ç¨‹{unique_id}] è­¦å‘Š: ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶"
            print(message)
            result_saver.log_message(message)
            
            result = {
                "case_key": case_key,
                "gt_start_frame": gt_start_frame,
                "pred_start_frame": None,
                "diff": None,
                "match_type": "failed",
                "error": "æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶",
                "timestamp": datetime.datetime.now().isoformat(),
                "thread_id": unique_id,
                "method_used": "ocr_ultra_fast"
            }
            result_saver.save_result(result)
            return result
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªè§†é¢‘æ–‡ä»¶
        video_path = video_files[0]
        message = f"[çº¿ç¨‹{unique_id}] ä½¿ç”¨è§†é¢‘æ–‡ä»¶: {os.path.basename(video_path)}"
        print(message)
        result_saver.log_message(message)
        
        # è°ƒç”¨OCRå¼€å§‹å¸§æ£€æµ‹å‡½æ•°
        ocr_result = llm_narrow_start_frame_ocr_ultra_fast(
            video_path=video_path,
            llm_client=llm_client,  # ä¼ é€’LLMå®¢æˆ·ç«¯
            do_evaluation=ocr_config.get("show_realtime_output", False),  # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ˜¾ç¤ºå®æ—¶è¾“å‡º
            max_frames=ocr_config.get("max_frames", 10000),
            skip_frames=ocr_config.get("skip_frames", 1),
            scale_factor=ocr_config.get("scale_factor", 0.25),
            early_stop=ocr_config.get("early_stop", True),
            save_frame_texts=ocr_config.get("save_frame_texts", True), # ä¼ é€’save_frame_textså‚æ•°
            ui_infer_config=ocr_config.get("ui_infer_config") # ä¼ é€’UI inferé…ç½®
        )
        
        if not ocr_result["success"]:
            message = f"[çº¿ç¨‹{unique_id}] OCRæ£€æµ‹å¤±è´¥: {ocr_result.get('error', 'æœªçŸ¥é”™è¯¯')}"
            print(message)
            result_saver.log_message(message)
            
            result = {
                "case_key": case_key,
                "gt_start_frame": gt_start_frame,
                "pred_start_frame": None,
                "diff": None,
                "match_type": "failed",
                "error": ocr_result.get('error', 'OCRæ£€æµ‹å¤±è´¥'),
                "timestamp": datetime.datetime.now().isoformat(),
                "thread_id": unique_id,
                "method_used": "ocr_ultra_fast"
            }
            result_saver.save_result(result)
            return result
        
        # è·å–é¢„æµ‹çš„å¼€å§‹å¸§
        start_frame_info = ocr_result.get("start_frame")
        if start_frame_info is None:
            message = f"[çº¿ç¨‹{unique_id}] æœªæ£€æµ‹åˆ°å¼€å§‹å¸§"
            print(message)
            result_saver.log_message(message)
            
            result = {
                "case_key": case_key,
                "gt_start_frame": gt_start_frame,
                "pred_start_frame": None,
                "diff": None,
                "match_type": "failed",
                "error": "æœªæ£€æµ‹åˆ°å¼€å§‹å¸§",
                "timestamp": datetime.datetime.now().isoformat(),
                "thread_id": unique_id,
                "method_used": "ocr_ultra_fast"
            }
            result_saver.save_result(result)
            return result
        
        pred_start_frame = start_frame_info["frame_number"]
        
        # è®¡ç®—å·®å¼‚ï¼ˆå¦‚æœæœ‰ground truthï¼‰
        if gt_start_frame is not None:
            diff = abs(pred_start_frame - gt_start_frame)
            
            # åˆ¤æ–­åŒ¹é…ç±»å‹
            if diff == 0:
                match_type = "perfect"
            elif diff <= 5:
                match_type = "close"
            elif diff <= 20:
                match_type = "mid"
            else:
                match_type = "other"
        else:
            diff = None
            match_type = "no_gt"
        
        message = f"[çº¿ç¨‹{unique_id}] é¢„æµ‹å¼€å§‹å¸§: {pred_start_frame}"
        print(message)
        result_saver.log_message(message)
        
        if diff is not None:
            message = f"[çº¿ç¨‹{unique_id}] å·®å¼‚: {diff}"
            print(message)
            result_saver.log_message(message)
        
        message = f"[çº¿ç¨‹{unique_id}] åŒ¹é…ç±»å‹: {match_type}"
        print(message)
        result_saver.log_message(message)
        
        result = {
            "case_key": case_key,
            "gt_start_frame": gt_start_frame,
            "pred_start_frame": pred_start_frame,
            "diff": diff,
            "match_type": match_type,
            "error": None,
            "timestamp": datetime.datetime.now().isoformat(),
            "thread_id": unique_id,
            "method_used": "ocr_ultra_fast",
            "ocr_config": ocr_config,
            "video_file": os.path.basename(video_path),
            "processing_time": ocr_result.get("processing_time", 0),
            "processed_frames": ocr_result.get("processed_frames", 0),
            "total_frames": ocr_result.get("total_frames", 0),
            "candidates_count": len(ocr_result.get("candidates", [])),
            "frame_texts": ocr_result.get("frame_texts", []) # æ·»åŠ frame_textsåˆ°ç»“æœä¸­
        }
        result_saver.save_result(result)
        return result
        
    except Exception as e:
        message = f"[çº¿ç¨‹{unique_id}] é”™è¯¯: å¤„ç†æµ‹è¯•ç”¨ä¾‹æ—¶å‡ºé”™: {e}"
        print(message)
        result_saver.log_message(message)
        
        result = {
            "case_key": case_key,
            "gt_start_frame": gt_start_frame,
            "pred_start_frame": None,
            "diff": None,
            "match_type": "failed",
            "error": str(e),
            "timestamp": datetime.datetime.now().isoformat(),
            "thread_id": unique_id,
            "method_used": "ocr_ultra_fast"
        }
        result_saver.save_result(result)
        return result


def test_ocr_basic():
    """æµ‹è¯•OCRåŸºæœ¬åŠŸèƒ½"""
    print("=== æµ‹è¯•OCRåŸºæœ¬åŠŸèƒ½ ===")
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾ç‰‡
    import numpy as np
    
    # åˆ›å»ºä¸€ä¸ªç™½è‰²èƒŒæ™¯çš„å›¾ç‰‡
    test_image = np.ones((100, 300, 3), dtype=np.uint8) * 255
    
    # åœ¨å›¾ç‰‡ä¸Šæ·»åŠ ä¸€äº›æ–‡å­—ï¼ˆæ¨¡æ‹Ÿï¼‰
    cv2.putText(test_image, "X:123 Y:456", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)
    
    # ä¿å­˜æµ‹è¯•å›¾ç‰‡
    cv2.imwrite("test_ocr_image.png", test_image)
    print("åˆ›å»ºæµ‹è¯•å›¾ç‰‡: test_ocr_image.png")
    
    try:
        print("å¼€å§‹OCRæµ‹è¯•...")
        texts = call_ocr_service(test_image, url="http://10.164.6.121:8417/paddelocr")
        print(f"OCRæµ‹è¯•æˆåŠŸï¼Œè¯†åˆ«åˆ° {len(texts)} ä¸ªæ–‡æœ¬")
        print(f"è¯†åˆ«çš„æ–‡æœ¬: {texts}")
        
        return True
    except Exception as e:
        print(f"OCRæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("=== è¶…é«˜æ•ˆOCRå¼€å§‹å¸§æ£€æµ‹æµ‹è¯• ===")
    print("ğŸ’¡ ä¼˜åŒ–ç­–ç•¥:")
    print("  - å†…å­˜ä¸­ç›´æ¥å¤„ç†ï¼Œä¸ä¿å­˜ä¸´æ—¶æ–‡ä»¶")
    print("  - æ¿€è¿›çš„è·³å¸§å¤„ç†ï¼ˆæ¯3å¸§å¤„ç†1å¸§ï¼‰")
    print("  - å¤§å¹…ç¼©å°å›¾ç‰‡å°ºå¯¸ï¼ˆç¼©å°åˆ°1/4ï¼‰")
    print("  - æ—©æœŸåœæ­¢ï¼ˆæ‰¾åˆ°ç¬¬ä¸€ä¸ªåæ ‡å°±åœæ­¢ï¼‰")
    print("  - å¼ºåˆ¶åƒåœ¾å›æ”¶é‡Šæ”¾å†…å­˜")
    print("  - è½»é‡çº§OCRé…ç½®")
    print("  - æ”¯æŒæ‰¹é‡æµ‹è¯•å’Œground truthè®¡ç®—")
    print("  - æ–°å¢è®¾å¤‡ç±»å‹æ£€æµ‹ï¼šå®‰å“è®¾å¤‡ä½¿ç”¨OCRï¼Œå…¶ä»–è®¾å¤‡ä½¿ç”¨UI Infer")
    print()
    
    # ç›´æ¥æµ‹è¯•temp/test_task_001ç›®å½•
    test_frame_dir = "temp/test_task_001"
    
    if not os.path.exists(test_frame_dir):
        print(f"é”™è¯¯: æµ‹è¯•ç›®å½•ä¸å­˜åœ¨: {test_frame_dir}")
        return
    
    print(f"å¼€å§‹æµ‹è¯•ç›®å½•: {test_frame_dir}")
    
    # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯ï¼ˆç”¨äºUI inferæ–¹æ¡ˆï¼‰
    llm_client = None
    try:
        import boto3
        llm_client = boto3.client('bedrock-runtime', region_name='us-east-1')
        print("LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        print("å°†æ— æ³•å¤„ç†éå®‰å“è®¾å¤‡çš„è§†é¢‘")
    
    # é…ç½®å‚æ•°
    ocr_config = {
        "max_frames": 10000,      # æœ€å¤§å¤„ç†å¸§æ•°
        "skip_frames": 1,       # è·³å¸§æ•°
        "scale_factor": 0.7,   # ç¼©æ”¾å› å­
        "early_stop": True,      # æ—©æœŸåœæ­¢
        "save_frame_texts": True, # ä¿å­˜æ¯å¸§æ–‡æœ¬
        "show_realtime_output": True, # æ˜¾ç¤ºå®æ—¶è¾“å‡º
        "ui_infer_config": { # æ–°å¢UI inferé…ç½®
            "activity_threshold": -0.001,
            "merge_window": 3,
            "start_threshold": -0.0001,
            "end_threshold": -0.00003,
            "ssim_threshold": 0.995,
            "model": "anthropic.claude-3.7-sonnet",
            "max_voting_rounds": 10,
            "temperature": 0.1,
            "remove_background": False,
            "enable_diff": False,
            "ui_infer_max_retries": 3,
            "ui_infer_retry_delay": 2
        }
    }
    
    print("é…ç½®å‚æ•°:")
    for key, value in ocr_config.items():
        print(f"  - {key}: {value}")
    
    # ç›´æ¥è¿›è¡Œæµ‹è¯•
    print(f"\nå¼€å§‹æµ‹è¯• {test_frame_dir}...")
    
    # 1. å…ˆæ£€æµ‹è®¾å¤‡ç±»å‹
    print("\n=== è®¾å¤‡ç±»å‹æ£€æµ‹ ===")
    is_android = detect_android_by_first_frame_from_frames(test_frame_dir)
    print(f"è®¾å¤‡ç±»å‹æ£€æµ‹ç»“æœ: {'å®‰å“è®¾å¤‡' if is_android else 'éå®‰å“è®¾å¤‡'}")
    
    # 2. æ ¹æ®è®¾å¤‡ç±»å‹é€‰æ‹©æ£€æµ‹æ–¹æ³•
    if is_android:
        print("\n=== ä½¿ç”¨OCRæ–¹æ³•æ£€æµ‹å¼€å§‹å¸§ ===")
        result = _llm_narrow_start_frame_ocr_ultra_fast_android_from_frames(
            frame_dir=test_frame_dir,
            do_evaluation=True,
            max_frames=300,
            skip_frames=1,
            scale_factor=1.0,
            early_stop=True,
            save_frame_texts=True,
            fps=30.0
        )
    else:
        print("\n=== ä½¿ç”¨UI Inferæ–¹æ³•æ£€æµ‹å¼€å§‹å¸§ ===")
        result = llm_narrow_start_frame_ocr_ultra_fast_from_frames(
            frame_dir=test_frame_dir,
            llm_client=llm_client,
            do_evaluation=True,
            max_frames=300,
            skip_frames=1,
            scale_factor=1.0,
            early_stop=True,
            save_frame_texts=True,
            ui_infer_config=ocr_config["ui_infer_config"],
            fps=30.0
        )
    
    # 3. è¾“å‡ºç»“æœ
    print("\n=== æµ‹è¯•ç»“æœ ===")
    if result["success"]:
        print(f"æ£€æµ‹æˆåŠŸ!")
        print(f"å¼€å§‹å¸§: {result['start_frame']}")
        if 'frame_texts' in result and result['frame_texts']:
            print(f"å¤„ç†å¸§æ•°: {len(result['frame_texts'])}")
    else:
        print(f"æ£€æµ‹å¤±è´¥: {result['error']}")
    
    print("\næµ‹è¯•å®Œæˆ!")

def test_ground_truth_calculation():
    """æµ‹è¯•ground truthè®¡ç®—åŠŸèƒ½"""
    print("=== æµ‹è¯•Ground Truthè®¡ç®—åŠŸèƒ½ ===")
    
    # æµ‹è¯•è§£æcase_key
    test_case_key = "2025-07-03_1751514078822,VID_20250703_113456/1_15s"
    video_name, action_time = parse_case_key_for_gt(test_case_key)
    print(f"è§£æcase_key: {test_case_key}")
    print(f"  è§†é¢‘å: {video_name}")
    print(f"  æ—¶é—´: {action_time}")
    
    # æµ‹è¯•æŸ¥æ‰¾action_infoæ–‡ä»¶
    if video_name and action_time is not None:
        action_info_path = find_action_info_file_for_gt(video_name, action_time, "downloaded_videos_processed")
        print(f"æ‰¾åˆ°action_infoæ–‡ä»¶: {action_info_path}")
        
        if action_info_path:
            # æµ‹è¯•è®¡ç®—ground truth
            gt_start_frame = calculate_ground_truth_start_frame_for_gt(action_info_path)
            print(f"è®¡ç®—å¾—åˆ°çš„ground truthå¼€å§‹å¸§: {gt_start_frame}")
        else:
            print("æœªæ‰¾åˆ°å¯¹åº”çš„action_infoæ–‡ä»¶")
    
    print("Ground Truthè®¡ç®—åŠŸèƒ½æµ‹è¯•å®Œæˆ")

# å¦‚æœéœ€è¦å•ç‹¬æµ‹è¯•ground truthè®¡ç®—åŠŸèƒ½ï¼Œå¯ä»¥å–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
# if __name__ == "__main__":
#     test_ground_truth_calculation() 

def llm_narrow_start_frame_ocr_ultra_fast_from_frames(
    frame_dir: str, 
    llm_client=None,
    do_evaluation: bool = False,
    max_frames: int = None,
    skip_frames: int = 1,
    scale_factor: float = 0.25,
    early_stop: bool = True,
    save_frame_texts: bool = True,
    ui_infer_config: dict = None,
    fps: float = 30.0  # é»˜è®¤å¸§ç‡
):
    """
    ä»å›¾ç‰‡æ–‡ä»¶å¤¹è¿›è¡Œè¶…é«˜æ•ˆçš„OCRå¼€å§‹å¸§æ£€æµ‹æ–¹æ³•
    æ”¯æŒè®¾å¤‡ç±»å‹æ£€æµ‹ï¼šå®‰å“è®¾å¤‡ä½¿ç”¨OCRæ–¹æ¡ˆï¼Œå…¶ä»–è®¾å¤‡ä½¿ç”¨UI inferæ–¹æ¡ˆ
    
    Args:
        frame_dir: å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹ï¼ˆç”¨äºUI inferæ–¹æ¡ˆï¼‰
        do_evaluation: æ˜¯å¦è¿›è¡Œè¯„ä¼°è¾“å‡º
        max_frames: æœ€å¤§å¤„ç†å¸§æ•°ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰å¸§
        skip_frames: è·³å¸§æ•°ï¼Œ1è¡¨ç¤ºæ¯å¸§éƒ½å¤„ç†
        scale_factor: å›¾ç‰‡ç¼©æ”¾å› å­ï¼Œ0.25è¡¨ç¤ºç¼©å°åˆ°åŸæ¥çš„1/4
        early_stop: æ˜¯å¦æ‰¾åˆ°ç¬¬ä¸€ä¸ªåæ ‡å°±åœæ­¢
        save_frame_texts: æ˜¯å¦ä¿å­˜æ¯å¸§æå–çš„æ–‡æœ¬
        ui_infer_config: UI inferé…ç½®å‚æ•°
        fps: å¸§ç‡ï¼Œç”¨äºè®¡ç®—æ—¶é—´æˆ³
    
    Returns:
        dict: åŒ…å«æ£€æµ‹ç»“æœçš„å­—å…¸
    """
    
    # å¯¼å…¥è®¾å¤‡ç±»å‹æ£€æµ‹å‡½æ•°
    from .llm_narrow_core import detect_android_by_first_frame, llm_narrow_start_frame_with_ui_infer_from_frames
    
    print(f"=== å¼€å§‹å¸§æ£€æµ‹ï¼ˆå›¾ç‰‡æ–‡ä»¶å¤¹ï¼‰: {os.path.basename(frame_dir)} ===")
    
    # 1. é¦–å…ˆæ£€æµ‹è®¾å¤‡ç±»å‹ï¼ˆä½¿ç”¨ç¬¬ä¸€å¸§ï¼‰
    print("æ­£åœ¨æ£€æµ‹è®¾å¤‡ç±»å‹...")
    is_android = detect_android_by_first_frame_from_frames(frame_dir)
    is_android = True
    if is_android:
        print("æ£€æµ‹åˆ°å®‰å“è®¾å¤‡ï¼Œä½¿ç”¨OCRæ–¹æ¡ˆ")
        return _llm_narrow_start_frame_ocr_ultra_fast_android_from_frames(
            frame_dir, do_evaluation, max_frames, skip_frames, 
            scale_factor, early_stop, save_frame_texts, fps
        )
    else:
        print("æ£€æµ‹åˆ°éå®‰å“è®¾å¤‡ï¼Œä½¿ç”¨UI Inferæ–¹æ¡ˆ")
        if llm_client is None:
            return {
                'success': False,
                'error': 'éå®‰å“è®¾å¤‡éœ€è¦LLMå®¢æˆ·ç«¯ï¼Œä½†æœªæä¾›',
                'start_frame': None,
                'candidates': []
            }
        
        # ä½¿ç”¨UI inferæ–¹æ¡ˆ
        ui_config = ui_infer_config or {}
        return llm_narrow_start_frame_with_ui_infer_from_frames(
            frame_dir=frame_dir,  # ä¼ é€’å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
            llm_client=llm_client,
            do_evaluation=do_evaluation,
            activity_threshold=ui_config.get('activity_threshold', -0.001),
            merge_window=ui_config.get('merge_window', 3),
            start_threshold=ui_config.get('start_threshold', -0.0001),
            end_threshold=ui_config.get('end_threshold', -0.00003),
            ssim_threshold=ui_config.get('ssim_threshold', 0.995),
            model=ui_config.get('model', "anthropic.claude-3.7-sonnet"),
            max_voting_rounds=ui_config.get('max_voting_rounds', 10),
            temperature=ui_config.get('temperature', 0.1),
            remove_background=ui_config.get('remove_background', False),
            enable_diff=ui_config.get('enable_diff', False),
            ui_infer_max_retries=ui_config.get('ui_infer_max_retries', 3),
            ui_infer_retry_delay=ui_config.get('ui_infer_retry_delay', 2),
            fps=fps
        )

def detect_android_by_first_frame_from_frames(frame_dir: str, ocr_url: str = "http://10.164.6.121:8417/paddelocr") -> bool:
    """
    é€šè¿‡ç¬¬ä¸€å¸§OCRæ£€æµ‹åˆ¤æ–­æ˜¯å¦ä¸ºå®‰å“è®¾å¤‡ï¼ˆä»å›¾ç‰‡æ–‡ä»¶å¤¹ï¼‰
    """
    import cv2
    import base64
    import requests
    
    def check_words(text):
        """æ£€æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«åæ ‡ä¿¡æ¯ï¼ˆç¬¬ä¸€å¸§æ£€æµ‹ï¼Œå®½æ¾ç‰ˆæœ¬ï¼‰"""
        import re
        # å®½æ¾ç‰ˆæœ¬ï¼šæœ‰å…³é”®å­—ä¸”æœ‰æ•°å­—ï¼Œä½†ä¸è¦æ±‚å†’å·
        pattern = re.compile(r'(?i)\b(Xv|Yv|dY|dX|X|Y)\b[^a-z]*\d+\.?\d*')
        match = pattern.search(text)
        if match:
            print(f"åŒ¹é…åˆ°åæ ‡ä¿¡æ¯: '{text}' -> {match.group()}")
        return match is not None
    
    def call_ocr_service(image, url):
        """è°ƒç”¨OCRæœåŠ¡"""
        try:
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            _, buffer = cv2.imencode('.jpg', image_rgb)
            img_base64 = base64.b64encode(buffer).decode('utf-8')
            data = {'image': img_base64}
            response = requests.post(url, data=data, timeout=10)
            
            if response.status_code == 200:
                result = response.json()
                if result.get('success'):
                    texts = []
                    for item in result.get('results', []):
                        text = item.get('text', '')
                        if text.strip():
                            texts.append(text)
                    return texts
                else:
                    print(f"OCRæœåŠ¡è¿”å›é”™è¯¯: {result.get('error')}")
                    return []
            else:
                print(f"HTTPè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return []
                
        except Exception as e:
            print(f"è°ƒç”¨OCRæœåŠ¡å‡ºé”™: {e}")
            return []
    
    try:
        # æŸ¥æ‰¾ç¬¬ä¸€å¸§å›¾ç‰‡æ–‡ä»¶
        first_frame_path = None
        possible_filenames = [
            "frame_0.jpeg", "frame_0.jpg", "frame_0.png",
            "0.jpeg", "0.jpg", "0.png",
            "0000.jpeg", "0000.jpg", "0000.png"
        ]
        
        for filename in possible_filenames:
            test_path = os.path.join(frame_dir, filename)
            if os.path.exists(test_path):
                first_frame_path = test_path
                break
        
        if first_frame_path is None:
            print(f"åœ¨ç›®å½• {frame_dir} ä¸­æœªæ‰¾åˆ°ç¬¬ä¸€å¸§å›¾ç‰‡æ–‡ä»¶")
            return False
        
        # è¯»å–ç¬¬ä¸€å¸§
        frame = cv2.imread(first_frame_path)
        if frame is None:
            print(f"æ— æ³•è¯»å–ç¬¬ä¸€å¸§å›¾ç‰‡: {first_frame_path}")
            return False
        
        # å…ˆè¿›è¡ŒèƒŒæ™¯æ‰£é™¤
        print(f"å¼€å§‹å¯¹ç¬¬ä¸€å¸§è¿›è¡ŒèƒŒæ™¯æ‰£é™¤: {first_frame_path}")
        try:
            # ä½¿ç”¨rembgè¿›è¡ŒèƒŒæ™¯æ‰£é™¤
            from src.utils.llm_narrow_core import remove_background_with_rembg
            from PIL import Image
            
            # ç›´æ¥åœ¨åŸæ–‡ä»¶å¤¹ä¸‹ä¿å­˜ä¸´æ—¶æ–‡ä»¶
            temp_frame_path = os.path.join(frame_dir, "temp_first_frame.png")
            
            # ä¿å­˜åŸå§‹å¸§ä¸ºPNGæ ¼å¼ï¼ˆrembgéœ€è¦ï¼‰
            cv2.imwrite(temp_frame_path, frame)
            
            # ä½¿ç”¨rembgè¿›è¡ŒèƒŒæ™¯æ‰£é™¤ï¼Œè¾“å‡ºæ–‡ä»¶ä¹Ÿä¼šåœ¨åŸæ–‡ä»¶å¤¹ä¸‹
            removed_bg_path = remove_background_with_rembg(temp_frame_path)
            
            if removed_bg_path and os.path.exists(removed_bg_path):
                # è¯»å–æ‰£é™¤èƒŒæ™¯åçš„å¸§
                frame = cv2.imread(removed_bg_path)
                if frame is not None:
                    print(f"rembgèƒŒæ™¯æ‰£é™¤å®Œæˆï¼Œæ–°å°ºå¯¸: {frame.shape}")
                else:
                    print(f"rembgèƒŒæ™¯æ‰£é™¤åæ— æ³•è¯»å–å›¾ç‰‡ï¼Œä½¿ç”¨åŸå§‹å›¾ç‰‡")
                    frame = cv2.imread(first_frame_path)
            else:
                print(f"rembgèƒŒæ™¯æ‰£é™¤å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å›¾ç‰‡")
                frame = cv2.imread(first_frame_path)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                if os.path.exists(temp_frame_path):
                    os.remove(temp_frame_path)
            except:
                pass
                
        except ImportError as e:
            print(f"æ— æ³•å¯¼å…¥rembgæ¨¡å—: {e}ï¼Œè·³è¿‡èƒŒæ™¯æ‰£é™¤")
        except Exception as e:
            print(f"èƒŒæ™¯æ‰£é™¤å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å›¾ç‰‡")
            frame = cv2.imread(first_frame_path)
        
        # è°ƒç”¨OCRæœåŠ¡
        texts = call_ocr_service(frame, ocr_url)
        
        # æ‰“å°æ‰€æœ‰è¯†åˆ«çš„æ–‡æœ¬
        print(f"ç¬¬ä¸€å¸§OCRè¯†åˆ«ç»“æœ:")
        print(f"è¯†åˆ«åˆ°çš„æ‰€æœ‰æ–‡æœ¬: {texts}")
        
        # ä¿å­˜æŠ å‡ºæ¥çš„å›¾ç‰‡ç”¨äºæŸ¥çœ‹
        debug_image_path = os.path.join(frame_dir, "debug_first_frame_cropped.jpeg")
        cv2.imwrite(debug_image_path, frame)
        print(f"æŠ å‡ºæ¥çš„å›¾ç‰‡å·²ä¿å­˜åˆ°: {debug_image_path}")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«åæ ‡ä¿¡æ¯
        for text in texts:
            if check_words(text):
                print(f"æ£€æµ‹åˆ°åæ ‡ä¿¡æ¯: {text}ï¼Œåˆ¤å®šä¸ºå®‰å“è®¾å¤‡")
                return True
        
        print("æœªæ£€æµ‹åˆ°åæ ‡ä¿¡æ¯ï¼Œåˆ¤å®šä¸ºéå®‰å“è®¾å¤‡")
        return False
        
    except Exception as e:
        print(f"æ£€æµ‹è®¾å¤‡ç±»å‹æ—¶å‡ºé”™: {e}")
        return False

def _llm_narrow_start_frame_ocr_ultra_fast_android_from_frames(
    frame_dir: str, 
    do_evaluation: bool = False,
    max_frames: int = None,
    skip_frames: int = 1,
    scale_factor: float = 1.0,
    early_stop: bool = True,
    save_frame_texts: bool = True,
    fps: float = 30.0
):
    """
    å®‰å“è®¾å¤‡çš„OCRå¼€å§‹å¸§æ£€æµ‹æ–¹æ³•ï¼ˆä»å›¾ç‰‡æ–‡ä»¶å¤¹ï¼‰
    """
    
    # è·å–æ‰€æœ‰å¸§æ–‡ä»¶
    frame_files = []
    import re
    def natural_key(s):
        return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]
    frame_files = sorted([f for f in os.listdir(frame_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))], key=natural_key)
    frame_files = [os.path.join(frame_dir, f) for f in frame_files]
    
    if not frame_files:
        return {
            'success': False,
            'error': f'åœ¨ç›®å½•ä¸­æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶: {frame_dir}',
            'start_frame': None,
            'candidates': [],
            'frame_texts': []
        }
    
    # è·å–è§†é¢‘ä¿¡æ¯ï¼ˆä»ç¬¬ä¸€å¸§æ¨æ–­ï¼‰
    first_frame = cv2.imread(frame_files[0])
    if first_frame is None:
        return {
            'success': False,
            'error': f'æ— æ³•è¯»å–ç¬¬ä¸€å¸§å›¾ç‰‡: {frame_files[0]}',
            'start_frame': None,
            'candidates': [],
            'frame_texts': []
        }
    
    total_frames = len(frame_files)
    height, width = first_frame.shape[:2]
    
    if do_evaluation:
        print(f"å›¾ç‰‡æ–‡ä»¶å¤¹ä¿¡æ¯:")
        print(f"  æ€»å¸§æ•°: {total_frames}")
        print(f"  FPS: {fps:.2f}")
        print(f"  åˆ†è¾¨ç‡: {width}x{height}")
        print(f"  è·³å¸§æ•°: {skip_frames}")
        print(f"  ç¼©æ”¾å› å­: {scale_factor}")
        print(f"  æ—©æœŸåœæ­¢: {early_stop}")
        if max_frames:
            print(f"  æœ€å¤§å¤„ç†å¸§æ•°: {max_frames}")
    
    candidates = []
    processed_frames = 0
    start_time = time.time()
    
    # è®°å½•æ¯å¸§æå–çš„æ–‡æœ¬
    frame_texts = []
    
    # å¤„ç†æ¯ä¸€å¸§
    for frame_idx, frame_path in enumerate(frame_files):
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§å¸§æ•°é™åˆ¶
        if max_frames and processed_frames >= max_frames:
            break
        
        processed_frames += 1
        frame_index = processed_frames - 1
        basename = os.path.splitext(os.path.basename(frame_path))[0]
        
        if do_evaluation and processed_frames % 5 == 0:
            elapsed_time = time.time() - start_time
            fps_processed = processed_frames / elapsed_time if elapsed_time > 0 else 0
            print(f"å¤„ç†å¸§ {processed_frames}: å¸§å· {frame_index}, å¤„ç†é€Ÿåº¦: {fps_processed:.1f} fps")
        
        try:
            # è¯»å–å›¾ç‰‡
            frame = cv2.imread(frame_path)
            if frame is None:
                if do_evaluation:
                    print(f"  å¸§ {frame_index} - æ— æ³•è¯»å–å›¾ç‰‡: {frame_path}")
                continue
            
            # å…ˆè¿›è¡ŒèƒŒæ™¯æ‰£é™¤
            if do_evaluation:
                print(f"  å¸§ {frame_index} - å¼€å§‹èƒŒæ™¯æ‰£é™¤...")
            
            try:
                # ä½¿ç”¨rembgè¿›è¡ŒèƒŒæ™¯æ‰£é™¤
                from src.utils.llm_narrow_core import remove_background_with_rembg
                from PIL import Image
                
                # ç›´æ¥åœ¨åŸæ–‡ä»¶å¤¹ä¸‹ä¿å­˜ä¸´æ—¶æ–‡ä»¶ï¼Œå‘½åç”¨åŸå§‹æ–‡ä»¶å
                temp_frame_path = os.path.join(frame_dir, f"temp_{basename}.png")
                
                # ä¿å­˜åŸå§‹å¸§ä¸ºPNGæ ¼å¼ï¼ˆrembgéœ€è¦ï¼‰
                cv2.imwrite(temp_frame_path, frame)
                
                # ä½¿ç”¨rembgè¿›è¡ŒèƒŒæ™¯æ‰£é™¤ï¼Œè¾“å‡ºæ–‡ä»¶ä¹Ÿä¼šåœ¨åŸæ–‡ä»¶å¤¹ä¸‹
                removed_bg_path = remove_background_with_rembg(temp_frame_path)
                
                if removed_bg_path and os.path.exists(removed_bg_path):
                    # è¯»å–æ‰£é™¤èƒŒæ™¯åçš„å¸§
                    frame = cv2.imread(removed_bg_path)
                    if frame is not None:
                        if do_evaluation:
                            print(f"  å¸§ {frame_index} - rembgèƒŒæ™¯æ‰£é™¤å®Œæˆï¼Œæ–°å°ºå¯¸: {frame.shape}")
                        # å¯¹ç§»é™¤èƒŒæ™¯åçš„å›¾ç‰‡åšå¤§å°è°ƒæ•´
                        from src.utils.llm_narrow_core import check_and_resize_saved_image
                        enhanced_path = check_and_resize_saved_image(removed_bg_path, max_file_size_mb=3.5, min_file_size_mb=2.5)
                        if enhanced_path and os.path.exists(enhanced_path):
                            from PIL import Image
                            enhanced_pil = Image.open(enhanced_path)
                            print(enhanced_path)
                            enhanced_frame = cv2.cvtColor(np.array(enhanced_pil), cv2.COLOR_RGB2BGR)
                            if enhanced_frame is not None:
                                frame = enhanced_frame
                                if do_evaluation:
                                    print(f"  å¸§ {frame_index} - ç§»é™¤èƒŒæ™¯åå›¾ç‰‡å¤§å°è°ƒæ•´å®Œæˆ: {enhanced_pil.size}")
                            else:
                                if do_evaluation:
                                    print(f"  å¸§ {frame_index} - ç§»é™¤èƒŒæ™¯åå›¾ç‰‡å¤§å°è°ƒæ•´åæ— æ³•è¯»å–ï¼Œä½¿ç”¨åŸå§‹å°ºå¯¸")
                        else:
                            if do_evaluation:
                                print(f"  å¸§ {frame_index} - ç§»é™¤èƒŒæ™¯åå›¾ç‰‡å¤§å°è°ƒæ•´å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å°ºå¯¸")
                    else:
                        if do_evaluation:
                            print(f"  å¸§ {frame_index} - rembgèƒŒæ™¯æ‰£é™¤åæ— æ³•è¯»å–å›¾ç‰‡ï¼Œä½¿ç”¨åŸå§‹å›¾ç‰‡")
                        frame = cv2.imread(frame_path)
                else:
                    if do_evaluation:
                        print(f"  å¸§ {frame_index} - rembgèƒŒæ™¯æ‰£é™¤å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å›¾ç‰‡")
                    frame = cv2.imread(frame_path)
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    if os.path.exists(temp_frame_path):
                        os.remove(temp_frame_path)
                except:
                    pass
                    
            except ImportError as e:
                if do_evaluation:
                    print(f"  å¸§ {frame_index} - æ— æ³•å¯¼å…¥rembgæ¨¡å—: {e}ï¼Œè·³è¿‡èƒŒæ™¯æ‰£é™¤")
            except Exception as e:
                if do_evaluation:
                    print(f"  å¸§ {frame_index} - èƒŒæ™¯æ‰£é™¤å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å›¾ç‰‡")
                frame = cv2.imread(frame_path)
            
            # å›¾ç‰‡ç¼©æ”¾ä»¥å‡å°‘å†…å­˜ä½¿ç”¨å’Œæé«˜å¤„ç†é€Ÿåº¦ï¼ˆåœ¨æ”¾å¤§ä¹‹åè¿›è¡Œï¼‰
            # å·²å»é™¤scale_factorç¼©å°å›¾ç‰‡çš„ä»£ç ï¼Œä¿è¯ç”¨çš„å°±æ˜¯check_and_resizeåçš„å›¾ç‰‡
            
            if do_evaluation:
                print(f"  å¸§ {frame_index} - å¼€å§‹OCRè¯†åˆ«...")
                print(f"  å›¾ç‰‡å°ºå¯¸: {frame.shape}")
                print(f"  å›¾ç‰‡æ•°æ®ç±»å‹: {frame.dtype}")
            
            # è°ƒç”¨HTTP OCRæœåŠ¡
            try:
                rec_texts = call_ocr_service(frame, url="http://10.164.6.121:8417/paddelocr")
                if do_evaluation:
                    print(f"  å¸§ {frame_index} - OCRè¯†åˆ«å®Œæˆ")
                    print(f"  æå–çš„æ–‡æœ¬: {rec_texts}")
            except Exception as ocr_error:
                if do_evaluation:
                    print(f"  å¸§ {frame_index} - OCRè¯†åˆ«å¤±è´¥: {ocr_error}")
                    print(f"  OCRé”™è¯¯ç±»å‹: {type(ocr_error).__name__}")
                    import traceback
                    traceback.print_exc()
                raise ocr_error
            
            # è®°å½•æ¯å¸§çš„æ–‡æœ¬ä¿¡æ¯
            frame_text_info = {
                'frame_number': frame_index,
                'frame_index': frame_index,
                'frame_file': os.path.basename(frame_path),
                'timestamp': frame_index / fps if fps > 0 else 0,
                'ocr_texts': rec_texts,
                'has_coordinates': False,
                'coordinate_texts': []
            }
            
            # åˆ¤æ–­æ˜¯å¦å­˜åœ¨ç‚¹å‡»åæ ‡
            has_coordinates = False
            coordinate_texts = []
            for text in rec_texts:
                if check_words(text):
                    has_coordinates = True
                    coordinate_texts.append(text)
                    break
            
            if has_coordinates:
                candidates.append({
                    'frame_path': frame_path,
                    'frame_index': frame_index,
                    'frame_file': os.path.basename(frame_path),
                    'ocr_texts': rec_texts,
                    'coordinate_texts': coordinate_texts
                })
                
                frame_text_info['has_coordinates'] = True
                frame_text_info['coordinate_texts'] = coordinate_texts
                
                if do_evaluation:
                    print(f"  æ£€æµ‹åˆ°åæ ‡ä¿¡æ¯: {coordinate_texts}")
                    print(f"  æ‰€æœ‰OCRæ–‡æœ¬: {rec_texts}")
                
                # å¦‚æœå¯ç”¨æ—©æœŸåœæ­¢ï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªåæ ‡å°±åœæ­¢
                if early_stop:
                    if do_evaluation:
                        print(f"  å¯ç”¨æ—©æœŸåœæ­¢ï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªåæ ‡å¸§ï¼Œåœæ­¢å¤„ç†")
                    break
            
            frame_texts.append(frame_text_info)
            
        except Exception as e:
            if do_evaluation:
                print(f"  å¤„ç†å¸§ {frame_path} æ—¶å‡ºé”™: {e}")
    
    # é€‰æ‹©ç¬¬ä¸€ä¸ªæ£€æµ‹åˆ°åæ ‡çš„å¸§ä½œä¸ºå¼€å§‹å¸§
    start_frame = candidates[0] if candidates else None
    
    result = {
        'success': True,
        'start_frame': start_frame,
        'candidates': candidates,
        'total_frames': total_frames,
        'processed_frames': processed_frames,
        'frame_texts': frame_texts if save_frame_texts else [],
        'method_used': 'ocr_ultra_fast_from_frames'
    }
    
    # è¯„ä¼°æ¨¡å¼è¾“å‡º
    if do_evaluation:
        print(f"\n=== æ£€æµ‹ç»“æœ ===")
        print(f"æ€»å¸§æ•°: {total_frames}")
        print(f"å¤„ç†å¸§æ•°: {processed_frames}")
        print(f"æ£€æµ‹åˆ°åæ ‡çš„å¸§æ•°: {len(candidates)}")
        
        if start_frame:
            print(f"\nå¼€å§‹å¸§: {os.path.basename(start_frame['frame_path'])}")
            print(f"å¸§ç´¢å¼•: {start_frame['frame_index']}")
            print(f"åæ ‡æ–‡æœ¬: {start_frame['coordinate_texts']}")
            print(f"æ‰€æœ‰OCRæ–‡æœ¬: {start_frame['ocr_texts']}")
        else:
            print("æœªæ£€æµ‹åˆ°åŒ…å«åæ ‡ä¿¡æ¯çš„å¸§")
    
    return result

def llm_narrow_start_frame_with_ui_infer(
    video_path: str,
    llm_client,
    do_evaluation=False,
    activity_threshold=-0.001,
    merge_window=3,
    start_threshold=-0.0001,
    end_threshold=-0.00003,
    ssim_threshold=0.995,
    model="anthropic.claude-3.7-sonnet",
    max_voting_rounds=10,
    temperature=0.1,
    remove_background=False,
    enable_diff=False,
    ui_infer_max_retries=3,
    ui_infer_retry_delay=2
):
    """
    åŸºäºUI inferçš„å¼€å§‹å¸§æ£€æµ‹æ–¹æ³•
    ä½¿ç”¨LLM+æ‹¼å›¾+æŠ•ç¥¨çš„æ–¹å¼æ‰¾åˆ°æ“ä½œå¼€å§‹çš„ç¬¬ä¸€å¸§
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹
        do_evaluation: æ˜¯å¦è¿›è¡Œè¯¦ç»†è¯„ä¼°
        activity_threshold: æ´»åŠ¨é˜ˆå€¼ï¼Œé»˜è®¤-0.001
        merge_window: åˆå¹¶çª—å£å¤§å°ï¼Œé»˜è®¤3
        start_threshold: å¼€å§‹ç‚¹é˜ˆå€¼ï¼Œé»˜è®¤-0.0001
        end_threshold: ç»“æŸç‚¹é˜ˆå€¼ï¼Œé»˜è®¤-0.00003
        ssim_threshold: SSIMé˜ˆå€¼ï¼Œé»˜è®¤0.995
        model: ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œé»˜è®¤"anthropic.claude-3.7-sonnet"
        max_voting_rounds: æœ€å¤§æŠ•ç¥¨è½®æ•°ï¼Œé»˜è®¤10
        temperature: æ¸©åº¦å‚æ•°ï¼Œé»˜è®¤0.1
        remove_background: æ˜¯å¦å»é™¤èƒŒæ™¯ï¼Œé»˜è®¤False
        enable_diff: æ˜¯å¦å¯ç”¨å·®å¼‚æ£€æµ‹ï¼Œé»˜è®¤False
        ui_infer_max_retries: UIæ¨æ–­æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤3æ¬¡
        ui_infer_retry_delay: UIæ¨æ–­é‡è¯•é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤2ç§’
    
    Returns:
        dict: åŒ…å«æ£€æµ‹ç»“æœçš„å­—å…¸
    """
    import math
    from src.toolchain_llm.service.ui_detection.horus_ocr import ClientHorus
    
    print(f"å¼€å§‹UI inferå¼€å§‹å¸§æ£€æµ‹: {video_path}")
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºå¤„ç†è§†é¢‘å¸§
    import tempfile
    import cv2
    import os
    
    temp_dir = tempfile.mkdtemp(prefix="start_frame_ui_infer_")
    print(f"ä¸´æ—¶ç›®å½•: {temp_dir}")
    
    try:
        # 1. æå–è§†é¢‘å¸§åˆ°ä¸´æ—¶ç›®å½•
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            return {
                'success': False,
                'error': f'æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}',
                'start_frame': None,
                'candidates': []
            }
        
        # è·å–è§†é¢‘ä¿¡æ¯
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        if do_evaluation:
            print(f"è§†é¢‘ä¿¡æ¯:")
            print(f"  æ€»å¸§æ•°: {total_frames}")
            print(f"  FPS: {fps:.2f}")
            print(f"  åˆ†è¾¨ç‡: {width}x{height}")
        
        # æå–å¸§åˆ°ä¸´æ—¶ç›®å½•
        frame_count = 0
        extracted_frames = []
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # ä¿å­˜å¸§ä¸ºPNGæ–‡ä»¶
            frame_path = os.path.join(temp_dir, f"frame_{frame_count:04d}.png")
            cv2.imwrite(frame_path, frame)
            extracted_frames.append(frame_path)
            frame_count += 1
            
            # é™åˆ¶å¤„ç†çš„å¸§æ•°ï¼Œé¿å…å¤„ç†è¿‡å¤šå¸§
            if frame_count >= 100:  # æœ€å¤šå¤„ç†100å¸§
                break
        
        cap.release()
        
        if not extracted_frames:
            return {
                'success': False,
                'error': 'æ— æ³•æå–è§†é¢‘å¸§',
                'start_frame': None,
                'candidates': []
            }
        
        print(f"æˆåŠŸæå– {len(extracted_frames)} å¸§")
        
        # 2. åˆ›å»ºaction_info.jsonæ–‡ä»¶ï¼ˆæ¨¡æ‹Ÿï¼‰
        action_info = {
            "original_action_item": {
                "action_desc": "å¼€å§‹å¸§æ£€æµ‹",
                "action_time": 0.0,
                "marked_start_time": 0.0,
                "marked_end_time": len(extracted_frames) / fps if fps > 0 else 0.0
            },
            "extraction_parameters": {
                "num_extracted_frames": len(extracted_frames),
                "fps": fps,
                "extract_start_time_sec": 0.0,
                "width": width,
                "height": height
            }
        }
        
        action_info_path = os.path.join(temp_dir, "action_info.json")
        with open(action_info_path, "w", encoding="utf-8") as f:
            json.dump(action_info, f, indent=2, ensure_ascii=False)
        
        # 3. åˆå§‹åŒ–Horuså®¢æˆ·ç«¯
        horus_client = ClientHorus()
        
        # 4. ç”Ÿæˆå€™é€‰å¸§ï¼ˆä»0å¼€å§‹ï¼Œä¸ç­›é™¤å¼€å§‹å¸§ä¹‹å‰çš„å¸§ï¼‰
        # ä½¿ç”¨æ´»åŠ¨æ£€æµ‹æ¥æ‰¾åˆ°å¯èƒ½çš„å¼€å§‹å¸§
        from src.algorithms.find_page_load_intelligent import preprocess_ssim_data, find_page_load_intelligent
        
        # è®¡ç®—å¸§é—´å·®å¼‚
        ssim_data = []
        for i in range(len(extracted_frames) - 1):
            try:
                img1 = cv2.imread(extracted_frames[i])
                img2 = cv2.imread(extracted_frames[i + 1])
                
                if img1 is not None and img2 is not None:
                    # è®¡ç®—SSIM
                    from skimage.metrics import structural_similarity as ssim
                    gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
                    gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
                    
                    # ç¡®ä¿ä¸¤ä¸ªå›¾åƒå°ºå¯¸ç›¸åŒ
                    if gray1.shape != gray2.shape:
                        gray2 = cv2.resize(gray2, (gray1.shape[1], gray1.shape[0]))
                    
                    ssim_score = ssim(gray1, gray2)
                    ssim_data.append(ssim_score)
                else:
                    ssim_data.append(1.0)  # å¦‚æœæ— æ³•è¯»å–å›¾åƒï¼Œå‡è®¾ç›¸ä¼¼
            except Exception as e:
                print(f"è®¡ç®—å¸§ {i} å’Œ {i+1} çš„SSIMæ—¶å‡ºé”™: {e}")
                ssim_data.append(1.0)
        
        # æ‰¾åˆ°æ´»åŠ¨ç‚¹ï¼ˆSSIMå˜åŒ–è¾ƒå¤§çš„åœ°æ–¹ï¼‰
        activity_points = []
        for i in range(len(ssim_data)):
            if i > 0:
                # è®¡ç®—SSIMå˜åŒ–
                ssim_change = abs(ssim_data[i] - ssim_data[i-1])
                if ssim_change > activity_threshold:
                    activity_points.append(i)
        
        # ç”Ÿæˆå€™é€‰å¸§ï¼ˆåŒ…æ‹¬æ´»åŠ¨ç‚¹å’Œä¸€äº›å‡åŒ€åˆ†å¸ƒçš„å¸§ï¼‰
        candidate_frames = set()
        
        # æ·»åŠ æ´»åŠ¨ç‚¹
        for point in activity_points:
            candidate_frames.add(point)
        
        # æ·»åŠ å‡åŒ€åˆ†å¸ƒçš„å¸§ï¼ˆç¡®ä¿è¦†ç›–æ•´ä¸ªè§†é¢‘ï¼‰
        step = max(1, len(extracted_frames) // 10)  # æœ€å¤š10ä¸ªå‡åŒ€åˆ†å¸ƒçš„å¸§
        for i in range(0, len(extracted_frames), step):
            candidate_frames.add(i)
        
        # ç¡®ä¿è‡³å°‘æœ‰5ä¸ªå€™é€‰å¸§
        if len(candidate_frames) < 5:
            for i in range(min(5, len(extracted_frames))):
                candidate_frames.add(i)
        
        candidate_frames = sorted(list(candidate_frames))
        
        if do_evaluation:
            print(f"å€™é€‰å¸§: {candidate_frames}")
            print(f"æ´»åŠ¨ç‚¹: {activity_points}")
        
        # 5. åˆ›å»ºæ‹¼å›¾å¹¶è¿›è¡ŒUI infer
        grid_image_path = create_voting_image_grid_with_ui_infer(
            temp_dir, 
            candidate_frames, 
            horus_client,
            max_single_size=1200,  # ä½¿ç”¨è¾ƒå°çš„å°ºå¯¸
            label_height=40,
            ui_infer_max_retries=ui_infer_max_retries,
            ui_infer_retry_delay=ui_infer_retry_delay,
            remove_background=remove_background,
            enable_diff=enable_diff
        )
        
        if grid_image_path is None:
            return {
                'success': False,
                'error': 'æ— æ³•åˆ›å»ºæ‹¼å›¾',
                'start_frame': None,
                'candidates': []
            }
        
        # 6. æ„å»ºå¼€å§‹å¸§æ£€æµ‹çš„prompt
        start_frame_prompt = (
            "You are a helpful assistant and an expert in start frame analysis. Your task is to identify the FIRST frame in the grid that shows the initial visible change after a user operation (e.g., click, tap, scroll) on a web or app page.\n\n"
            "Below is a grid of frames from a video showing the page loading or transition process. The frames are arranged from left to right, top to bottom.\n\n"
            "## Context:\n"
            "- The user has just performed an operation (e.g., click, tap, scroll).\n"
            "- Each frame is a snapshot of the page after the operation.\n"
            "- Your goal is to find the first frame where the page starts to change in response to the operation.\n\n"
            "## Definition of start frame:\n"
            "1. The start frame is the FIRST frame that shows any visible change or reaction after the user operation.\n"
            "2. Changes may include: button color change, loading indicator appearing, page content starting to update, new UI elements appearing, animations starting, etc.\n"
            "3. Ignore minor flickers, background animation, or non-essential UI changes that are not related to the main operation.\n"
            "4. The change should be a direct result of the user action, not random or unrelated animation.\n\n"
            "## Task:\n"
            "- Carefully analyze each frame in the grid (from left to right, top to bottom) and describe the subtle changes you observe between consecutive frames.\n"
            "- Pay special attention to:\n"
            "  - Button or UI element color/state changes\n"
            "  - Loading indicators or spinners appearing\n"
            "  - New content or images starting to load\n"
            "  - Any visible feedback that the operation has triggered a response\n"
            "- Ignore:\n"
            "  - Minor background animation\n"
            "  - Floating system buttons or overlays not related to the operation\n"
            "  - Small text scrolling or ticker effects\n\n"
            "## IMPORTANT:\n"
            "- You must choose ONLY from the candidate frames (gray borders). Do NOT select any reference or non-candidate frames.\n"
            "- Be precise: select the earliest frame that shows a clear, operation-related change.\n\n"
            "## Please answer in JSON format:\n"
            "{\n"
            "    \"frame_analysis\": [\n"
            "        {\"frame_number\": <frame_number>, \"description\": \"Detailed description of subtle changes in this frame\"},\n"
            "        {\"frame_number\": <frame_number>, \"description\": \"...\"},\n"
            "        // ... continue for all frames\n"
            "    ],\n"
            "    \"target_frame\": <frame_number>,\n"
            "    \"reason\": \"Detailed explanation of why this frame is the start frame, and why the others are not.\"\n"
            "}\n"
        )

        # 7. è°ƒç”¨LLMè¿›è¡Œåˆ¤æ–­
        try:
            messages = [{"role": "user", "content": start_frame_prompt, "image_url": [grid_image_path]}]
            llm_response = llm_vision_request_with_retry(
                llm_client, 
                messages, 
                max_tokens=1024, 
                model=model,
                temperature=temperature
            )
            
            if do_evaluation:
                print(f"LLMå“åº”: {llm_response}")
            
            # 8. è§£æLLMå“åº”
            try:
                # å°è¯•ç›´æ¥è§£æJSON
                response_json = json.loads(llm_response)
            except json.JSONDecodeError:
                # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•ä»markdownä»£ç å—ä¸­æå–
                pattern = r"```(?:json\s*)?\n([\s\S]+?)\n```"
                match = re.search(pattern, llm_response)
                if match:
                    response_json = json.loads(match.group(1))
                else:
                    # å¦‚æœè¿˜æ˜¯æ— æ³•è§£æï¼Œä½¿ç”¨é»˜è®¤å€¼
                    print("æ— æ³•è§£æLLMå“åº”ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                    response_json = {"target_frame": candidate_frames[0] if candidate_frames else 0}
            
            target_frame = response_json.get("target_frame", 0)
            reason = response_json.get("reason", "æ— ç†ç”±")
            confidence = response_json.get("confidence", 0.5)
            
            # ç¡®ä¿target_frameåœ¨å€™é€‰å¸§èŒƒå›´å†…
            if target_frame not in candidate_frames:
                print(f"è­¦å‘Š: LLMè¿”å›çš„å¸§å· {target_frame} ä¸åœ¨å€™é€‰å¸§ä¸­ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå€™é€‰å¸§")
                target_frame = candidate_frames[0] if candidate_frames else 0
            
            # 9. æ„å»ºç»“æœ
            result = {
                'success': True,
                'start_frame': {
                    'frame_number': target_frame,
                    'timestamp': target_frame / fps if fps > 0 else 0,
                    'reason': reason,
                    'confidence': confidence
                },
                'candidates': [
                    {
                        'frame_number': frame_idx,
                        'timestamp': frame_idx / fps if fps > 0 else 0,
                        'is_activity_point': frame_idx in activity_points
                    }
                    for frame_idx in candidate_frames
                ],
                'total_frames': len(extracted_frames),
                'activity_points': activity_points,
                'llm_response': llm_response,
                'method_used': 'ui_infer_start_frame'
            }
            
            if do_evaluation:
                print(f"\n=== UI Inferå¼€å§‹å¸§æ£€æµ‹ç»“æœ ===")
                print(f"æ€»å¸§æ•°: {len(extracted_frames)}")
                print(f"å€™é€‰å¸§æ•°: {len(candidate_frames)}")
                print(f"æ´»åŠ¨ç‚¹: {activity_points}")
                print(f"é¢„æµ‹å¼€å§‹å¸§: {target_frame}")
                print(f"æ—¶é—´æˆ³: {target_frame / fps:.2f}s")
                print(f"ç†ç”±: {reason}")
                print(f"ç½®ä¿¡åº¦: {confidence}")
            
            return result
            
        except Exception as e:
            print(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            return {
                'success': False,
                'error': f'LLMè°ƒç”¨å¤±è´¥: {e}',
                'start_frame': None,
                'candidates': []
            }
    
    finally:
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        try:
            import shutil
            shutil.rmtree(temp_dir)
            print(f"å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")
        except Exception as e:
            print(f"æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")


    


def llm_narrow_start_frame_with_ui_infer_from_frames(
    frame_dir: str,
    llm_client,
    do_evaluation=False,
    activity_threshold=-0.001,
    merge_window=3,
    start_threshold=-0.0001,
    end_threshold=-0.00003,
    ssim_threshold=0.995,
    model="anthropic.claude-3.7-sonnet",
    max_voting_rounds=10,
    temperature=0.1,
    remove_background=False,
    enable_diff=False,
    ui_infer_max_retries=3,
    ui_infer_retry_delay=2,
    fps=30.0
):
    """
    åŸºäºUI inferçš„å¼€å§‹å¸§æ£€æµ‹æ–¹æ³•ï¼ˆä»å›¾ç‰‡æ–‡ä»¶å¤¹ï¼‰
    è¿™æ˜¯llm_narrow_start_frame_with_ui_inferçš„é€‚é…å™¨ç‰ˆæœ¬ï¼Œç”¨äºå¤„ç†å›¾ç‰‡æ–‡ä»¶å¤¹
    """
    import cv2
    import tempfile
    import os
    
    print(f"å¼€å§‹UI inferå¼€å§‹å¸§æ£€æµ‹ï¼ˆå›¾ç‰‡æ–‡ä»¶å¤¹ï¼‰: {frame_dir}")
    
    # åˆ›å»ºä¸´æ—¶è§†é¢‘æ–‡ä»¶
    temp_video_path = os.path.join(tempfile.gettempdir(), f"temp_video_{os.getpid()}.mp4")
    
    try:
        # ä»å›¾ç‰‡æ–‡ä»¶å¤¹åˆ›å»ºè§†é¢‘æ–‡ä»¶
        frame_files = []
        for filename in sorted(os.listdir(frame_dir)):
            if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
                frame_files.append(os.path.join(frame_dir, filename))
        
        if not frame_files:
            return {
                'success': False,
                'error': f'åœ¨ç›®å½•ä¸­æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶: {frame_dir}',
                'start_frame': None,
                'candidates': []
            }
        
        # è¯»å–ç¬¬ä¸€å¸§è·å–å°ºå¯¸ä¿¡æ¯
        first_frame = cv2.imread(frame_files[0])
        if first_frame is None:
            return {
                'success': False,
                'error': f'æ— æ³•è¯»å–ç¬¬ä¸€å¸§å›¾ç‰‡: {frame_files[0]}',
                'start_frame': None,
                'candidates': []
            }
        
        height, width = first_frame.shape[:2]
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(temp_video_path, fourcc, fps, (width, height))
        
        # å†™å…¥æ‰€æœ‰å¸§
        for frame_path in frame_files:
            frame = cv2.imread(frame_path)
            if frame is not None:
                out.write(frame)
        
        out.release()
        
        # è°ƒç”¨åŸæœ‰çš„è§†é¢‘ç‰ˆæœ¬å‡½æ•°
        result = llm_narrow_start_frame_with_ui_infer(
            video_path=temp_video_path,
            llm_client=llm_client,
            do_evaluation=do_evaluation,
            activity_threshold=activity_threshold,
            merge_window=merge_window,
            start_threshold=start_threshold,
            end_threshold=end_threshold,
            ssim_threshold=ssim_threshold,
            model=model,
            max_voting_rounds=max_voting_rounds,
            temperature=temperature,
            remove_background=remove_background,
            enable_diff=enable_diff,
            ui_infer_max_retries=ui_infer_max_retries,
            ui_infer_retry_delay=ui_infer_retry_delay
        )
        
        return result
        
    except Exception as e:
        print(f"UI inferå¼€å§‹å¸§æ£€æµ‹ï¼ˆå›¾ç‰‡æ–‡ä»¶å¤¹ï¼‰å‡ºé”™: {e}")
        return {
            'success': False,
            'error': f'UI inferå¼€å§‹å¸§æ£€æµ‹å‡ºé”™: {str(e)}',
            'start_frame': None,
            'candidates': []
        }
    finally:
        # æ¸…ç†ä¸´æ—¶è§†é¢‘æ–‡ä»¶
        if os.path.exists(temp_video_path):
            try:
                os.remove(temp_video_path)
            except:
                pass