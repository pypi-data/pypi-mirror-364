{
  "hash": "687868d3fdd0b9d5518f45c3fa0009c0",
  "result": {
    "markdown": "---\nengine: knitr\n---\n\n\n\n\n# Location-scale regression\n\nThis tutorial implements a Bayesian location-scale regression model within the Liesel framework.\nIn contrast to the standard linear model with constant variance, the location-scale model allows for heteroscedasticity such that both the mean of the response variable as well as its variance depend on (possibly) different covariates.\n\nThis tutorial assumes a linear relationship between the expected value of the response and the regressors, whereas a logarithmic link is chosen for the standard deviation.\nMore specifically, we choose the model\n\n$$\n\\begin{aligned}\ny_i \\sim \\mathcal{N}_{} \\left( \\mathbf{x}_i^T \\boldsymbol{\\beta}, \\exp \\left( \\mathbf{ z}_i^T \\boldsymbol{\\gamma} \\right)^2 \\right)\n\\end{aligned}\n$$\nin which the single observation are conditionally independent.\n\nFrom the equation we see that *location* covariates are collected in the design matrix $\\mathbf{X}$ and *scale* covariates are contained in the design matrix $\\mathbf{Z}$. Both matrices can, but generally do not have to, share common regressors.\nWe refer to $\\boldsymbol{\\beta}$ as location parameter and to $\\boldsymbol{\\gamma}$ as scale parameter.\n\nIn this notebook, both design matrices only contain one intercept and one regressor column.\nHowever, the model design naturally generalizes to any (reasonable) number of covariates.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport jax\nimport jax.numpy as jnp\nimport tensorflow_probability.substrates.jax.distributions as tfd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport liesel.goose as gs\nimport liesel.model as lsl\n\nsns.set_theme(style=\"whitegrid\")\n```\n:::\n\n\nFirst lets generate the data according to the model\n\n\n::: {.cell}\n\n```{.python .cell-code}\nkey = jax.random.PRNGKey(13)\nn = 500\n\nkey, key_X, key_Z, key_y = jax.random.split(key, 4)\n\ntrue_beta = jnp.array([1.0, 3.0])\ntrue_gamma = jnp.array([0.0, 0.5])\n\nX_mat = jnp.column_stack([jnp.ones(n), tfd.Uniform(low=0., high=5.).sample(n, seed=key_X)])\nZ_mat = jnp.column_stack([jnp.ones(n), tfd.Normal(loc=2., scale=1.).sample(n, seed=key_Z)])\n\ntrue_mean = X_mat @ true_beta\ntrue_scale = jnp.exp(Z_mat @ true_gamma)\ny_vec = tfd.Normal(loc=true_mean, scale=true_scale).sample(seed=key_y)\n```\n:::\n\n\nThe simulated data displays a linear relationship between the response $\\mathbf{y}$ and the covariate $\\mathbf{x}$.\nThe slope of the estimated regression line is close to the true $\\beta_1 = 3$.\nThe right plot shows the relationship between $\\mathbf{y}$ and the scale covariate vector $\\mathbf{z}$.\nLarger values of $\\mathbf{ z}$ lead to a larger variance of the response.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\nsns.regplot(\n    x=X_mat[:, 1],\n    y=y_vec,\n    fit_reg=True,\n    scatter_kws=dict(color=\"grey\", s=20),\n    line_kws=dict(color=\"blue\"),\n    ax=ax1,\n).set(xlabel=\"x\", ylabel=\"y\", xlim=[-0.2, 5.2])\n\nsns.scatterplot(\n    x=Z_mat[:, 1],\n    y=y_vec,\n    color=\"grey\",\n    s=40,\n    ax=ax2,\n).set(xlabel=\"z\", xlim=[-1, 5.2])\n\nfig.suptitle(\"Location-Scale Regression Model with Heteroscedastic Error\")\nfig.tight_layout()\nplt.show()\n```\n\n::: {.cell-output-display}\n![](02-ls-reg_files/figure-commonmark/plot-data-1.png)\n:::\n:::\n\n\nSince positivity of the variance is ensured by the exponential function, the linear part $\\mathbf{z}_i^T \\boldsymbol{\\gamma}$ is not restricted to the positive real line.\nHence, setting a normal prior distribution for $\\gamma$ is feasible, leading to an almost symmetric specification of the location and scale parts of the model.\nThe variables `beta` and `gamma` are initialized with values far away from zero to support a stable sampling process:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndist_beta = lsl.Dist(tfd.Normal, loc=0.0, scale=100.0)\nbeta = lsl.Var.new_param(jnp.array([10., 10.]), dist_beta, name=\"beta\")\n\ndist_gamma = lsl.Dist(tfd.Normal, loc=0.0, scale=100.0)\ngamma = lsl.Var.new_param(jnp.array([5., 5.]), dist_gamma, name=\"gamma\")\n```\n:::\n\n\nThe additional complexity of the location-scale model compared to the standard linear model is handled in the next step.\nSince `gamma` takes values on the whole real line, but the response variable `y` expects a positive scale input, we need to apply the exponential function to the linear predictor to ensure positivity.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nX = lsl.Var.new_obs(X_mat, name=\"X\")\nZ = lsl.Var.new_obs(Z_mat, name=\"Z\")\n\nmu = lsl.Var(lsl.Calc(jnp.dot, X, beta), name=\"mu\")\n\nlog_scale = lsl.Calc(jnp.dot, Z, gamma)\nscale = lsl.Var(lsl.Calc(jnp.exp, log_scale), name=\"scale\")\n\ndist_y = lsl.Dist(tfd.Normal, loc=mu, scale=scale)\ny = lsl.Var.new_obs(y_vec, dist_y, name=\"y\")\n```\n:::\n\n\nWe can now combine the nodes in a model and visualize it\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsns.set_theme(style=\"white\")\n\ngb = lsl.GraphBuilder()\ngb.add(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGraphBuilder(0 nodes, 1 vars)\n```\n:::\n\n```{.python .cell-code}\nmodel = gb.build_model() # builds the model from the graph (PGMs)\n\nlsl.plot_vars(model=model, width=12, height=8)\n```\n\n::: {.cell-output-display}\n![](02-ls-reg_files/figure-commonmark/build-and-plot-graph-3.png)\n:::\n:::\n\n\nWe choose the No U-Turn sampler for generating posterior samples.\nTherefore the location and scale parameters can be drawn by separate NUTS kernels, or, if all remaining inputs to the kernel coincide, by one common kernel.\nThe latter option might lead to better estimation results but lacks the flexibility to e.g. choose different step sizes during the sampling process.\n\nHowever, we will just fuse everything into one kernel, do not use any specific arguments and hope that the default warmup scheme (similar to the warmup used in STAN) will do the trick.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nbuilder = gs.EngineBuilder(seed=73, num_chains=4)\n\nbuilder.set_model(gs.LieselInterface(model))\nbuilder.set_initial_values(model.state)\n\nbuilder.add_kernel(gs.NUTSKernel([\"beta\", \"gamma\"]))\nbuilder.set_duration(warmup_duration=1500, posterior_duration=1000, term_duration=500)\n\nengine = builder.build()\nengine.sample_all_epochs()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  0%|                                                  | 0/3 [00:00<?, ?chunk/s]\n 33%|##############                            | 1/3 [00:01<00:03,  1.95s/chunk]\n100%|##########################################| 3/3 [00:01<00:00,  1.54chunk/s]\n\n  0%|                                                  | 0/1 [00:00<?, ?chunk/s]\n100%|########################################| 1/1 [00:00<00:00, 2709.50chunk/s]\n\n  0%|                                                  | 0/2 [00:00<?, ?chunk/s]\n100%|########################################| 2/2 [00:00<00:00, 3540.99chunk/s]\n\n  0%|                                                  | 0/4 [00:00<?, ?chunk/s]\n100%|########################################| 4/4 [00:00<00:00, 3741.57chunk/s]\n\n  0%|                                                  | 0/8 [00:00<?, ?chunk/s]\n100%|#########################################| 8/8 [00:00<00:00, 452.59chunk/s]\n\n  0%|                                                 | 0/22 [00:00<?, ?chunk/s]\n 73%|############################3          | 16/22 [00:00<00:00, 146.57chunk/s]\n100%|#######################################| 22/22 [00:00<00:00, 134.02chunk/s]\n\n  0%|                                                 | 0/20 [00:00<?, ?chunk/s]\n 80%|###############################2       | 16/20 [00:00<00:00, 148.56chunk/s]\n100%|#######################################| 20/20 [00:00<00:00, 137.01chunk/s]\n\n  0%|                                                 | 0/40 [00:00<?, ?chunk/s]\n 48%|##################5                    | 19/40 [00:00<00:00, 181.64chunk/s]\n 95%|#####################################  | 38/40 [00:00<00:00, 133.62chunk/s]\n100%|#######################################| 40/40 [00:00<00:00, 137.67chunk/s]\n```\n:::\n:::\n\n\nNow that we have 1000 posterior samples per chain, we can check the results. Starting with the trace plots just using one chain.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nresults = engine.get_results()\ng = gs.plot_trace(results, ncol=4)\n```\n\n::: {.cell-output-display}\n![](02-ls-reg_files/figure-commonmark/traceplots-5.png)\n:::\n:::\n",
    "supporting": [
      "02-ls-reg_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}