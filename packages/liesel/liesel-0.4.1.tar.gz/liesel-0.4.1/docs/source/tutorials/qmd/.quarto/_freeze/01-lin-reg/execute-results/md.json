{
  "hash": "b3ee5c95cd348ccfe352ecc8a9ccba71",
  "result": {
    "markdown": "---\nengine: knitr\n---\n\n\n\n\n# Linear regression\n\nIn this tutorial, we build a linear regression model with Liesel and estimate it\nwith Goose. Our goal is to illustrate the most important features of the software in a straightforward context.\n\n## Model building with Liesel\n\nLiesel is based on the concept of probabilistic graphical models (PGMs) to represent\n(primarily Bayesian) statistical models, so let us start with a very brief look at what PGMs are and how they are implemented in Liesel.\n\n### Probabilistic graphical models\n\nIn a PGM, each variable is represented as a node. There are two basic types of nodes in\nLiesel: strong and weak nodes. A strong node is a node whose value is defined \"outside\"\nof the model, for example, if the node represents some observed data or a parameter\n(parameters are usually set by an inference algorithm such as an optimizer or sampler).\nIn contrast, a weak node is a node whose value is defined \"within\" the model, that is,\nit is a deterministic function of some other nodes. An exp-transformation mapping a\nreal-valued parameter to a positive number, for example, would be a weak node.\n\nIn addition, each node can have an optional probability distribution.\nThe probability density or mass function of the distribution evaluated at the value of\nthe node gives its log-probability. In a typical Bayesian regression model, the response\nnode would have a normal distribution and the parameter nodes would have some prior\ndistribution (for example, a normal-inverse-gamma prior). The following table shows the\ndifferent node types and some examples of their use cases.\n\n|                          | **Strong node**                | **Weak node**                                        |\n|--------------------------|--------------------------------|------------------------------------------------------|\n| **With distribution**    | Response, parameter, ...       | Copula, ...                                          |\n| **Without distribution** | Covariate, hyperparameter, ... | Inverse link function, parameter transformation, ... |\n\nA PGM is essentially a collection of connected nodes. Two nodes can be connected\nthrough a directed edge, meaning that the first node is an input for the value or the\ndistribution of the second node. Nodes *without* an edge between them are assumed to\nbe conditionally independent, allowing us to factorize the model log-probability as\n\n$$\n\\log p(\\text{Model}) = \\sum_{\\text{Node $\\in$ Model}} \\log p(\\text{Node} \\mid \\text{Inputs}(\\text{Node})).\n$$\n\n### Imports\n\nBefore we can generate the data and build the model graph, we need to load Liesel and a number of other packages. We usually import the model building library `liesel.model` as `lsl`, and the MCMC library `liesel.goose` as `gs`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# We use distributions and bijectors from tensorflow probability\nimport tensorflow_probability.substrates.jax.distributions as tfd\nimport tensorflow_probability.substrates.jax.bijectors as tfb\n\nimport liesel.goose as gs\nimport liesel.model as lsl\n\nrng = np.random.default_rng(42)\n```\n:::\n\n\n### Generating the data\n\nNow we can simulate 500 observations from the linear regression model $y_i \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_i, \\;\\sigma^2)$ with the true parameters $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1)' = (1, 2)'$ and $\\sigma = 1$. The relationship between the response $y_i$ and the covariate $x_i$ is visualized in the following scatterplot.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# sample size and true parameters\n\nn = 500\ntrue_beta = np.array([1.0, 2.0])\ntrue_sigma = 1.0\n\n# data-generating process\n\nx0 = rng.uniform(size=n)\nX_mat = np.column_stack([np.ones(n), x0])\neps = rng.normal(scale=true_sigma, size=n)\ny_vec = X_mat @ true_beta + eps\n\n# plot the simulated data\n\nplt.scatter(x0, y_vec)\nplt.title(\"Simulated data from the linear regression model\")\nplt.xlabel(\"Covariate x\")\nplt.ylabel(\"Response y\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](01-lin-reg_files/figure-commonmark/generate-data-1.png)\n:::\n:::\n\n\n### Building the model graph\n\nThe graph of a Bayesian linear regression model is a tree, where the hyperparameters of\nthe prior are the leaves and the response is the root.\nTo build this tree in Liesel, we need to start from the leaves and work our way down\nto the root. As the most basic building blocks of a model, Liesel provides the\n{class}`.Var` class for instantiating variables and the\n{class}`.Dist`, class for wrapping probability distributions. The {class}`.Var` class\ncomes with four constructors, namely {meth}`.Var.new_param` for parameters,\n{meth}`.Var.new_obs` for observed data, {meth}`.Var.new_calc` for variables that are\ndeterministic functions of other variables in the model, and {meth}`.Var.new_value` for\nfixed values. See also [Model Building (liesel.model)](model_overview).\n\n#### The regression coefficients\n\nLet's assume the weakly informative prior $\\beta_0, \\beta_1 \\sim \\mathcal{N}(0, 100^2)$\nfor the regression coefficients.\nNow, let us create the node for the regression coefficients.\n\nTo do so, we need to define its initial value and its node distribution using the\n{class}`.Dist` class. This class wraps distribution classes with the TensorFlow\nProbability (TFP) API to connect them to our node classes. Here, the node distribution\nis initialized with three arguments: the TFP distribution object\n[(`tfd.Normal`)](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Normal),\nand the two hyperparameter nodes representing the parameters of the distribution.\nTFP uses the names `loc` for the mean and `scale` for the standard deviation, so we have\nto use the same names here. This is a general feature of {class}`.Dist`, you can always\nuse the parameter names from TFP to refer to the parameters of your distribution.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nbeta_prior = lsl.Dist(tfd.Normal, loc=0.0, scale=100.0)\n```\n:::\n\n\nNote that you could also provide a {class}`.Node` or {class}`.Var` instance for the\n`loc` and `scale` argument - this fact allows you to build hierarchical models.\nIf you provide floats like we do here, Liesel will turn them into {class}`.Value`\nnodes under the hood.\n\nWith this distribution object, we can now create the node for our regression\ncoefficient with the {meth}`.Var.new_param` constructor:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nbeta = lsl.Var.new_param(value=np.array([0.0, 0.0]), distribution=beta_prior, name=\"beta\")\n```\n:::\n\n\n#### The standard deviation\n\nThe second branch of the tree contains the residual standard deviation.\nWe build it in a similar way, now using the weakly informative prior\n$\\sigma^2 \\sim \\text{InverseGamme}(a, b)$ with $a = b = 0.01$ on the squared standard\ndeviation, i.e. the variance. Again, we use the parameter names based on TFP.\nThis time, we supply the hyperparameters as {class}`.Var` instances.\n\n\n::: {.cell}\n\n```{.python .cell-code}\na = lsl.Var.new_param(0.01, name=\"a\")\nb = lsl.Var.new_param(0.01, name=\"b\")\nsigma_sq_prior = lsl.Dist(tfd.InverseGamma, concentration=a, scale=b)\nsigma_sq = lsl.Var.new_param(value=10.0, distribution=sigma_sq_prior, name=\"sigma_sq\")\n```\n:::\n\n\nSince we need to work not only with the variance, but with the scale, we\ninitialize the scale using {meth}`.Var.new_calc`, to compute the square root.\n\nWe can use this variable constructor to include computations based on our nodes.\nIt always takes a\nfunction as its first argument, and the nodes to be used as function inputs as the\nfollowing arguments.\nThis is the first weak node that we are setting up - all previous\nnodes have been strong.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsigma = lsl.Var.new_calc(jnp.sqrt, sigma_sq, name=\"sigma\").update()\n```\n:::\n\n\n\n#### Design matrix, fitted values, and response\n\nTo compute the matrix-vector product $\\mathbf{X}\\boldsymbol{\\beta}$, we use another\nvariable instantiated via {meth}`.Var.new_calc`. We can view our model as\n$y_i \\sim \\mathcal{N}(\\mu_i, \\;\\sigma^2)$ with $\\mu_i = \\beta_0 + \\beta_1 x_i$,\nso we use the name `mu` for this product.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nX = lsl.Var.new_obs(X_mat, name=\"X\")\nmu = lsl.Var.new_calc(jnp.dot, X, beta, name=\"mu\")\n```\n:::\n\n\nFinally, we can connect the branches of the tree in a response node.\nThe value of the node are our observed response values.\nAnd since we assumed the model\n$y_i \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_i, \\;\\sigma^2)$,\nwe also need to specify the response's distribution.\nWe use our previously created nodes `sigma` and `mu` to specify this distribution:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ny_dist = lsl.Dist(tfd.Normal, loc=mu, scale=sigma)\ny = lsl.Var.new_obs(y_vec, distribution=y_dist, name=\"y\")\n```\n:::\n\n\n#### Bringing the model together\n\nNow, to construct a full-fledged Liesel model from our individual node objects, we\ncan set up the {class}`.Model`. Here, we will only add the response node.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel = lsl.Model([y])\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel(24 nodes, 8 vars)\n```\n:::\n:::\n\n\nSince all other nodes are directly or indirectly connected to this node, the Model will\nadd those nodes automatically when it builds the model.\nThe model provides a couple of convenience features, for example, to evaluate the model\nlog-probability, or to update the nodes in a topological order.\n\nThe {func}`.plot_vars()` function visualizes the graph of a model.\nStrong nodes are shown in blue, weak nodes in red.\nNodes with a probability distribution are highlighted with a star.\nIn the figure below, we can see the tree-like structure of the graph and identify the\ntwo branches for the mean and the standard deviation of the response.\nIf the layout of the graph looks messy for you,\nplease make sure you have the `pygraphviz` package installed.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlsl.plot_vars(model)\n```\n\n::: {.cell-output-display}\n![](01-lin-reg_files/figure-commonmark/plot-vars-3.png)\n:::\n:::\n\n\n### Node and model log-probabilities\n\nThe log-probability of the model, which can be interpreted as the (unnormalized)\nlog-posterior in a Bayesian context, can be accessed with the `log_prob` property.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel.log_prob\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nArray(-1179.656, dtype=float32)\n```\n:::\n:::\n\n\nThe individual nodes also have a `log_prob` property.\nIn fact, because of the conditional independence assumption of the model,\nthe log-probability of the model is given by the sum of the log-probabilities of the\nnodes with probability distributions. We take the sum for the `.log_prob` attributes of\n`beta` and `y` because, per default, the attributes return the individual\nlog-probability contributions of each element in the values of the nodes.\nSo for `beta` we would get two log-probability values, and for `y` we would get 500.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nbeta.log_prob.sum() + sigma_sq.log_prob + y.log_prob.sum()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nArray(-1179.656, dtype=float32)\n```\n:::\n:::\n\n\nNodes without a probability distribution return a log-probability of zero.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsigma.log_prob\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.0\n```\n:::\n:::\n\n\nThe log-probability of a node depends on its value and its inputs.\nThus, if we change the variance of the response from 10 to 1, the log-probability of\nthe corresponding node, the log-probability of the response node, and the\nlog-probability of the model change as well.\nNote that, since the actual input to the response distribution is the standard\ndeviation $\\sigma$, we have to update its value after changing the value of $\\sigma^2$.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(f\"Old value of sigma_sq: {sigma_sq.value}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOld value of sigma_sq: 10.0\n```\n:::\n\n```{.python .cell-code}\nprint(f\"Old log-prob of sigma_sq: {sigma_sq.log_prob}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOld log-prob of sigma_sq: -6.972140312194824\n```\n:::\n\n```{.python .cell-code}\nprint(f\"Old log-prob of y: {y.log_prob.sum()}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOld log-prob of y: -1161.6356201171875\n```\n:::\n\n```{.python .cell-code}\nsigma_sq.value = 1.0\nsigma.update()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVar(name=\"sigma\")\n```\n:::\n\n```{.python .cell-code}\nprint(f\"New value of sigma_sq: {sigma_sq.value}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNew value of sigma_sq: 1.0\n```\n:::\n\n```{.python .cell-code}\nprint(f\"New log-prob of sigma_sq: {sigma_sq.log_prob}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNew log-prob of sigma_sq: -4.655529975891113\n```\n:::\n\n```{.python .cell-code}\nprint(f\"New log-prob of y: {y.log_prob.sum()}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNew log-prob of y: -1724.6702880859375\n```\n:::\n\n```{.python .cell-code}\nprint(f\"New model log-prob: {model.log_prob}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNew model log-prob: -1740.3740234375\n```\n:::\n:::\n\n\nFor most inference algorithms, we need the gradient of the model log-probability with\nrespect to the parameters. Liesel uses [the JAX library for numerical computing and\nmachine learning](https://github.com/google/jax) to compute gradients using automatic\ndifferentiation.\n\n## MCMC inference with Goose\n\nThis section illustrates the key features of Liesel's MCMC framework Goose. To use\nGoose, the user needs to select one or more sampling algorithms, called (transition)\nkernels, for the model parameters. Goose comes with a number of standard kernels such\nas Hamiltonian Monte Carlo ({class}`~.goose.HMCKernel`) or the No U-Turn Sampler\n({class}`~.goose.NUTSKernel`). Multiple kernels can be combined in one sampling scheme\nand assigned to different parameters, and the user can implement their own\nproblem-specific kernels, as long as they are compatible with the {class}`.Kernel`\nprotocol. In any case, the user is responsible for constructing a\nmathematically valid algorithm.\nRefer to [MCMC Sampling (liesel.goose)](goose_overview) for an overview of\nimportant Goose functionality.\n\nWe start with a very simple sampling scheme, keeping $\\sigma^2$ fixed at the true value\nand using a NUTS sampler for $\\boldsymbol{\\beta}$.\nThe kernels are added to a {class}`~.goose.Engine`, which coordinates the sampling,\nincluding the kernel tuning during the warmup, and the MCMC bookkeeping.\nThe engine can be configured step by step with a {class}`.EngineBuilder`.\nWe need to inform the builder about the model, the initial values, the kernels, and the\nsampling duration. Finally, we can call the {meth}`.EngineBuilder.build` method, which\nreturns a fully configured engine.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsigma_sq.value = true_sigma**2 # setting sigma_sq to the true value\n\nbuilder = gs.EngineBuilder(seed=1337, num_chains=4)\n\ninterface = gs.LieselInterface(model)\nbuilder.set_model(interface)\nbuilder.set_initial_values(model.state)\n\nbuilder.add_kernel(gs.NUTSKernel([\"beta\"]))\n\nbuilder.set_duration(warmup_duration=1000, posterior_duration=1000)\n\nengine = builder.build()\n```\n:::\n\n\nNow we can run the MCMC algorithm for the specified duration by calling the\n{meth}`~.goose.Engine.sample_all_epochs` method. In a first step, the model and the\nsampling algorithm are compiled, so don't worry if you don't see an output right away.\nThe subsequent samples will be generated much faster.\nFinally, we can extract the results and print a summary table.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nengine.sample_all_epochs()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  0%|                                                  | 0/3 [00:00<?, ?chunk/s]\n 33%|##############                            | 1/3 [00:01<00:03,  1.57s/chunk]\n100%|##########################################| 3/3 [00:01<00:00,  1.91chunk/s]\n\n  0%|                                                  | 0/1 [00:00<?, ?chunk/s]\n100%|########################################| 1/1 [00:00<00:00, 2545.09chunk/s]\n\n  0%|                                                  | 0/2 [00:00<?, ?chunk/s]\n100%|########################################| 2/2 [00:00<00:00, 3344.74chunk/s]\n\n  0%|                                                  | 0/4 [00:00<?, ?chunk/s]\n100%|########################################| 4/4 [00:00<00:00, 3462.79chunk/s]\n\n  0%|                                                  | 0/8 [00:00<?, ?chunk/s]\n100%|########################################| 8/8 [00:00<00:00, 1356.17chunk/s]\n\n  0%|                                                 | 0/20 [00:00<?, ?chunk/s]\n100%|#######################################| 20/20 [00:00<00:00, 382.55chunk/s]\n\n  0%|                                                  | 0/2 [00:00<?, ?chunk/s]\n100%|########################################| 2/2 [00:00<00:00, 3378.42chunk/s]\n\n  0%|                                                 | 0/40 [00:00<?, ?chunk/s]\n 82%|################################1      | 33/40 [00:00<00:00, 324.23chunk/s]\n100%|#######################################| 40/40 [00:00<00:00, 309.27chunk/s]\n```\n:::\n\n```{.python .cell-code}\nresults = engine.get_results()\nsummary = gs.Summary(results)\nsummary\n```\n\n::: {.cell-output-display}\n<p><strong>Parameter summary:</strong></p>\n<table border=\"0\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>kernel</th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>q_0.05</th>\n      <th>q_0.5</th>\n      <th>q_0.95</th>\n      <th>sample_size</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>rhat</th>\n    </tr>\n    <tr>\n      <th>parameter</th>\n      <th>index</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">beta</th>\n      <th>(0,)</th>\n      <td>kernel_00</td>\n      <td>0.980</td>\n      <td>0.089</td>\n      <td>0.833</td>\n      <td>0.981</td>\n      <td>1.127</td>\n      <td>4000</td>\n      <td>1023.503</td>\n      <td>1223.167</td>\n      <td>1.001</td>\n    </tr>\n    <tr>\n      <th>(1,)</th>\n      <td>kernel_00</td>\n      <td>1.915</td>\n      <td>0.154</td>\n      <td>1.660</td>\n      <td>1.914</td>\n      <td>2.170</td>\n      <td>4000</td>\n      <td>996.507</td>\n      <td>1195.113</td>\n      <td>1.001</td>\n    </tr>\n  </tbody>\n</table>\n<p><strong>Error summary:</strong></p>\n<table border=\"0\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>count</th>\n      <th>relative</th>\n    </tr>\n    <tr>\n      <th>kernel</th>\n      <th>error_code</th>\n      <th>error_msg</th>\n      <th>phase</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">kernel_00</th>\n      <th rowspan=\"2\" valign=\"top\">1</th>\n      <th rowspan=\"2\" valign=\"top\">divergent transition</th>\n      <th>warmup</th>\n      <td>61</td>\n      <td>0.015</td>\n    </tr>\n    <tr>\n      <th>posterior</th>\n      <td>0</td>\n      <td>0.000</td>\n    </tr>\n  </tbody>\n</table>\n:::\n:::\n\n\nIf we need more samples, we can append another epoch to the engine and sample it by\ncalling either the {meth}`~.goose.Engine.sample_next_epoch` or the\n{meth}`~.goose.Engine.sample_all_epochs` method.\nThe epochs are described by {class}`.EpochConfig` objects.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nengine.append_epoch(\n    gs.EpochConfig(gs.EpochType.POSTERIOR, duration=1000, thinning=1, optional=None)\n)\n\nengine.sample_next_epoch()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  0%|                                                 | 0/40 [00:00<?, ?chunk/s]\n 82%|################################1      | 33/40 [00:00<00:00, 319.40chunk/s]\n100%|#######################################| 40/40 [00:00<00:00, 307.52chunk/s]\n```\n:::\n:::\n\n\nNo compilation is required at this point, so this is pretty fast.\n\n\n### Using a Gibbs kernel\n\nSo far, we have not sampled our variance parameter `sigma_sq`; we simply fixed it to\nthe true value of one.\nNow we extend our model with a Gibbs sampler for `sigma_sq`.\nUsing a Gibbs kernel is a bit more complicated, because Goose doesn't automatically\nderive the full conditional from the model graph.\nHence, the user needs to provide a function to sample from the full conditional.\nThe function needs to accept a PRNG state and a model state as arguments, and it\nneeds to return a dictionary with the node name as the key and the new node value as\nthe value. We could also update multiple parameters with one Gibbs kernel if we\nreturned a dictionary of length two or more.\n\nTo retrieve the relevant values of our nodes from the `model_state`,\nwe use the method {meth}`~.goose.interface.LieselInterface.extract_position`\nof the {class}`~.goose.interface.LieselInterface`.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef draw_sigma_sq(prng_key, model_state):\n\n    # extract relevant values from model state\n    pos = interface.extract_position(\n      position_keys=[\"y\", \"mu\", \"sigma_sq\", \"a\", \"b\"],\n      model_state=model_state\n    )\n\n    # calculate relevant intermediate quantities\n    n = len(pos[\"y\"])\n    resid = pos[\"y\"] - pos[\"mu\"]\n    a_gibbs = pos[\"a\"] + n / 2\n    b_gibbs = pos[\"b\"] + jnp.sum(resid**2) / 2\n\n    # draw new value from full conditional\n    draw = b_gibbs / jax.random.gamma(prng_key, a_gibbs)\n\n    # return key-value pair of variable name and new value\n    return {\"sigma_sq\": draw}\n```\n:::\n\n\nWe build the engine in a similar way as before, but this time adding the\nGibbs kernel as well.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nbuilder = gs.EngineBuilder(seed=1338, num_chains=4)\n\nbuilder.set_model(gs.LieselInterface(model))\nbuilder.set_initial_values(model.state)\n\nbuilder.add_kernel(gs.NUTSKernel([\"beta\"]))\nbuilder.add_kernel(gs.GibbsKernel([\"sigma_sq\"], draw_sigma_sq))\n\nbuilder.set_duration(warmup_duration=1000, posterior_duration=1000)\n\nengine = builder.build()\nengine.sample_all_epochs()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  0%|                                                  | 0/3 [00:00<?, ?chunk/s]\n 33%|##############                            | 1/3 [00:01<00:03,  1.54s/chunk]\n100%|##########################################| 3/3 [00:01<00:00,  1.94chunk/s]\n\n  0%|                                                  | 0/1 [00:00<?, ?chunk/s]\n100%|########################################| 1/1 [00:00<00:00, 2657.99chunk/s]\n\n  0%|                                                  | 0/2 [00:00<?, ?chunk/s]\n100%|########################################| 2/2 [00:00<00:00, 3086.32chunk/s]\n\n  0%|                                                  | 0/4 [00:00<?, ?chunk/s]\n100%|########################################| 4/4 [00:00<00:00, 3233.85chunk/s]\n\n  0%|                                                  | 0/8 [00:00<?, ?chunk/s]\n100%|########################################| 8/8 [00:00<00:00, 1206.65chunk/s]\n\n  0%|                                                 | 0/20 [00:00<?, ?chunk/s]\n100%|#######################################| 20/20 [00:00<00:00, 378.32chunk/s]\n\n  0%|                                                  | 0/2 [00:00<?, ?chunk/s]\n100%|########################################| 2/2 [00:00<00:00, 3184.74chunk/s]\n\n  0%|                                                 | 0/40 [00:00<?, ?chunk/s]\n 82%|################################1      | 33/40 [00:00<00:00, 319.10chunk/s]\n100%|#######################################| 40/40 [00:00<00:00, 305.01chunk/s]\n```\n:::\n:::\n\n\nGoose provides a couple of convenient numerical and graphical summary tools.\nThe {class}`~.goose~.goose.Summary` class computes several summary statistics\nthat can be either accessed programmatically or displayed as a summary table.\n\n\n\n```{.python .cell-code}\nresults = engine.get_results()\nsummary = gs.Summary(results)\nsummary\n```\n\n::: {.cell-output-display}\n<p><strong>Parameter summary:</strong></p>\n<table border=\"0\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>kernel</th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>q_0.05</th>\n      <th>q_0.5</th>\n      <th>q_0.95</th>\n      <th>sample_size</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>rhat</th>\n    </tr>\n    <tr>\n      <th>parameter</th>\n      <th>index</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">beta</th>\n      <th>(0,)</th>\n      <td>kernel_00</td>\n      <td>0.983</td>\n      <td>0.089</td>\n      <td>0.836</td>\n      <td>0.983</td>\n      <td>1.129</td>\n      <td>4000</td>\n      <td>1178.689</td>\n      <td>1425.977</td>\n      <td>1.003</td>\n    </tr>\n    <tr>\n      <th>(1,)</th>\n      <td>kernel_00</td>\n      <td>1.911</td>\n      <td>0.157</td>\n      <td>1.655</td>\n      <td>1.911</td>\n      <td>2.177</td>\n      <td>4000</td>\n      <td>1234.211</td>\n      <td>1393.635</td>\n      <td>1.004</td>\n    </tr>\n    <tr>\n      <th>sigma_sq</th>\n      <th>()</th>\n      <td>kernel_01</td>\n      <td>1.044</td>\n      <td>0.067</td>\n      <td>0.939</td>\n      <td>1.040</td>\n      <td>1.161</td>\n      <td>4000</td>\n      <td>3857.532</td>\n      <td>3510.714</td>\n      <td>1.001</td>\n    </tr>\n  </tbody>\n</table>\n<p><strong>Error summary:</strong></p>\n<table border=\"0\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>count</th>\n      <th>relative</th>\n    </tr>\n    <tr>\n      <th>kernel</th>\n      <th>error_code</th>\n      <th>error_msg</th>\n      <th>phase</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">kernel_00</th>\n      <th rowspan=\"2\" valign=\"top\">1</th>\n      <th rowspan=\"2\" valign=\"top\">divergent transition</th>\n      <th>warmup</th>\n      <td>59</td>\n      <td>0.015</td>\n    </tr>\n    <tr>\n      <th>posterior</th>\n      <td>0</td>\n      <td>0.000</td>\n    </tr>\n  </tbody>\n</table>\n:::\n\n\nWe can plot the trace plots of the chains with {func}`~.goose.plot_trace()`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ng = gs.plot_trace(results)\n```\n\n::: {.cell-output-display}\n![](01-lin-reg_files/figure-commonmark/trace-plot-5.png)\n:::\n:::\n\n\nWe could also take a look at a kernel density estimator\nwith {func}`~.goose.plot_density()` and the estimated\nautocorrelation with {func}`~.goose.plot_cor()`.\nAlternatively, we can output all three diagnostic plots together\nwith {func}`~.goose.plot_param()`. The following plot shows the parameter $\\beta_0$.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ngs.plot_param(results, param=\"beta\", param_index=0)\n```\n\n::: {.cell-output-display}\n![](01-lin-reg_files/figure-commonmark/parameter-summary-plot-7.png)\n:::\n:::\n\n\n\nHere, we end this first tutorial. We have learned about a lot of\ndifferent classes and we have\nseen how we can flexibly use different Kernels for drawing MCMC samples - that is\nquite a bit for the start. Now, have fun modelling with Liesel!\n",
    "supporting": [
      "01-lin-reg_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}