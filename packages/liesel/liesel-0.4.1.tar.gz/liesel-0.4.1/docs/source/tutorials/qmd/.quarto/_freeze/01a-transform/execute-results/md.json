{
  "hash": "c21d37ecfab18e396521f234a12396f0",
  "result": {
    "markdown": "---\nengine: knitr\n---\n\n\n\n\n# Parameter transformations\n\nThis tutorial builds on the [linear regression tutorial](01-lin-reg.md#linear-regression). Here, we demonstrate how we can easily transform a parameter in our model to sample it with NUTS instead of a Gibbs Kernel.\n\nFirst, let's set up our model again. This is the same model as in the [linear regression tutorial](01-lin-reg.md#linear-regression), so we will not go into the details here.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# We use distributions and bijectors from tensorflow probability\nimport tensorflow_probability.substrates.jax.distributions as tfd\nimport tensorflow_probability.substrates.jax.bijectors as tfb\n\nimport liesel.goose as gs\nimport liesel.model as lsl\n\nrng = np.random.default_rng(42)\n\n# data-generating process\nn = 500\ntrue_beta = np.array([1.0, 2.0])\ntrue_sigma = 1.0\nx0 = rng.uniform(size=n)\nX_mat = np.c_[np.ones(n), x0]\ny_vec = X_mat @ true_beta + rng.normal(scale=true_sigma, size=n)\n\n# Model\n# Part 1: Model for the mean\nbeta_prior = lsl.Dist(tfd.Normal, loc=0.0, scale=100.0)\nbeta = lsl.Var.new_param(value=np.array([0.0, 0.0]), distribution=beta_prior,name=\"beta\")\n\nX = lsl.Var.new_obs(X_mat, name=\"X\")\nmu = lsl.Var(lsl.Calc(jnp.dot, X, beta), name=\"mu\")\n\n# Part 2: Model for the standard deviation\na = lsl.Var(0.01, name=\"a\")\nb = lsl.Var(0.01, name=\"b\")\nsigma_sq_prior = lsl.Dist(tfd.InverseGamma, concentration=a, scale=b)\nsigma_sq = lsl.Var.new_param(value=10.0, distribution=sigma_sq_prior, name=\"sigma_sq\")\n\nsigma = lsl.Var(lsl.Calc(jnp.sqrt, sigma_sq), name=\"sigma\")\n\n# Observation model\ny_dist = lsl.Dist(tfd.Normal, loc=mu, scale=sigma)\ny = lsl.Var(y_vec, distribution=y_dist, name=\"y\")\n```\n:::\n\n\nNow let's try to sample the full parameter vector $(\\boldsymbol{\\beta}', \\sigma)'$ with a single NUTS kernel instead of using a NUTS kernel for $\\boldsymbol{\\beta}$ and a Gibbs kernel for $\\sigma^2$. Since the standard deviation is a positive-valued parameter, we need to log-transform it to sample it with a NUTS kernel. The {class}`.Var` class provides the method {meth}`.Var.transform` for this purpose.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsigma_sq.transform(tfb.Exp())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVar(name=\"sigma_sq_transformed\")\n```\n:::\n\n```{.python .cell-code}\ngb = lsl.GraphBuilder().add(y)\nmodel = gb.build_model()\nlsl.plot_vars(model)\n```\n\n::: {.cell-output-display}\n![](01a-transform_files/figure-commonmark/graph-and-transformation-1.png)\n:::\n:::\n\n\n\nThe response distribution still requires the standard deviation on the original scale. The model graph shows that the back-transformation from the logarithmic to the original scale is performed by a inserting the `sigma_sq_transformed` node and turning the `sigma_sq` node into a weak node. This weak node now deterministically depends on `sigma_sq_transformed`: its value is the back-transformed variance.\n\nNow we can set up and run an MCMC algorithm with a NUTS kernel for all parameters.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nbuilder = gs.EngineBuilder(seed=1339, num_chains=4)\n\nbuilder.set_model(gs.LieselInterface(model))\nbuilder.set_initial_values(model.state)\n\nbuilder.add_kernel(gs.NUTSKernel([\"beta\", \"sigma_sq_transformed\"]))\n\nbuilder.set_duration(warmup_duration=1000, posterior_duration=1000)\n\n# by default, goose only stores the parameters specified in the kernels.\n# let's also store the standard deviation on the original scale.\nbuilder.positions_included = [\"sigma_sq\"]\n\nengine = builder.build()\nengine.sample_all_epochs()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  0%|                                                  | 0/3 [00:00<?, ?chunk/s]\n 33%|##############                            | 1/3 [00:01<00:03,  1.88s/chunk]\n100%|##########################################| 3/3 [00:01<00:00,  1.59chunk/s]\n\n  0%|                                                  | 0/1 [00:00<?, ?chunk/s]\n100%|########################################| 1/1 [00:00<00:00, 2624.72chunk/s]\n\n  0%|                                                  | 0/2 [00:00<?, ?chunk/s]\n100%|########################################| 2/2 [00:00<00:00, 3501.09chunk/s]\n\n  0%|                                                  | 0/4 [00:00<?, ?chunk/s]\n100%|########################################| 4/4 [00:00<00:00, 3901.68chunk/s]\n\n  0%|                                                  | 0/8 [00:00<?, ?chunk/s]\n100%|########################################| 8/8 [00:00<00:00, 1003.27chunk/s]\n\n  0%|                                                 | 0/20 [00:00<?, ?chunk/s]\n100%|#######################################| 20/20 [00:00<00:00, 263.48chunk/s]\n\n  0%|                                                  | 0/2 [00:00<?, ?chunk/s]\n100%|########################################| 2/2 [00:00<00:00, 3489.44chunk/s]\n\n  0%|                                                 | 0/40 [00:00<?, ?chunk/s]\n 60%|#######################4               | 24/40 [00:00<00:00, 239.02chunk/s]\n100%|#######################################| 40/40 [00:00<00:00, 206.91chunk/s]\n```\n:::\n:::\n\n\nJudging from the trace plots, it seems that all chains have converged.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nresults = engine.get_results()\ng = gs.plot_trace(results)\n```\n\n::: {.cell-output-display}\n![](01a-transform_files/figure-commonmark/traceplots-3.png)\n:::\n:::\n\n\nWe can also take a look at the summary table, which includes the original $\\sigma^2$ and the transformed $\\log(\\sigma^2)$.\n\n\n\n```{.python .cell-code}\ngs.Summary(results)\n```\n\n::: {.cell-output-display}\n<p><strong>Parameter summary:</strong></p>\n<table border=\"0\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>kernel</th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>q_0.05</th>\n      <th>q_0.5</th>\n      <th>q_0.95</th>\n      <th>sample_size</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>rhat</th>\n    </tr>\n    <tr>\n      <th>parameter</th>\n      <th>index</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">beta</th>\n      <th>(0,)</th>\n      <td>kernel_00</td>\n      <td>0.989</td>\n      <td>0.092</td>\n      <td>0.841</td>\n      <td>0.987</td>\n      <td>1.138</td>\n      <td>4000</td>\n      <td>1386.502</td>\n      <td>1433.462</td>\n      <td>1.003</td>\n    </tr>\n    <tr>\n      <th>(1,)</th>\n      <td>kernel_00</td>\n      <td>1.900</td>\n      <td>0.161</td>\n      <td>1.638</td>\n      <td>1.903</td>\n      <td>2.159</td>\n      <td>4000</td>\n      <td>1370.704</td>\n      <td>1355.871</td>\n      <td>1.002</td>\n    </tr>\n    <tr>\n      <th>sigma_sq</th>\n      <th>()</th>\n      <td>-</td>\n      <td>1.044</td>\n      <td>0.066</td>\n      <td>0.941</td>\n      <td>1.042</td>\n      <td>1.156</td>\n      <td>4000</td>\n      <td>2468.921</td>\n      <td>2080.546</td>\n      <td>1.001</td>\n    </tr>\n    <tr>\n      <th>sigma_sq_transformed</th>\n      <th>()</th>\n      <td>kernel_00</td>\n      <td>0.041</td>\n      <td>0.063</td>\n      <td>-0.061</td>\n      <td>0.041</td>\n      <td>0.145</td>\n      <td>4000</td>\n      <td>2468.921</td>\n      <td>2080.546</td>\n      <td>1.001</td>\n    </tr>\n  </tbody>\n</table>\n<p><strong>Error summary:</strong></p>\n<table border=\"0\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>count</th>\n      <th>relative</th>\n    </tr>\n    <tr>\n      <th>kernel</th>\n      <th>error_code</th>\n      <th>error_msg</th>\n      <th>phase</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">kernel_00</th>\n      <th rowspan=\"2\" valign=\"top\">1</th>\n      <th rowspan=\"2\" valign=\"top\">divergent transition</th>\n      <th>warmup</th>\n      <td>62</td>\n      <td>0.015</td>\n    </tr>\n    <tr>\n      <th>posterior</th>\n      <td>0</td>\n      <td>0.000</td>\n    </tr>\n  </tbody>\n</table>\n:::\n\n\nFinally, let's check the autocorrelation of the samples.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ng = gs.plot_cor(results)\n```\n\n::: {.cell-output-display}\n![](01a-transform_files/figure-commonmark/correlation-plots-5.png)\n:::\n:::\n",
    "supporting": [
      "01a-transform_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}