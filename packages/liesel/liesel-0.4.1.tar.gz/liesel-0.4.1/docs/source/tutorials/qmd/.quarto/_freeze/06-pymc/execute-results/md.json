{
  "hash": "893e8ef123d99a1ae70bc76c9e32edf2",
  "result": {
    "markdown": "---\nengine: knitr\n---\n\n\n\n\n# PyMC and Liesel: Spike and Slab\n\nLiesel provides an interface for [PyMC](https://www.pymc.io/welcome.html), a popular Python library for Bayesian Models. In this tutorial, we see how to specify a model in PyMC and then fit it using Liesel.\n\nBe sure that you have `pymc` installed. If that's not the case, you can install Liesel with the optional dependency PyMC.\n\n```bash\npip install liesel[pymc]\n```\n\nWe will build a Spike and Slab model, a Bayesian approach that allows for variable selection by assuming a mixture of two distributions for the prior distribution of the regression coefficients: a point mass at zero (the \"spike\") and a continuous distribution centered around zero (the \"slab\"). The model assumes that each coefficient $\\beta_j$ has a corresponding indicator variable $\\delta_j$ that takes a value of either 0 or 1, indicating whether the variable is included in the model or not. The prior distribution of the indicator variables is a Bernoulli distribution, with a parameter $\\theta$ that controls the sparsity of the model. When the parameter is close to 1, the model is more likely to include all variables, while when it is close to 0, the model is more likely to select only a few variables. In our case, we assign a Beta hyperprior to $\\theta$:\n\n$$\n\\begin{eqnarray}\n\\mathbf{y} &\\sim& \\mathcal{N} \\left( \\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I} \\right)\\\\\n\\boldsymbol{\\beta}_j &\\sim& \\mathbfcal{N}\\left(0, (1 - \\delta_j)\\nu + \\delta_j\\tau^2_j / \\sigma^2 \\right)\\\\\n\\tau^2_j &\\sim& \\mathcal{IG}(\\text{a}_{\\tau}, \\text{b}_{\\tau})\\\\\n\\delta_j &\\sim& \\text{Bernoulli}(\\theta)\\\\\n\\theta &\\sim& \\text{Beta}(\\text{a}_\\theta, \\text{b}_\\theta)\\\\\n\\sigma^2 &\\sim& \\mathcal{IG}(\\text{a}_{\\sigma^2}, \\text{b}_{\\sigma^2})\n\\end{eqnarray}.\n$$\n\nwhere $\\nu$ is a hyperparameter that we set to a fixed small value. That way, when $\\delta_j = 0$,\nthe prior variance for $\\beta_j$ is extremely small, practically forcing it to be close\nto zero.\n\nFirst, we generate the data. We use a model with four coefficients but assume that only two variables are relevant, namely the first and the third one.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nRANDOM_SEED = 123\nrng = np.random.RandomState(RANDOM_SEED)\n\nn = 1000\np = 4\n\nsigma_scalar = 1.0\nbeta_vec = np.array([3.0, 0.0, 4.0, 0.0])\n\nX = rng.randn(n, p).astype(np.float32)\n\nerrors = rng.normal(size=n).astype(np.float32)\n\ny = X @ beta_vec + sigma_scalar * errors\n```\n:::\n\n\nThen, we can specify the model using PyMC.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nspike_and_slab_model = pm.Model()\n\nmu = 0.\n\nalpha_tau = 1.0\nbeta_tau = 1.0\n\nalpha_sigma = 1.0\nbeta_sigma = 1.0\n\nalpha_theta = 8.0\nbeta_theta = 8.0\n\nnu = 0.1\n\nwith spike_and_slab_model:\n    # priors\n    sigma2 = pm.InverseGamma(\n        \"sigma2\", alpha=alpha_sigma, beta=beta_sigma\n    )\n\n    theta = pm.Beta(\"theta\", alpha=alpha_theta, beta=beta_theta)\n    delta = pm.Bernoulli(\"delta\", p=theta, size=p)\n    tau = pm.InverseGamma(\"tau\", alpha=alpha_tau, beta=beta_tau)\n\n    beta = pm.Normal(\"beta\", mu=0.0, sigma=nu * (1 - delta) + delta * pm.math.sqrt(tau / sigma2), shape=p)\n\n    # make a data node\n    Xx = pm.Data(\"X\", X)\n\n    # likelihood\n    pm.Normal(\"y\", mu=Xx @ beta, sigma=pm.math.sqrt(sigma2), observed=y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ny\n```\n:::\n:::\n\n\nLet's take a look at our model:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nspike_and_slab_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<pymc.model.core.Model object at 0x167d21940>\n```\n:::\n:::\n\n\nThe class {class}`.PyMCInterface` offers an interface between PyMC and Goose. By default, the constructor of {class}`.PyMCInterface` keeps track only of a representation of random variables that can be used in sampling. For example, `theta` is transformed to the real-numbers space with a log-odds transformation, and therefore the model only keeps track of `theta_log_odds__`. However, we would like to access the untransformed samples as well. We can do this by including them in the `additional_vars` argument of the constructor of the interface.\n\nThe initial position can be extracted with {meth}`.get_initial_state`. The model state is represented as a `Position`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ninterface = PyMCInterface(spike_and_slab_model, additional_vars=[\"sigma2\", \"tau\", \"theta\"])\nstate = interface.get_initial_state()\n```\n:::\n\n\nSince $\\delta_j$ is a discrete variable, we need to use a Gibbs sampler to draw samples for it. Unfortunately, we cannot derive the posterior analytically, but what we can do is use a Metropolis-Hastings step as a transition function:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef delta_transition_fn(prng_key, model_state):\n    draw_key, mh_key = jax.random.split(prng_key)\n    theta_logodds = model_state[\"theta_logodds__\"]\n    p = jax.numpy.exp(theta_logodds) / (1 + jax.numpy.exp(theta_logodds))\n    draw = jax.random.bernoulli(draw_key, p=p, shape=(4,))\n    proposal = {\"delta\": jax.numpy.asarray(draw,dtype=np.int64)}\n    _, state = gs.mh.mh_step(prng_key=mh_key, model=interface, proposal=proposal, model_state=model_state)\n    return state\n```\n:::\n\n\nFinally, we can sample from the posterior as we do for any other Liesel model. In this case, we use a {class}`~.goose.GibbsKernel` for $\\boldsymbol{\\delta}$ and a {class}`~.goose.NUTSKernel` both for the remaining parameters.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nbuilder = gs.EngineBuilder(seed=13, num_chains=4)\nbuilder.set_model(interface)\nbuilder.set_initial_values(state)\nbuilder.set_duration(warmup_duration=1000, posterior_duration=2000)\n\nbuilder.add_kernel(gs.NUTSKernel(position_keys=[\"beta\", \"sigma2_log__\", \"tau_log__\", \"theta_logodds__\"]))\nbuilder.add_kernel(gs.GibbsKernel([\"delta\"], transition_fn=delta_transition_fn))\n\nbuilder.positions_included = [\"sigma2\", \"tau\"]\n\nengine = builder.build()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n/Users/johannesbrachem/Documents/git/liesel/.venv/lib/python3.13/site-packages/jax/_src/numpy/array_methods.py:122: UserWarning: Explicitly requested dtype float64 requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n  return lax_numpy.astype(self, dtype, copy=copy, device=device)\n```\n:::\n\n```{.python .cell-code}\nengine.sample_all_epochs()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  0%|                                                  | 0/3 [00:00<?, ?chunk/s]<string>:6: UserWarning: Explicitly requested dtype <class 'numpy.int64'> requested in asarray is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n\n 33%|##############                            | 1/3 [00:03<00:06,  3.08s/chunk]\n100%|##########################################| 3/3 [00:03<00:00,  1.03s/chunk]\n\n  0%|                                                  | 0/1 [00:00<?, ?chunk/s]\n100%|########################################| 1/1 [00:00<00:00, 3355.44chunk/s]\n\n  0%|                                                  | 0/2 [00:00<?, ?chunk/s]\n100%|########################################| 2/2 [00:00<00:00, 4275.54chunk/s]\n\n  0%|                                                  | 0/4 [00:00<?, ?chunk/s]\n100%|########################################| 4/4 [00:00<00:00, 5187.76chunk/s]\n\n  0%|                                                  | 0/8 [00:00<?, ?chunk/s]\n100%|#########################################| 8/8 [00:00<00:00, 857.99chunk/s]\n\n  0%|                                                 | 0/20 [00:00<?, ?chunk/s]\n100%|#######################################| 20/20 [00:00<00:00, 281.81chunk/s]\n\n  0%|                                                  | 0/2 [00:00<?, ?chunk/s]\n100%|########################################| 2/2 [00:00<00:00, 3523.14chunk/s]\n\n  0%|                                                 | 0/80 [00:00<?, ?chunk/s]\n 35%|#############6                         | 28/80 [00:00<00:00, 270.01chunk/s]\n 70%|###########################3           | 56/80 [00:00<00:00, 229.85chunk/s]\n100%|#######################################| 80/80 [00:00<00:00, 220.23chunk/s]\n100%|#######################################| 80/80 [00:00<00:00, 226.23chunk/s]\n```\n:::\n:::\n\n\nNow, we can take a look at the summary of the results and at the trace plots.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nresults = engine.get_results()\nprint(gs.Summary(results))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n/Users/johannesbrachem/Documents/git/liesel/.venv/lib/python3.13/site-packages/arviz/stats/diagnostics.py:845: RuntimeWarning: invalid value encountered in scalar divide\n  varsd = varvar / evar / 4\n/Users/johannesbrachem/Documents/git/liesel/.venv/lib/python3.13/site-packages/arviz/stats/diagnostics.py:845: RuntimeWarning: invalid value encountered in scalar divide\n  varsd = varvar / evar / 4\n/Users/johannesbrachem/Documents/git/liesel/.venv/lib/python3.13/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n                         var_fqn     kernel  ...   hdi_low  hdi_high\nvariable                                     ...                    \nbeta                     beta[0]  kernel_00  ...  2.983979  3.089679\nbeta                     beta[1]  kernel_00  ... -0.060213  0.037683\nbeta                     beta[2]  kernel_00  ...  3.901725  4.003946\nbeta                     beta[3]  kernel_00  ... -0.052502  0.048943\ndelta                   delta[0]  kernel_01  ...  1.000000  1.000000\ndelta                   delta[1]  kernel_01  ...  0.000000  0.000000\ndelta                   delta[2]  kernel_01  ...  1.000000  1.000000\ndelta                   delta[3]  kernel_01  ...  0.000000  0.000000\nsigma2                    sigma2          -  ...  0.941187  1.091447\nsigma2_log__        sigma2_log__  kernel_00  ... -0.059287  0.088789\ntau                          tau          -  ...  0.316583  0.679981\ntau_log__              tau_log__  kernel_00  ...  0.961559  3.379387\ntheta_logodds__  theta_logodds__  kernel_00  ... -0.769520  0.753686\n\n[13 rows x 17 columns]\n```\n:::\n:::\n\n\nAs we can see from the posterior means of the $\\boldsymbol{\\delta}$ parameters, the model was able to recognize those variable with no influence on the respose $\\mathbf{y}$:\n\n1. $\\delta_1$ and $\\delta_3$ (`delta[0]` and `delta[2]` in the table) have a posterior mean of $1$, indicating inclusion.\n2. $\\delta_2$ and $\\delta_4$ (`delta[1]` and `delta[3]` in the table) have a posterior mean of $0.06$, indicating exclusion.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ngs.plot_trace(results)\n```\n\n::: {.cell-output-display}\n![](06-pymc_files/figure-commonmark/results-plot-1.png)\n:::\n\n::: {.cell-output-display}\n![](06-pymc_files/figure-commonmark/results-plot-2.png)\n:::\n:::\n",
    "supporting": [
      "06-pymc_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}