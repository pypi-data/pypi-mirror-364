#!/usr/bin/env python3
"""
LiteLLM Proxy Integration Example - Revenium Griptape

This example demonstrates how to integrate Revenium usage metering
with LiteLLM proxy using the ReveniumLiteLLMDriver.

Features:
- LiteLLM proxy integration with official Revenium approach
- Automatic usage tracking and cost monitoring
- Rich metadata injection for analytics
- Works with any LiteLLM proxy-supported model

Note: This uses the official Revenium LiteLLM proxy integration approach
with requests.post and Authorization Bearer headers.
"""

import os
import sys
import logging
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Add src to path for development
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

# Set up logging
logging.basicConfig(level=logging.INFO)

def run_litellm_proxy_example():
    """Run the LiteLLM proxy integration example."""
    
    print("=" * 60)
    print("üöÄ LiteLLM Proxy Integration with Revenium")
    print("=" * 60)
    print("Demonstrating multi-provider usage metering via LiteLLM proxy")
    print()
    
    # Check required environment variables
    if not os.getenv("REVENIUM_METERING_API_KEY"):
        print("‚ùå Error: REVENIUM_METERING_API_KEY environment variable is required")
        return False
    
    # LiteLLM proxy specific requirements
    proxy_url = os.getenv("LITELLM_PROXY_URL")
    proxy_key = os.getenv("LITELLM_API_KEY")
    
    if not proxy_url:
        print("‚ö†Ô∏è  LITELLM_PROXY_URL not found.")
        print("üí° For LiteLLM proxy integration, you need:")
        print("   - A running LiteLLM proxy server")
        print("   - LITELLM_PROXY_URL environment variable")
        print("   - Optional: LITELLM_API_KEY for authentication")
        print()
        print("üîß Example setup:")
        print("   export LITELLM_PROXY_URL='http://localhost:4000'")
        print("   export LITELLM_API_KEY='your-proxy-key'")
        print()
        print("üìö See: https://docs.litellm.ai/docs/proxy/quick_start")
        return False
    
    try:
        # Import after validation to ensure clean error messages
        from revenium_griptape import ReveniumLiteLLMDriver
        from griptape.structures import Agent
        
        # Simple metadata - replace with your own values
        metadata = {
            "organization_id": "demo-litellm-proxy",     # Your organization ID
            "subscriber": {
                "id": "demo-user",                       # Current user ID
                "email": "demo@litellm-proxy.com",
                "credential": {
                    "name": "proxy_api_key",
                    "value": "proxy_key_value"
                }
            },
            "task_type": "litellm_proxy_example",        # Type of AI task
            "product_id": "griptape-demo"                # Your product/feature
        }
        
        print("üîß Creating LiteLLM driver with Revenium integration...")
        print(f"Tracking metadata: {list(metadata.keys())}")
        print(f"Organization ID: '{metadata['organization_id']}'")
        print(f"Proxy URL: {proxy_url}")
        
        # Create Revenium-enabled LiteLLM driver
        driver_kwargs = {
            "model": "gpt-3.5-turbo",  # Model available via your proxy
            "usage_metadata": metadata,
            "proxy_url": proxy_url,
            "temperature": 0.7,
            "max_tokens": 150
        }
        
        # Add proxy authentication if available
        if proxy_key:
            driver_kwargs["proxy_api_key"] = proxy_key
            print(f"Using proxy authentication")
        
        driver = ReveniumLiteLLMDriver(**driver_kwargs)
        
        print("‚úÖ Driver created successfully")
        
        # Create agent with the driver
        agent = Agent(prompt_driver=driver)
        
        # Run example tasks
        print("\nüåê Running AI tasks via LiteLLM proxy...")
        
        try:
            # Task 1: Simple response
            result1 = agent.run("Say 'Hello from LiteLLM proxy integration!'")
            print(f"Response: {result1.output.value}")
            
            # Task 2: Analysis task
            result2 = agent.run("Explain the benefits of using a proxy for AI APIs in 2 sentences.")
            print(f"Analysis: {result2.output.value}")
            
            # Task 3: Creative task  
            result3 = agent.run("Write a haiku about cloud infrastructure.")
            print(f"Creative: {result3.output.value}")
            
            print(f"\n‚úÖ LiteLLM proxy integration completed successfully!")
            print(f"üîç Check your Revenium dashboard for:")
            print(f"   Organization ID: '{metadata['organization_id']}'")
            print(f"   Provider: LiteLLM Proxy")
            print(f"   Usage metrics and cost tracking")
            
        except Exception as api_error:
            print(f"\n‚ö†Ô∏è  API call failed (this is common with proxy setup):")
            print(f"   Error: {api_error}")
            print(f"üí° Common solutions:")
            print(f"   - Ensure your LiteLLM proxy is running and accessible")
            print(f"   - Verify the model 'gpt-3.5-turbo' is configured in your proxy")
            print(f"   - Check proxy authentication if required")
            print(f"   - Ensure underlying provider API keys are configured")
            
            # Return success since driver creation worked, just API call failed
            return True
        
    except Exception as e:
        print(f"\n‚ùå LiteLLM proxy example failed:")
        print(f"   Error: {e}")
        print(f"   Check your LiteLLM proxy configuration")
        return False
        
    return True

def show_proxy_setup_guide():
    """Show a quick guide for setting up LiteLLM proxy with Revenium."""
    
    print("\nLiteLLM Proxy + Revenium Setup Guide")
    print("=" * 50)
    print()
    print("1. Install LiteLLM proxy middleware:")
    print("   pip install 'revenium-middleware-litellm[litellm_proxy]'")
    print()
    print("2. Create proxy config (config.yaml):")
    print("""   model_list:
     - model_name: gpt-3.5-turbo
       litellm_params:
         model: openai/gpt-3.5-turbo
         api_key: env/OPENAI_API_KEY
   
   litellm_settings:
     callbacks: ["revenium"]""")
    print()
    print("3. Start proxy with Revenium callback:")
    print("   REVENIUM_METERING_API_KEY=your-key litellm --config config.yaml")
    print()
    print("4. Set environment variables:")
    print("   export LITELLM_PROXY_URL='http://localhost:4000'")
    print()
    print("Full docs: https://pypi.org/project/revenium-middleware-litellm/")

if __name__ == "__main__":
    success = run_litellm_proxy_example()
    
    if success:
        print("\nLiteLLM proxy example completed!")
        print("Next steps:")
        print("   - Check your Revenium dashboard for usage data")
        print("   - Try different models configured in your proxy")
        print("   - Use litellm_client_example.py for direct client integration")
        print("   - Use universal_example.py for automatic provider detection")
    else:
        show_proxy_setup_guide()
        print("\n‚ùå LiteLLM proxy example failed. See setup guide above.")
        exit(1) 